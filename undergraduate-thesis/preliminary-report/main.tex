%dica: use a opção oneside se houver um limite (e.g., 20) de páginas
\documentclass[
  english,
  lmodern,
  oneside
]{ufsc-thesis-rn46-2019/ufsc-thesis-rn46-2019}

\usepackage[T1]{fontenc} % fontes
\usepackage[utf8]{inputenc} % UTF-8
\usepackage{pdfpages} % Inclui PDF externo (ficha catalográfica)

\usepackage{tikz}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Configurações da classe (dados do trabalho)                  %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Preâmbulo
\titulo{Template \LaTeX{} seguindo a RN 46/2019/CPG da UFSC}
\autor{Bernardo Ferrari Mendonça}
\data{3 de Dezembro de 2020}
\instituicao{Universidade Federal de Santa Catarina}
\centro{Centro Tecnológico}
\local{Florianópolis}
\programa{Programa de Pós-Graduação em Ciência da Computação}
\tcc{}
\curso{Ciência da Computação}
\departamento{Departamento de Informática e Estatística}
\titulode{Bacharel em Ciência da Computação}

\orientador{Prof.\ Alcides Fonseca, Dr.}
\coorientador{Prof.\ Rafael de Santiago, Dr.}
\afiliacaoorientador{Universidade de Lisboa}
\afiliacaocoorientador{Universidade Federal de Santa Catarina}

\membrabanca{Prof\textsuperscript{a}.\ Jerusa Marchi, Dr.}{Universidade Federal de Santa Catarina}

\coordenador{Prof.\ José Francisco Fletes, Dr}

\begin{document}

\pretextual{}
\imprimircapa{}
\imprimirfolhaderosto*
\protect%\incluirfichacatalografica{ficha.pdf}
\imprimirfolhadecertificacao{}



\begin{dedicatoria}
  TODO:\@ dedicatoria
\end{dedicatoria}

\begin{agradecimentos}
  TODO:\@ agradecimentos
\end{agradecimentos}

\begin{epigrafe}
  TODO:\@ epigrafe
\end{epigrafe}


% 150 a 500 palavras.
% Mantatorio ser em Português, usando a voz ativa na terceira pessoa.
% Contexto; Problema; Estado da Arte; Solução proposta; Resultados.
\begin{resumo}[Resumo]
Compiladores são os programas de computadores que fazem o papel de tradutor entre as linguagens de programação e as linguagens de maquina.
Eles recebem como entrada um texto escrito em uma linguagem fonte, chamado de programa fonte, e produzem como saída um texto escrito em uma linguagem de maquina alvo, chamado de programa alvo.
O processo de tradução executado por um compilador pode ser separado em duas grandes etapas: a etapa de analise e a etapa de síntese.
Durante a etapa de analise, um sistema de tipos pode ser utilizado.
O sistema de tipos classificara as diferentes sentenças de um programa quanto aos tipos dos valores que as sentenças computam com o objetivo de provar a ausencia de determinados comportamentos de programas do programa fonte.
Um dos possiveis sistemas de tipos é chamado de sistema de tipos refinados, onde é anexado os tipos dos valores de um programa predicados lógicos que garantem propriedades extras.
Entre as etapas de analise e síntese pode se fazer o uso de uma linguagem intermediária.
O uso de uma linguagem intermediária é especialmente vantajoso pois, um compilador para a linguagem $i$ e maquina $j$ pode ser construído combinando um \textit{front end} para $i$ e um \textit{back end} para $j$.
O projeto LLVM é uma biblioteca de funcionalidades para otimização de código intermediário e geração de código alvo.
Estas funcionalidades são construídas ao redor de uma linguagem intermediária chamada \textit{LLVM intermediate representation}, ou LLVM IR\@.
O objetivo do trabalho consiste em propor o projeto de uma linguagem de programação que utilize um sistema de refinamento de tipos e a confecção de seu compilador utilizando o ecossistema de ferramentas LLVM\@.
É esperada que a ferramenta consiga fazer a analise do código fonte utilizando o sistema de tipos refinados proposto e sua tradução para código de maquina alvo.

\vspace{\onelineskip}
\noindent
\textbf{Palavras-chave}: linguagem de programação\@. representação intermediária de código\@. compilador\@. LLVM\@. LLVM IR\@.
\end{resumo}

% \begin{resumo}[Resumo Estendido]
%   \section*{Introdução}
%   A hifenização é alterada para \texttt{brazil}, mesmo para documentos em inglês. Descrever brevemente esses itens exigidos pela BU. Como a RN 95/CUn/2017 é mais recente e impõe outras regras a revelia de regimentos e regulamentos, é mais sábio obedecê-la. Lembre que esse resumo estendido deve term entre 2 e 5 páginas.
%
%   \lipsum[1]
%   \section*{Objetivos}
%   \lipsum[2]
%   \section*{Metodologia}
%   \lipsum[3]
%   \section*{Resultados e Discussão}
%   \lipsum[4]
%   \section*{Considerações Finais}
%   \lipsum[5]
%
%   \vspace{\baselineskip}  % Atenção! manter igual ao resumo
%   \textbf{Palavras-chave:} Palavra-chave. Outra Palavra-chave composta. Bla.
% \end{resumo}


\begin{abstract}
  TODO:\@ abstract
  Enlish version of the plain ``resumo'' above. Done with environment
  \texttt{abstract}. Hyphenization is automatically changed to english.

  \vspace{\baselineskip}
  \textbf{Keywords:} Keyword. Another Compound Keyword. Bla.
\end{abstract}

\listoffigures*

\listofalgorithms*

\begin{listadesimbolos}
  % $\gets$   & Atribuição \\
  % $\exists$   & Quantificação existencial \\
  % $\rightarrow$   & Implicação \\
  % $\wedge$   & E lógico \\
  % $\vee$   & Ou lógico \\
  % $\neg$   & Negação lógica \\
  % $\mapsto$   & Mapeia para \\
  % $\sqsubseteq$   & Subclasse (em ontologias) \\
  % $\subseteq$   & Subconjunto: $\forall x\;.\; x \in A \rightarrow x \in B$ \\
  % $\langle\ldots\rangle$ & Tupla \\
  % $\forall$   & Quantificação universal \\
  % mmmmm & Nenhum sentido, apenas estou aqui para demonstrar a largura máxima dessas colunas. Ao abrir o ambiente \texttt{listadesimbolos}, pode-se fornecer um argumento opcional indicando a largura da coluna da esquerda (o default é de 5em): \texttt{\textbackslash{}begin\{listadesimbolos\}[2cm] .... \textbackslash{}end\{listadesimbolos\}} \\
\end{listadesimbolos}

\tableofcontents*

\textual{}

\chapter{Introduction}\label{chapter:introduction}

This work deals with the problem of translating a programming language with refinement types to the intermediate representation language LLVM-IR.\@
Throughout the work, we will be presenting: the design of a small programming language; an operational semantics with refinement types for the designed language's type system; and a compiler implementation for the designed language to LLVM-IR.\@
In this introductory chapter we will explore the motivations behind this work and its goals.

\section{Motivation}\label{chapter:introduction:sec:motivation}

Programming languages are notations to describe computations to people and machines.
This notations can take numerous forms, from binary code, ready to be executed by a specific machine, to a higher-level language, capable of expressing more abstract notations such as functions and types.
The use of higher-level programming languages eases how people can describe computations to one another.
Their abstractions are capable of describing computations without needing to expose details from the machine that will execute them.
Although, in order to translate a higher-lever programming language to binary code capable of running on a machine's processor, we need to design and build programs called compilers.

A compiler is a program that receive as input a program written in a \textit{source} language and outputs a program written in a \textit{target} language.
In our study the \textit{target} language is the binary code capable of running on a specific machine's processor, also called \textit{target machine language}.
During the translation between the \textit{source} and \textit{target} languages, the compiler goes through two major execution steps: the analysis step; and the synthesis step.

The analysis step, also called the compiler's \textit{front end}, may be further subdivided into: lexical analysis; syntax analysis; semantic analysis; and intermediate code generation.
Furthermore, the synthesis step, also called the compiler's \textit{back end}, may be further subdivided into: machine-independent code optimization; code generation; and machine-dependent code optimization.
By the end of the analysis step, we generate an intermediate code which may have no information about the higher-level notations of the source language.
Having only the information needed by the synthesis step to run optimization algorithms and output the target language.

The separation of the compiler in analysis, \textit{front end}, synthesis, \textit{back end}, and the use of an intermediate representation between them, is very useful because, if we wish to implement a compiler for the source language $i$ to the target machine language $j$, we can implement just the compiler's \textit{front end} for $i$ and use a proven working \textit{back end} for $j$.
Therefore, if we wish to implement compilers for $n$ different programming languages to $m$ different machine languages, we can avoid building $n \times m$ compilers building $n$ \textit{front ends} and $m$ \textit{back ends}.

The intermediate code representation used by this work is the \textit{LLVM intermediate representation} (LLVM-IR), proposed by the LLVM project~\cite{lattner2004llvm} together with its \textit{back end} libraries.

Although this work will not go further on the implementation of a compilers \textit{back end}, the analysis step done in the \textit{front end} is very much of our interest and we will explore it further on the following sections.

\subsection{Lexical Analysis}

The first phase of a compiler is called lexical analysis, or \textit{scanning}.
The lexical analyzer will read a stream of characters from the source program and group the characters into sequences called \textit{lexemes}.
For each lexeme, the lexical analyzer generate a \textit{token} with different meta-data depending on the token type.
For example, analysing the following program:
\begin{quote}
\begin{verbatim}
let a = b + 60 / c
\end{verbatim}
\end{quote}
A lexical analyzer may output the following token stream:
\begin{quote}\label{figure:introduction_token_stream}
\begin{verbatim}
<let> <id, a> <=> <id, b> <+> <number, 60> </> <id, c>
\end{verbatim}
\end{quote}
Where tokens are of the form \verb+<token-type, meta-data>+, when no meta-data is needed it can be omitted from the token notation.
We can see that this lexical analyzer decided to map the lexemes `let', `=', `+' and `/', to the respective tokens \verb+<let>+, \verb+<=>+, \verb-<+>- and \verb+</>+;
the lexemes `a', `b' and `c', to the respective tokens \verb+<id, a>+, \verb+<id, b>+, and \verb+<id, c>+, which all have the same token type `id' and carry as meta-data the lexeme that generated the mapping; and the lexeme `60', to the token \verb+<number, 60>+, which has token type `number' and has the its value of $60$ as meta-data.
\subsection{Syntax Analysis}

The second phase of a compiler is called syntax analysis, or \textit{parsing}.
The \textit{parser} then receives as input a token stream produced by the lexical analyzer and creates a tree-like intermediate representation that is constrained by a particular grammatical structure.

% We can then build a parser $p$ for the following gammar in BNF notation:
% \begin{quote}
% \begin{verbatim}
% <let-statement> ::= let id = <expr>
% <expr> ::= <term> + <expr> | <term>
% <term> ::= <term> / <factor> | <factor>
% <factor> ::= id | number
% \end{verbatim}
% \end{quote}
% where we define productions of the type $symbol ::= expression$

If we give the token stream from Section~\ref{figure:introduction_token_stream} as input to a parser it may output the tree structure found on Figure~\ref{figure:introduction_ast}.
This structure already has more information than the linear token stream it received as input.
For example, it is prepared in such a way to preserve the order of operations from classic arithmetic, the tree has an interior node labled $/$ with \verb+<number, 60>+ as its left child and \verb+<id, c>+ as its right child making it explicit that we must first divide $60$ by $c$ before evaluating the interior node labled $+$ which depends on the result of $/$ as its right child.

\begin{figure}
\caption{A possible parser output for the token stream from Section~\ref{figure:introduction_token_stream}}\label{figure:introduction_ast}
\centering
\begin{tikzpicture}
	\node (is-root) {=}
		[sibling distance=2cm]
		child { node {<id, a>} }
		child {
			node {+}
				[sibling distance=2cm]
        child { node {<id, b>} }
				child {
          node {/}
            [sibling distance=2.2cm]
            child { node {<number, 60>} }
            child { node {<id, c>} }
        }
		};
\end{tikzpicture}
\end{figure}

\subsection{Semantic Analysis}

The third phase of a compilers is called semantic analysis.
It is responsible to use the tree structure generated by the parser to check if the source program is semantically consistent with the language specification.
An important part of semantic analysis is \textit{type checking} were it will try to validate the program to the language specification's type system.

A type system can be described as: A tractable syntactic method for proving the absence of certain program behaviours by classifying phrases according to the kinds of values they compute~\cite{pierce2002types}.
Type systems used by popular modern programming languages such as C\#, Haskell, Java, OCaml, Rust and Scala are the most widespread tool used to guarantee the correct behaviour of a program.
They are mainly used to describe valid sets of values that can be used for different computations, so that the compiler can eliminate, during its execution, a variety of possible run-time errors durring the target program execution.
Although effective, well-typed programs do go wrong.


\section{Goals}\label{chapter:introduction:sec:goals}

Desenvolver uma linguagem de programação com refinement types.
Construir um compilador para tradução desta linguagem para LLVM-IR.\@
Explorar possiveis optimizações no LLVM-IR utilizando a semantica adicionada pelo sistema de tipos refinados.

\chapter{Background}\label{chapter:background}

In this chapter we will present prior work related to refinement types and llvm.

\chapter{Related Work}\label{chapter:related_work}

\chapter{TODO:\@ proposal}

\postextual{}
\bibliography{example}

\apendices{}

\end{document}

