%dica: use a opção oneside se houver um limite (e.g., 20) de páginas
\documentclass[
  english,
  lmodern,
  oneside
]{ufsc-thesis-rn46-2019/ufsc-thesis-rn46-2019}

\usepackage[T1]{fontenc} % fontes
\usepackage[utf8]{inputenc} % UTF-8
\usepackage{pdfpages} % Inclui PDF externo (ficha catalográfica)

\usepackage{tikz}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Configurações da classe (dados do trabalho)                  %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Preâmbulo
\titulo{Template \LaTeX{} seguindo a RN 46/2019/CPG da UFSC}
\autor{Bernardo Ferrari Mendonça}
\data{3 de Dezembro de 2020}
\instituicao{Universidade Federal de Santa Catarina}
\centro{Centro Tecnológico}
\local{Florianópolis}
\programa{Programa de Pós-Graduação em Ciência da Computação}
\tcc{}
\curso{Ciência da Computação}
\departamento{Departamento de Informática e Estatística}
\titulode{Bacharel em Ciência da Computação}

\orientador{Prof.\ Alcides Fonseca, Dr.}
\coorientador{Prof.\ Rafael de Santiago, Dr.}
\afiliacaoorientador{Universidade de Lisboa}
\afiliacaocoorientador{Universidade Federal de Santa Catarina}

\membrabanca{Prof\textsuperscript{a}.\ Jerusa Marchi, Dr.}{Universidade Federal de Santa Catarina}

\coordenador{Prof.\ José Francisco Fletes, Dr}

\begin{document}

\pretextual{}
\imprimircapa{}
\imprimirfolhaderosto*
\protect%\incluirfichacatalografica{ficha.pdf}
\imprimirfolhadecertificacao{}



\begin{dedicatoria}
  TODO:\@ dedicatoria
\end{dedicatoria}

\begin{agradecimentos}
  TODO:\@ agradecimentos
\end{agradecimentos}

\begin{epigrafe}
  TODO:\@ epigrafe
\end{epigrafe}


% 150 a 500 palavras.
% Mantatorio ser em Português, usando a voz ativa na terceira pessoa.
% Contexto; Problema; Estado da Arte; Solução proposta; Resultados.
\begin{resumo}[Resumo]
% Compiladores são os programas de computadores que fazem o papel de tradutor entre as linguagens de programação e as linguagens de maquina.
% Eles recebem como entrada um texto escrito em uma linguagem fonte, chamado de programa fonte, e produzem como saída um texto escrito em uma linguagem de maquina alvo, chamado de programa alvo.
% O processo de tradução executado por um compilador pode ser separado em duas grandes etapas: a etapa de analise e a etapa de síntese.
% Durante a etapa de analise, um sistema de tipos pode ser utilizado.
% O sistema de tipos classificara as diferentes sentenças de um programa quanto aos tipos dos valores que as sentenças computam com o objetivo de provar a ausencia de determinados comportamentos de programas do programa fonte.
% Um dos possiveis sistemas de tipos é chamado de sistema de tipos refinados, onde é anexado os tipos dos valores de um programa predicados lógicos que garamtem propriedades extras.
% Entre as etapas de analise e síntese pode se fazer o uso de uma linguagem intermediária.
% O uso de uma linguagem intermediária é especialmente vantajoso pois, um compilador para a linguagem $i$ e maquina $j$ pode ser construído combinando um \textit{front end} para $i$ e um \textit{back end} para $j$.
% O projeto LLVM é uma biblioteca de funcionalidades para otimização de código intermediário e geração de código alvo.
% Estas funcionalidades são construídas ao redor de uma linguagem intermediária chamada \textit{LLVM intermediate representation}, ou LLVM IR\@.
% O objetivo do trabalho consiste em propor o projeto de uma linguagem de programação que utilize um sistema de refinamento de tipos e a confecção de seu compilador utilizando o ecossistema de ferramentas LLVM\@.
% É esperada que a ferramenta consiga fazer a analise do código fonte utilizando o sistema de tipos refinados proposto e sua tradução para código de maquina alvo.
TODO: Resumo

\vspace{\onelineskip}
\noindent
\textbf{Palavras-chave}: linguagem de programação\@. representação intermediária de código\@. compilador\@. LLVM\@. LLVM IR\@.
\end{resumo}

% \begin{resumo}[Resumo Estendido]
%   \section*{Introdução}
%   A hifenização é alterada para \texttt{brazil}, mesmo para documentos em inglês. Descrever brevemente esses itens exigidos pela BU. Como a RN 95/CUn/2017 é mais recente e impõe outras regras a revelia de regimentos e regulamentos, é mais sábio obedecê-la. Lembre que esse resumo estendido deve term entre 2 e 5 páginas.
%
%   \lipsum[1]
%   \section*{Objetivos}
%   \lipsum[2]
%   \section*{Metodologia}
%   \lipsum[3]
%   \section*{Resultados e Discussão}
%   \lipsum[4]
%   \section*{Considerações Finais}
%   \lipsum[5]
%
%   \vspace{\baselineskip}  % Atenção! manter igual ao resumo
%   \textbf{Palavras-chave:} Palavra-chave. Outra Palavra-chave composta. Bla.
% \end{resumo}


\begin{abstract}
  TODO:\@ abstract
  Enlish version of the plain ``resumo'' above. Done with environment
  \texttt{abstract}. Hyphenization is automatically changed to english.

  \vspace{\baselineskip}
  \textbf{Keywords:} Keyword. Another Compound Keyword. Bla.
\end{abstract}

% \listoffigures*

% \listofalgorithms*

% \begin{listadesimbolos}
  % $\gets$   & Atribuição \\
  % $\exists$   & Quantificação existencial \\
  % $\rightarrow$   & Implicação \\
  % $\wedge$   & E lógico \\
  % $\vee$   & Ou lógico \\
  % $\neg$   & Negação lógica \\
  % $\mapsto$   & Mapeia para \\
  % $\sqsubseteq$   & Subclasse (em ontologias) \\
  % $\subseteq$   & Subconjunto: $\forall x\;.\; x \in A \rightarrow x \in B$ \\
  % $\langle\ldots\rangle$ & Tupla \\
  % $\forall$   & Quantificação universal \\
  % mmmmm & Nenhum sentido, apenas estou aqui para demonstrar a largura máxima dessas colunas. Ao abrir o ambiente \texttt{listadesimbolos}, pode-se fornecer um argumento opcional indicando a largura da coluna da esquerda (o default é de 5em): \texttt{\textbackslash{}begin\{listadesimbolos\}[2cm] .... \textbackslash{}end\{listadesimbolos\}} \\
% \end{listadesimbolos}

\tableofcontents*

\textual%

\chapter{Introduction}\label{chapter:introduction}

This work deals with the problem of translating a programming language with refinement types to the intermediate representation language LLVM-IR.\@
Throughout the work, we will be presenting: the design of a small programming language; an operational semantics with refinement types for the designed language's type system; and a compiler implementation for the designed language to LLVM-IR.\@
In this introductory chapter we will explore the motivations behind this work and its goals.

\section{Motivation}\label{chapter:introduction:sec:motivation}

Programming languages are notations to describe computations to people and machines.
This notations can take numerous forms, from binary code, ready to be executed by a specific machine, to a higher-level language, capable of expressing more abstract notations such as functions and types.
The use of higher-level programming languages eases how people can describe computations to one another.
Their abstractions are capable of describing computations without needing to expose details from the machine that will execute them.
Although, in order to translate a higher-lever programming language to binary code capable of running on a machine's processor, we need to design and build programs called compilers.

A compiler is a program that receive as input a program written in a \textit{source} language and outputs a program written in a \textit{target} language.
In our study the \textit{target} language of interest is the binary code capable of running on a specific machine's processor, also called \textit{target machine language}.
During the translation between the \textit{source} and \textit{target} languages, the compiler goes through two major execution steps: the analysis step; and the synthesis step.

The analysis step, also called the compiler's \textit{front end}, may be further subdivided into: lexical analysis; syntax analysis; semantic analysis; and intermediate code generation.
Furthermore, the synthesis step, also called the compiler's \textit{back end}, may be further subdivided into: machine-independent code optimization; code generation; and machine-dependent code optimization.
By the end of the analysis step, we generate an intermediate code which may have no information about the higher-level notations of the source language.
Having only the information needed by the synthesis step to run optimization algorithms and output the target language.

The separation of the compiler in analysis, \textit{front end}, synthesis, \textit{back end}, and the use of an intermediate representation between them, is very useful because, if we wish to implement a compiler for the source language $i$ to the target machine language $j$, we can implement just the compiler's \textit{front end} for $i$ and use a proven working \textit{back end} for $j$.
Therefore, if we wish to implement compilers for $n$ different programming languages to $m$ different machine languages, we can avoid building $n \times m$ compilers building $n$ \textit{front ends} and $m$ \textit{back ends}.

The intermediate code representation used by this work is the \textit{LLVM intermediate representation} (LLVM-IR), proposed by the LLVM project~\cite{lattner2004llvm} together with its \textit{back end} libraries.

Although this work will not go further on the implementation of a compilers \textit{back end}, the analysis step done in the \textit{front end} is very much of our interest and we will explore it further on the following sections.

\subsection{Lexical Analysis}

The first phase of a compiler is called lexical analysis, or \textit{scanning}.
The lexical analyzer will read a stream of characters from the source program and group the characters into sequences called \textit{lexemes}.
For each lexeme, the lexical analyzer generate a \textit{token} with different meta-data depending on the token type.
For example, analysing the following program:
\begin{quote}
\begin{verbatim}
let a = b + 60 / c
\end{verbatim}
\end{quote}
A lexical analyzer may output the following token stream:
\begin{quote}\label{figure:introduction_token_stream}
\begin{verbatim}
<let> <id, a> <=> <id, b> <+> <number, 60> </> <id, c>
\end{verbatim}
\end{quote}
Where tokens are of the form \verb+<token-type, meta-data>+, when no meta-data is needed it can be omitted from the token notation.
We can see that this lexical analyzer decided to map the lexemes `let', `=', `+' and `/', to the respective tokens \verb+<let>+, \verb+<=>+, \verb-<+>- and \verb+</>+;
the lexemes `a', `b' and `c', to the respective tokens \verb+<id, a>+, \verb+<id, b>+, and \verb+<id, c>+, which all have the same token type `id' and carry as meta-data the lexeme that generated the mapping; and the lexeme `60', to the token \verb+<number, 60>+, which has token type `number' and has the its value of $60$ as meta-data.
\subsection{Syntax Analysis}

The second phase of a compiler is called syntax analysis, or \textit{parsing}.
The \textit{parser} then receives as input a token stream produced by the lexical analyzer and creates a tree-like intermediate representation that is constrained by a particular grammatical structure.

% We can then build a parser $p$ for the following gammar in BNF notation:
% \begin{quote}
% \begin{verbatim}
% <let-statement> ::= let id = <expr>
% <expr> ::= <term> + <expr> | <term>
% <term> ::= <term> / <factor> | <factor>
% <factor> ::= id | number
% \end{verbatim}
% \end{quote}
% where we define productions of the type $symbol ::= expression$

If we give the token stream from Section~\ref{figure:introduction_token_stream} as input to a parser it may output the tree structure found on Figure~\ref{figure:introduction_ast}.
This structure already has more information than the linear token stream it received as input.
For example, it is prepared in such a way to preserve the order of operations from classic arithmetic, the tree has an interior node labled $/$ with \verb+<number, 60>+ as its left child and \verb+<id, c>+ as its right child making it explicit that we must first divide $60$ by $c$ before evaluating the interior node labled $+$ which depends on the result of $/$ as its right child.

\begin{figure}
\caption{A possible parser output for the token stream from Section~\ref{figure:introduction_token_stream}}\label{figure:introduction_ast}
\centering
\begin{tikzpicture}
	\node (is-root) {=}
		[sibling distance=2cm]
		child { node {<id, a>} }
		child {
			node {+}
				[sibling distance=2cm]
        child { node {<id, b>} }
				child {
          node {/}
            [sibling distance=2.2cm]
            child { node {<number, 60>} }
            child { node {<id, c>} }
        }
		};
\end{tikzpicture}
\end{figure}

\subsection{Semantic Analysis}

The third phase of a compilers is called semantic analysis.
It is responsible to use the tree structure generated by the parser to check if the source program is semantically consistent with the language specification.
An important part of semantic analysis is \textit{type checking} where it will try to validate the program to the language specification's type system.

If we feed a type checker with the tree structure presented on Figure~\ref{figure:introduction_ast}.
It may make use of a symbol table containing the type information of the identifiers `b' and `c' to type check all nodes of the tree for consistency.
Suppose a symbol table that maps the identifier `c' to the type `boolean', written `\verb+c: boolean+', which would compose of the values `true' and `false'.
With this information, the type checker would be able to decide if the operation `\verb+60 / c+' is semantically sound to the language's type system.
Does the language type system specifies as valid to divide a number by a boolean?
If it does, what does it mean to divide the value 60 by `false'?
Maybe, whoever designed the language decided that if the dividend is a value of type number and the divisor is a value of type boolean, it will use some rule to convert the divisor's type to number in order to keep the program semantically sound.
Maybe, the language's type system forbids this behaviour and will halt the compilation process with some error message.
This are decisions made when building a language's type system.

A type system can be described as: A tractable syntactic method for proving the absence of certain program behaviours by classifying phrases according to the kinds of values they compute~\cite{pierce2002types}.
Type systems used by popular modern programming languages such as C\#, Haskell, Java, OCaml, Rust and Scala are the most widespread tool used to guarantee the correct behaviour of a program.
They are mainly used to describe valid sets of values that can be used for different computations, so that the compiler can eliminate, during its execution, a variety of possible run-time errors during the target program execution.
Although effective, well-typed programs do go wrong~\cite{jhala2020refinement}.

\begin{itemize}
  \item \textbf{Division by zero:} Constraining the types of the division operation to `int' does not protect the program to execute a division by zero at run-time.
Also, it does not guarantee that the arithmetic operations will not under- or over-flow.
  \item \textbf{Buffer overflow:} Constraining the index of the access of a `array' or `string' to `int' does not protect the program to try to access data from beyond the data structure's end.
  \item \textbf{Logic bugs:} The type system of modern languages allow for the creation of custom types like \verb+structure date { day: int, month: int, year: int}+, where its values are compound of three `int' values for day, month and year.
But the type `date' can not guarantee that the value for day will be valid for a certain value of month and year.
  \item \textbf{Correctness errors:} The type system can guarantee that a sorting procedure will produce a value of type list  but can not guarantee that the list was, in fact, sorted.
\end{itemize}

A type system can be extended to further \textit{refine} its types with logic predicates.
This method is called \textit{Refinement types with predicates}.
It allows programmers to constrain existing types by using predicates to assert desired properties of the values they want to describe.
For example, while `int' types can assume any integer values, we can write the refined type
\begin{quote}
\begin{verbatim}
type nat = int[v | 0 <= v]
\end{verbatim}
\end{quote}
where `nat' will only be able to assume positive integer values.
Alone, this refinements may seam a just a gimmick, but combined with function the programmer can describe precise contracts that describe the functions legal inputs and outputs.
For example, the author of a array library may write the functions
\begin{quote}
\begin{verbatim}
val size: x: array(a) -> nat[v | v = length(x)]
val get: x: array(a) -> nat[v | v < length(x)] -> a
\end{verbatim}
\end{quote}
which guarantees that a call to \verb+size(arr)+ returns a value with the exact length of \verb+arr+, and that a call to \verb+get(arr, i)+ requires the index \verb+i+ to be within the bounds of \verb+arr+.
Given this definitions the refinement type checker can then prove, during the analysis phase (i.e.\ at compile-time), that the contracts of both \verb+size+ and \verb+get+ will not be violated, ensuring all array access to be sate when executing the target program (i.e.\ at run-time).

Refinement types offer the option to add information to the type system about the invariants and correctness properties a programmer may care about.
It is done in such a way that, if the programmer desires, no refinement needs to be added and can be thought like a typical type system.
On the other hand, programmers can incrementally add refinements to ensure important properties about the source program.
They could begin with basic safety requirements, e.g.\ eliminating division by zero and buffer overflow, or guarantee that a function does not receive a empty collection, and then incrementally add to the specification invariants of custom data types.
Ultimately going all the way to specifying and verifying the correctness of different procedures at compile-time.
By enabling verification on the same language as the programming language, refinement types bridge implementation and prof together.
This approach creates a development cycle were the implementation hits programmers to what properties are important to verify, and the verification hits on how the implementation can be restructured to better express the invariants and enable formal proof.

\section{Goals}\label{chapter:introduction:sec:goals}

After exploring the motivations behind the use of an intermediate language like LLVM-IR and the use of type checkers with refinement types in the compiler's analysis phase.
We are ready to discuss the objectives of this work.
They are:
\begin{itemize}
    \item Design a programming language;
    \item Design the programming language's type system with refinement types using operational semantics;
    \item Implement a compiler's front end including a lexical analyzer, parser and a semantic analyzer with a type checker for the designed language;
    \item Use the LLVM libraries to generate LLVM-IR\@ and explore possible optimizations brought by refinement types during intermediate code generation.
\end{itemize}

Designing a new language is particularly advantageous because we have full control of all the language features being implemented and allow us to build the language, the refinement type system, and the intermediate code generation, one feature at a time.

\chapter{Background}\label{chapter:background}

In this chapter we will present the background needed to build a compiler and create an operational semantic for the refinement type system.

\chapter{Related Work}\label{chapter:related_work}

In this chapter we will present prior work related to refinement types and llvm.

\chapter{TODO:\@ proposal}

In this chapter we will present the built language and type system.

\postextual{}
\bibliography{example}

\apendices{}

\end{document}

