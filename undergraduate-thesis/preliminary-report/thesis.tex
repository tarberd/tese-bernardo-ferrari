\documentclass[
  oneside,
  english,
  coorientadorbanca,
  noabntexcite
]{ufsc-thesis-rn46-2019}

% font config
\usepackage{fontspec}

\usepackage{polyglossia}
\setdefaultlanguage{english}
\setotherlanguages{portuguese}

% math and code typesetting
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{mathpartir} % typesetting inference rules
\usepackage[outputdir=build, newfloat]{minted}

% graphics
\usepackage{pdfpages} % including pdf files
\usepackage{tikz}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{positioning}
\usetikzlibrary{trees}

% text revisions
\usepackage{paper_alcides}

% citation and references
\usepackage{hyperref} % links
\usepackage{csquotes} % Quoting authors
\usepackage{caption} % configures hyperref to point to top of images
\usepackage{subcaption} % adds subfigures

% bibliography
\usepackage[style=abnt]{biblatex}
\addbibresource{bibliography/llvm.bib}
\addbibresource{bibliography/pratt.bib}
\addbibresource{bibliography/refinements.bib}
\addbibresource{bibliography/thesis.bib}

% commands
\setmainfont{lmroman10}[
  Path=./fonts/lm/,
  OpticalSize=18,
  Scale=MatchLowercase,
  LetterSpace=1.0,
  UprightFont=*-regular,
  BoldFont=*-bold,
  ItalicFont=*-italic,
  BoldItalicFont=*-bolditalic,
  FontFace = {m}{\shapedefault}{*-regular},
  FontFace = {m}{it}{*-italic},
  FontFace = {b}{\shapedefault}{*-bold},
  FontFace = {b}{it}{*-bolditalic},
]

\setsansfont{lmsans10}[
  Path=./fonts/lm/,
  OpticalSize=18,
  LetterSpace=1.0,
  Scale=MatchLowercase,
  UprightFont=*-regular,
  BoldFont=*-bold,
  ItalicFont=*-oblique,
  BoldItalicFont=*-boldoblique,
  FontFace = {m}{\shapedefault}{*-regular},
  FontFace = {m}{sl}{*-oblique},
  FontFace = {b}{\shapedefault}{*-bold},
  FontFace = {b}{sl}{*-boldoblique},
]

\setmonofont{lmmonolt10}[%
  Path=./fonts/lm/,
  OpticalSize=18,
  LetterSpace=1.0,
  Scale=MatchLowercase,
  UprightFont=*-regular,
  BoldFont=*-bold,
  ItalicFont=*-oblique,
  BoldItalicFont=*-boldoblique,
  FontFace = {m}{\shapedefault}{*-regular},
  FontFace = {m}{sl}{*-oblique},
  FontFace = {b}{\shapedefault}{*-bold},
  FontFace = {b}{sl}{*-boldoblique}%,
]

\newfontfamily\scpfamily{SourceCodePro}[
  NFSSFamily=sourcecodepro,
  Path=./fonts/sourcecodepro/,
  OpticalSize=18,
  Scale=MatchLowercase,
  UprightFont=*-Medium,
  ItalicFont=*-MediumIt,
  BoldFont=*-Bold,
  BoldItalicFont=*-BoldIt,
  FontFace = {el}{\shapedefault}{*-ExtraLight},
  FontFace = {el}{it}{*-ExtraLightIt},
  FontFace = {l}{\shapedefault}{*-Light},
  FontFace = {l}{it}{*-LightIt},
  FontFace = {sl}{\shapedefault}{*-Regular},
  FontFace = {sl}{it}{*-It},
  FontFace = {m}{\shapedefault}{*-Medium},
  FontFace = {m}{it}{*-MediumIt},
  FontFace = {sb}{\shapedefault}{*-Semibold},
  FontFace = {sb}{it}{*-SemiboldIt},
  FontFace = {b}{\shapedefault}{*-Bold},
  FontFace = {b}{it}{*-BoldIt},
  FontFace = {ub}{\shapedefault}{*-Black},
  FontFace = {ub}{it}{*-BlackIt},
]

\newminted{rust}{
  fontfamily=sourcecodepro,
  breaklines,
  linenos,
  fontsize=\footnotesize
}

\newmintedfile[rustfile]{rust}{
  fontfamily=sourcecodepro,
  breaklines,
  linenos,
  fontsize=\footnotesize
}

\newcommand\myflowchartlowermargin{1.2cm}

\def\bnfdef{::=}
\newcommand{\codett}[1]{\text{\scpfamily#1}}
\newcommand{\code}[1]{\text{\scpfamily\setlength\spaceskip{0.35em}#1}}
\newcommand{\codem}[1]{$\code{#1}$}
\newcommand{\bnfvar}[1]{\codett{#1}}
\newcommand{\bnfter}[1]{\textrm{`}\codett{#1}\textrm{'}}
\newcommand{\bnfor}[1]{{$\mid$} #1}
\newcommand{\bnfprod}[2]{\bnfvar{#1} & \ & \bnfdef{} & \ \code{#2}}
\newcommand{\bnfmore}[1]{            & \ & \mid{}    & \ \code{#1}}
\newcommand{\bnfcont}[1]{            & \ &           & \ \code{#1}}

\newcommand{\token}[1]{$\langle\codett{#1}\rangle$}

\newcommand{\astprod}[2]{\code{#1} & \ & \bnfdef{} & \ \code{#2}}
\newcommand{\astmore}[1]{\bnfmore{#1}}

\newcommand{\typer}[2]{\code{#1 \codett{:} #2}}
\newcommand{\ctxtr}[2]{\code{{$\Gamma$}#1 {$\vdash$} #2}}
\newcommand{\ctxtre}[2]{\code{#1 {$\vdash$} #2}}
\newcommand{\typecxr}[3]{\ctxtr{#1}{\typer{#2}{#3}}}
\newcommand{\typecxre}[3]{\code{#1 {$\vdash$} \typer{#2}{#3}}}
\newcommand{\typecx}[2]{,\;\typer{#1}{#2}}

\newcommand{\subtyr}[2]{\code{#1 $\prec :$ #2}}
\newcommand{\subtycxr}[3]{\ctxtr{#1}{\subtyr{#2}{#3}}}

\newcommand{\synthr}[2]{\code{#1 $\Rightarrow$ #2}}
\newcommand{\synthcxr}[3]{\ctxtr{#1}{\synthr{#2}{#3}}}

\newcommand{\checkr}[2]{\code{#1 $\Leftarrow$ #2}}
\newcommand{\checkcxr}[3]{\ctxtr{#1}{\checkr{#2}{#3}}}

\newcommand{\evalr}[2]{\code{#1 $\rightarrow$ #2}}

\newcommand{\fnty}[2]{\code{#1 $\rightarrow$ #2}}
\newcommand{\fnabs}[3]{\code{$\lambda$\typer{#1}{#2}.#3}}
\newcommand{\astlet}[3]{\code{let #1 = #2 in #3}}

\newcommand{\implcons}[3]{\code{$($\code{#1} $::$ #2$)$ $\implies$ #3}}

\newcommand{\tokenstream}[4]{
  & \langle\text{#1,} && \text{\{\textit{lexeme}: #2,} && \text{\textit{begin}: #3,} && \text{\textit{end}: #4\}}\rangle%
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Configurações da classe (dados do trabalho)                  %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Informações para capa e folha de rosto/certificacao

% Caso o título contenha alguma porção LaTeX ilegível, defina um título
% alternativo opcional com []'s para ser usado no campo Title do PDF
% IMPORTANTE: Os títulos deveriam ser iguais. Apenas use um título
% alternativo se o título não puder ser expresso com letras e números
\titulo{A programming language with refinement types and its LLVM-IR front end implementation.}

\autor{Bernardo Ferrari Mendonça}
\data{24 de Fevereiro de 2022} % ou \today
\instituicao{Universidade Federal de Santa Catarina}
\centro{Centro Tecnológico}
\local{Florianópolis} % Apenas cidade! Sem estado
\programa{Programa de Graduação em Ciência da Computação}
% Os dois próximos itens são usados para gerar o \preambulo
%\tese % ou \dissertacao ou \tcc
%\titulode{doutor em Ciência da Computação}

%%% Atenção! No caso de TCC, além de usar \tcc, outros comandos devem ser fornecidos:
%%%
\tcc{}
\departamento{Departamento de Informática e Estatística}
\curso{Ciência da Computação}
\titulode{Bacharel em Ciência da Computação}
% %% Para TCCs, orientadores e coorientadores podem ser externos, logo a
% %% BU exige que sua afiliação seja explicitada. Por padrão, assume-se
% %% UFSC. Você pode alterar a afiliação com os comandos abaixo:

% Orientador, coorientador, membros da banca e coordenador
% As regras da BU agora exigem que Dr. apareça **depois** do nome
% Dica: para gerar Profᵃ. use Prof\textsuperscript{a}.
% Dica 2: para feminino use \orientadora e \coorientadora
\orientadorext{Prof.\ Alcides Miguel Cachulo Aguiar Fonseca, Dr.}{Universidade de Lisboa}
\coorientadorext{Prof.\ Rafael de Santiago, Dr.}{Universidade Federal de Santa Catarina}
\membrabanca{Prof\textsuperscript{a}. Jerusa Marchi, Dr.}{Universidade Federal de Santa Catarina}
% Dica: se feminino, \coordenadora
\coordenador{Prof.\ Jean Everson Martina, Dr.}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Principais elementos pré-textuais                            %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Inicia parte pré-textual do documento capa, folha de rosto, folha de
% aprovação, aprovação, resumo, lista de tabelas, lista de figuras, etc.
\pretextual{}
\imprimircapa{}
\imprimirfolhaderosto*
% Atenção! \cleardoublepages são inseridos automaticamente
% Atenção! esse \protect é importante
\protect{}%\incluirfichacatalografica{ficha.pdf}
\imprimirfolhadecertificacao{}

% Listas de "coisas". O * no final faz com que as listas não sejam
% incluídas como entratas do sumário (\tableofcontents)

% \listoftables*
% \listofalgorithms*
% \listoffigures*
% \tableofcontents*

\begin{dedicatoria}
  This work is dedicated to my mother, Marcela Ferrari, my sister Camila Ferrari, and to my grandparents Márcio Ferrari and Cláudia Ferrari who plowed the field where I bloom.
\end{dedicatoria}

\begin{agradecimentos}
  TODO ack
\end{agradecimentos}

\begin{epigrafe}
  TODO epigrafe
\end{epigrafe}

\begin{resumo}[Resumo]
  TODO Resumo

  % Atenção! a BU exige separação através de ponto (.). Ela recomanda de 3 a 5 keywords
  \vspace{\baselineskip}
  \textbf{Palavras-chave:} linguagem de programação\@. tipos refinados\@. representação intermediária de código\@. compilador\@. LLVM\@. LLVM-IR\@.
\end{resumo}

\begin{abstract}
  TODO abstract

  \vspace{\baselineskip}
  \textbf{Keywords:} Keyword. Another Compound Keyword. Bla.
\end{abstract}

\listoffigures*  % O * evita que apareça no sumário

% \begin{listadesimbolos}
%   \($1\)   & Atribuição \\
%   \($1\)   & Quantificação existencial \\
%   $\rightarrow$   & Implicação \\
%   $\wedge$   & E lógico \\
%   $\vee$   & Ou lógico \\
%   $\neg$   & Negação lógica \\
%   $\mapsto$   & Mapeia para \\
%   $\sqsubseteq$   & Subclasse (em ontologias) \\
%   $\subseteq$   & Subconjunto: $\forall x\;.\; x \in A \rightarrow x \in B$ \\
%   $\langle\ldots\rangle$ & Tupla \\
%   $\forall$   & Quantificação universal \\
%   mmmmm & Nenhum sentido, apenas estou aqui para demonstrar a largura máxima dessas colunas. Ao abrir o ambiente \texttt{listadesimbolos}, pode-se fornecer um argumento opcional indicando a largura da coluna da esquerda (o default é de 5em): \texttt{\textbackslash{}begin\{listadesimbolos\}[2cm] .... \textbackslash{}end\{listadesimbolos\}} \\
% \end{listadesimbolos}

\tableofcontents*

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Corpo do texto                                               %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\textual{}

\chapter{Introduction}\label{chapter:introduction}

As stated by~\textcite{Aho:2006:CPT:1177220}, \textquote{Programming languages are notations to describe computations to people and to machines}.
These notations can take numerous forms.
They range from lower-level languages, such as machine code ready to be executed by a specific machine, to a higher-level language, such as C, Java, Rust, Haskell and Ml.
Lower-level languages like machine code are very verbose in how they describe computations; usually describing directly to the machine when and how to execute each computation through simple notations like: add the values from two data locations, compare two values, jump the next 4 instructions, and so on~\cite{Aho:2006:CPT:1177220}.
Whereas, in a higher-level language, we can describe computations in a more abstract set of notations, such as functions and types, without needing to expose details from the specific machine that will execute them~\cite{Aho:2006:CPT:1177220}.
Hence, using higher-level programming languages eases how people can describe computations to machines and to one another~\cite{Aho:2006:CPT:1177220}.
However, in order to translate a higher-lever programming language to machine code capable of running on a machine's processor, we need to design and build programs called compilers~\cite{Aho:2006:CPT:1177220}.

A compiler is a program that receives as input a program written in a \textit{source} language and translates it to a semantically equivalent program written in a \textit{target} language
~\cite{Aho:2006:CPT:1177220}.
This is what enables us to write programs in higher-level languages that are able to execute at a machine's processor.
A program written with a higher-level language such as C is fed into a compiler like Clang, then Clang translates the provided source program to a program in a target language, like Intel's x86 processor's machine code, ready to be executed.
During the translation process between the \textit{source} and \textit{target} languages, the compiler goes through two major execution steps: the analysis step, and the synthesis step~\cite{Aho:2006:CPT:1177220}.
The analysis step, called the compiler's \textit{front end}, organizes the information included in the source program into a grammatical structure, and then uses this grammatical structure, together with some metadata collected during its construction, to build what is called an intermediate representation~\cite{Aho:2006:CPT:1177220}.
Furthermore, the synthesis step, called the compiler's \textit{back end}, uses this intermediate representation to compute the desired target program~\cite{Aho:2006:CPT:1177220}.

Though it is possible to build a compiler that translates directly to a target machine code, this hinders portability and modularity~\cite{appel2003modern}.
Suppose we wish to implement a compiler for the source language $i$ to the target machine language $j$, we can implement just the compiler's \textit{front end} for $i$ and use a proven working \textit{back end} for $j$~\cite{Aho:2006:CPT:1177220}.
Therefore, if we wish to implement compilers for $n$ different programming languages to $m$ different machine languages, we can avoid building $n \times m$ compilers building $n$ \textit{front ends} and $m$ \textit{back ends}~\cite{Aho:2006:CPT:1177220}.

If we give the analysis-synthesis model of a compiler a more fine-grained look, we can identify that the compiler's front and back end operate as a series of phases, each one transforming one intermediate representation to another in order to further advance the computation of the target program~\cite{Aho:2006:CPT:1177220}.
The analysis step, or front end, may be subdivided into: a lexical analyzer, a syntax analyzer, a semantic analyzer and an intermediate code generator; also, the synthesis step may be subdivided into: machine-independent code optimization, code generation, and machine-dependent code optimization~\cite{Aho:2006:CPT:1177220}.
The different phases and the intermediate representations between them can be seen at Figure~\ref{figure:compilation-phases}.

\begin{figure}[!ht]
  \centering
  \begin{tikzpicture}[
    >={Latex[width=2mm, length=2mm]},
    ir/.style = {
        fill=white
      },
    phase/.style = {
        rectangle,
        rounded corners,
        minimum width=7.2cm,
        minimum height=1cm,
        text centered,
        draw=black,
      },
    ]
    \node (start) [ir] {source program};
    \node (lexical) [phase, below=\myflowchartlowermargin/2 of start] {Lexical Analyzer};
    \node (syntax) [phase, below=\myflowchartlowermargin of lexical] {Syntax Analyzer};
    \node (semantic) [phase, below=\myflowchartlowermargin of syntax] {Semantic Analyzer};
    \node (intermediate) [phase, below=\myflowchartlowermargin of semantic] {Intermediate Code Generation};
    \node (opt ind) [phase, below=\myflowchartlowermargin of intermediate] {Machine-Independent Code Optimizer};
    \node (codegen) [phase, below=\myflowchartlowermargin of opt ind] {Code Generator};
    \node (opt dep) [phase, below=\myflowchartlowermargin of codegen] {Machine-Dependent Code Optimizer};
    \node (end) [below=\myflowchartlowermargin of opt dep] {};

    \node (frontend) [right=2cm of syntax] {front end};
    \node (backend) [right=2cm of codegen] {back end};

    \draw [->] (start) -- (lexical);
    \draw [->] (lexical) -- node [ir] {token stream} (syntax);
    \draw [->] (syntax) -- node [ir] {syntax tree} (semantic);
    \draw [->] (semantic) -- node [ir] (syntax tree) {syntax tree} (intermediate);
    \draw [->] (intermediate) -- node [ir] {intermediate representation} (opt ind);
    \draw [->] (opt ind) -- node [ir] {intermediate representation} (codegen);
    \draw [->] (codegen) -- node [ir] {target-machine code} (opt dep);
    \draw [->] (opt dep) -- node [ir] {target-machine code} (end);
    \draw (lexical.east) -| (frontend.north);
    \draw (intermediate) -| (frontend);
    \draw (opt ind.east) -| (backend.north);
    \draw (opt dep.east) -| (backend);
  \end{tikzpicture}
  \caption{Phases of a compiler and the intermediate representations between them. Adapted from~\textcite{Aho:2006:CPT:1177220}.}\label{figure:compilation-phases}
\end{figure}

According to \textcite{appel2003modern}, \textquote{An intermediate representation (IR) is a kind of abstract machine language that can express the target-machine operations without committing to too much machine-specific detail}.
The authors continue by adding that the IR \textquote{is also independent of the details of the source language}~\cite{appel2003modern}.
This means that the abstract notations exclusive to a higher-level language are handled by the front end of a compiler so that, by the time the source program is transformed into the IR, the computations described by the IR are semantic equivalent to the computations described by the source program but are now in the notations of an abstract machine language.
Although the semantics of the computations are the same, there may be semantics in the higher-level language that are not present in the IR.\@
The front end of a compiler is then responsible to check if all semantic aspects of the source program are sound to the source language specifications before the generation of the IR~\cite{Aho:2006:CPT:1177220}.
The authors explain that the checks made during compilation are called \textit{Static Checks}, and they are not only capable of assuring that the source program can be successfully compiled, but have the potential to catch programming errors early, before the program can be executed~\cite{Aho:2006:CPT:1177220}. One of the static checks executed during compilation is \textit{type checking} and is part of the semantic analysis phase of the compiler's front end~\cite{appel2003modern}.

The type checking executed during the semantic analysis phase is designed in accordance with the source language's type system.
\textcite{pierce2002types} defines a type system as: \textquote{A tractable syntactic method for proving the absence of certain program behaviors by classifying phrases according to the kinds of values they compute}.
Hence, a phrase written with higher-level notations such as
\begin{equation}\label{eq:type_example1}
  \codett{x * 30}
\end{equation}
may be classified to compute a value of type \codett{Int}, written
\begin{equation*}
  \typer{x * 30}{Int}
\end{equation*}
meaning that~\eqref{eq:type_example1} is a phrase that computes a mathematical integer value.
As a counter example, if defined by the type system that the multiplication between a value of type \codett{Bool} and a value of type \codett{Int} was not allowed, and we had both classifications
\begin{equation*}
  \typer{x}{Bool}
  \qquad
  \text{and}
  \qquad
  \typer{30}{Int}
\end{equation*}
the phrase in~\eqref{eq:type_example1} would be semantically unsound and should be indicated as a type error by the type checker.

According to \textcite{jhala2020tutorial}, type systems are mainly used to describe valid sets of values that can be used for different computations so that the compiler can eliminate a variety of possible run-time errors before the target program execution.
Type systems like the ones used by popular modern programming languages, such as C\#, Haskell, Java, OCaml, Rust and Scala, have similar kinds of rules and are the most widespread tool used to guarantee the correct behavior of a program~\cite{jhala2020tutorial}.
\textcite{jhala2020tutorial} affirm that, although type systems are widespread and effective, well-typed programs do go wrong.
The authors elaborate a few wrong behaviors that are common to the most popular type systems. Within them, we have:
\begin{itemize}
  \item \textbf{Division by zero:} Constraining the types of the division operation to \codett{Int} does not protect the program to execute a division by zero at run-time, and it does not guarantee that the arithmetic operations will not under- or over-flow~\cite{jhala2020tutorial}.
  \item \textbf{Buffer overflow:} Constraining the index of the access of an \codett{Array} or \codett{String} to \codett{Int} does not protect the program to try to access data from beyond the data structure's end~\cite{jhala2020tutorial}.
\end{itemize}

An effort can be made while designing a type system so that they can further restrict the values of certain types.
We can extend a type system to further \textit{refine} its types with logic predicates and this method is called \textit{Refinement types with predicates}~\cite{jhala2020tutorial}.
It allows programmers to constrain existing types by using predicates to assert desired properties of the values they want to describe~\cite{jhala2020tutorial}.
For example, while \codett{Int} types can assume any integer values, we can write the refined type
\begin{equation*}
  \codett{type Nat = \{v:Int | 0 <= v\}}
\end{equation*}
where the newly defined type \codett{Nat} will only be able to assume positive integer values.
Alone, this refinements may seam just a gimmick, but combined with functions the programmer can describe precise contracts that describe the functions legal inputs and outputs~\cite{jhala2020tutorial}.
For example, the author of an \codett{array} library may specify the functions signatures types
\begin{equation*}
  \begin{aligned}
     & \codett{fn size: x:array(a) -> \{v:Nat | v = length(x)\}}      \\
     & \codett{fn  get: x:array(a) -> \{v:Nat | v < length(x)\} -> a} \\
  \end{aligned}
\end{equation*}
where \codett{size} and \codett{get} are functions and \codett{x} is the name of the first argument.
In this type system, a call to \code{size(arr)} returns a value $s$ of type \codett{Nat} constrained to a single value equal to the length of \codett{arr}; hence, the type of $s$ constrains $s$ to the exact length of \codett{arr}.
Furthermore, a call to \code{get(arr, i)} requires the index \codett{i} to be within the bounds of \codett{arr}.
Given these definitions, the refinement type checker can then prove, during the analysis phase (i.e.,\ at compile-time), that the contracts of both \codett{size} and \codett{get} will not be violated, ensuring all array access to be sated when executing the target program (i.e.,\ at run-time)~\cite{jhala2020tutorial}.

\section{Motivation and research problem}

There is an increasing number of uses of refinement types being implemented on top of existing languages.
For example: the work of~\textcite{vazou2014liquidhaskell} presenting LiquidHaskell as refinement types for the Haskell language; the work of~\textcite{vekris2016refinementtypescript} integrating refinement types for the TypeScript language; the work of~\textcite{sammler2021refinedc} integrating refinement types for the C language; and, the work of~\textcite{vazou2018refinementruby} integrating refinement types for the Ruby language.

Although refinement types have been proved useful in improving the static checking capabilities of higher level languages, there is a lack of research on bringing refinement types to the intermediate code generation phase of a compiler's front end.
Hence, we state the research problem in the form of the question: What can we discover when we add refinement types to the intermediate code generation phase of a compiler's front end?

\section{Goals}\label{chapter:introduction:sec:goals}

The primary goal of this work is to discuss and validate the following thesis: A front-end for a higher-level language with refinement types can make use of refinement types to allow optimizations opportunities during intermediate code generation not present in higher-level languages without refinement types.

For allowing this discussion, we will be designing a language with refinement types called \textit{Ekitai} and implementing its front end.
Designing a new language is particularly advantageous because we have full control of all the language features being implemented and allow us to incrementally design the language, the refinement type system, and the intermediate code generation, one feature at a time.

\subsection{Specific Goals}\label{chapter:introduction:sec:goals:specific_goals}
The specific research artifacts constructed by this work that allow the discussion of the thesis are:
\begin{itemize}
  \item The specification of \textit{Ekitai's} lexical elements;
  \item The specification of \textit{Ekitai's} syntax;
  \item The specification of \textit{Ekitai's} type system with refinement types;
  \item The implementation of \textit{Ekitai's} front end including:
        \begin{itemize}
          \item A lexical analyzer;
          \item A syntactic analyzer;
          \item A semantic analyzer with a type checker;
          \item An intermediate code generator;
        \end{itemize}
  \item The implementation of optimizations during intermediate code generation.
\end{itemize}

\section{Methodology}

The aim of this work is to find optimizations opportunities during intermediate code generation of a higher-level language with refinement types.
In order to achieve the specific goals specified in Section~\ref{chapter:introduction:sec:goals:specific_goals} we will employ different methods during research.

In the specification of the \textit{Ekitai} programming language aspects we will employ techniques from the established literature, such as books and articles, on language design and compiler construction.
Also, in order to specify the type system with refinement types we analyze the recent publications about refinement types in the ACM Sigplan's conferences and journals.
Whereas in the implementation of \textit{Ekitai's} front end we will develop a front end using the Rust programming language employing techniques from the established literature, such as books and articles, and from open-source implementations of modern industry programming language compilers and tools.

\section{Structure of the work}

In the next chapter, Chapter~\ref{chapter:background}, we will introduce the background knowledge needed to design a higher level language's front end.
Furthermore, in Chapter~\ref{ch:refinement_types}, we explore how to add refinements to a lambda calculus language.
In Chapter~\ref{ch:llvm}, we explore how to use the LLVM intermediate representation.
Then, in Chapter~\ref{chapter:proposal}, we describe the proposed \textit{Ekitai} language.

\chapter{A review on front end design and implementation}\label{chapter:background}

In this chapter we present a brief review of the background needed to build a compiler's front end.
We begin by presenting the formal definition of a language in Section~\ref{ch:background:sec:strings_and_languages}.
Then, give a brief overview of what is a lexical analyzer and how it is constructed in Section~\ref{chapter:background:sec:lexical}.
We continue by presenting the aspects of a syntax analyzer in Section~\ref{chapter:background:sec:syntax}.
Furthermore, we go on to explore the semantic analyzer and the tools used to formalize the type system in Section~\ref{chapter:background:sec:semantic}.
And then, explore the aspects of intermediate code generation in Section~\ref{chapter:background:sec:intermediate}.

\section{Strings and Languages}\label{ch:background:sec:strings_and_languages}

As stated by \textcite{sipser2012introduction}, \textquote{strings of characters are fundamental building blocks in computer science}.
In order to define what a string of characters is, \textcite{sipser2012introduction} defines an \textit{alphabet} to be any nonempty finite set composed by its \textit{symbols}.
The author elaborates that a \textit{string over an alphabet} is a finite sequence of symbols from that alphabet~\cite{sipser2012introduction}.
For example, if we build the English alphabet as the set of symbols
\begin{equation*}
  \Sigma = \{\code{a}, \code{b}, \code{c}, \code{d}, \dots, \code{x}, \code{y}, \code{z}\}
\end{equation*}
then, the word \code{compiler} would be a string over $\Sigma$.

Formally, a string $s$ over an alphabet $\Sigma$ is a string $s$ such that $s \in \Sigma^*$, called the Kleen closure of $\Sigma$~\cite{HopcroftMotwaniUllman07}.
In order to perform the inductive construction of $\Sigma^*$ we need to first define the empty string, written $\epsilon$, then, define the concatenation operation $uv = s$, where
\begin{gather*}
  u, v, s, w \in \Sigma^* \\
  u = u_1 u_2 u_3 \dots u_i \quad \text{where} \quad u_1 u_2 u_3 \dots u_i \in \Sigma \quad \text{and} \quad i \geq 0\\
  v = v_1 v_2 v_3 \dots v_i \quad \text{where} \quad v_1 v_2 v_3 \dots v_i \in \Sigma \quad \text{and} \quad j \geq 0\\
  s = u_1 u_2 u_3 \dots u_i v_1 v_2 v_3 \dots v_i \\
  u\epsilon = u \quad \epsilon u = u\\
  (uv)w = u(vw)
\end{gather*}
Next, we perform the inductive construction
\begin{align*}
  \Sigma^0     & = \{ \epsilon \}                                           \\
  \Sigma^1     & = \Sigma                                                   \\
  \Sigma^{i+1} & = \{ uv \mid u \in \Sigma^{i}\ \text{and}\ v \in \Sigma \} \\
  \Sigma^*     & = \bigcup_{i \geq 0} \Sigma^i
\end{align*}
This construction shows that $\Sigma^*$ is composed of all strings of finite size for all permutations of the alphabet~\cite{HopcroftMotwaniUllman07}.

After the definition of the set of all strings over an alphabet, we can broadly define a language $L$ as a proper subset of $\Sigma^*$, written $L \subseteq \Sigma^*$, and, define the operations on languages as such: given two languages $L$ and $K$ we have that
\begin{align*}
  L \cup K & = \{ s\ |\ s \in L\ \text{or}\ s \in K \}   \\
  L K      & = \{ uv\ |\ u \in L\ \text{and}\ v \in K \} \\
  L^*      & = \bigcup_{i \geq 0} L^{i}                  \\
  L^+      & = \bigcup_{i \geq 1} L^{i}                  \\
\end{align*}
Although having a simple definition, proving that a string $s$ belongs to a language $L$ turns out to be rather challenging.

We end up using different languages with different symbols for lexical, syntax and semantic analysis in order to verify the source program.
Although, for example, the lexer may take as its symbols the characters present in the encoding of its input file (e.g., UTF-8 and ASCII), the parser may take the token names outputted by the lexer as its symbols.
In the next sections~\ref{chapter:background:sec:lexical},~\ref{chapter:background:sec:syntax}, and~\ref{chapter:background:sec:semantic}, we explore the different formal tools the different analysis steps use to prove that a string of their respective symbols is a valid source program and how they produce an output for the next step in compilation.

\section{Lexical Analysis}\label{chapter:background:sec:lexical}

The first phase of a compiler is called lexical analysis.
The lexical analyzer will read a stream of characters from the source program and group the characters into sequences called \textit{lexemes} according to the \textit{patterns} defined for each \textit{token name}~\cite{Aho:2006:CPT:1177220}.
\textcite{Aho:2006:CPT:1177220} describes that, for each lexeme grouped by reading the input character stream, the lexical analyzer will generate a \textit{token} which constitutes a pair of the token name and some optional metadata (e.g., the start and end position of the respective lexeme in the input character stream) written
\begin{equation*}
  \langle \textit{token name},\ \textit{metadata}\rangle
\end{equation*}
For example, upon analyzing the following program:
\begin{equation*}
  \codett{let a = b + 60 / c}
\end{equation*}
A lexical analyzer may output the following token stream:
\begin{equation}\label{figure:introduction_token_stream}
  \begin{aligned}
    \tokenstream{\codett{let}}{\codett{let}}{$0$}{$3$}      \\
    \tokenstream{\codett{identifier}}{\codett{a}}{$4$}{$5$} \\
    \tokenstream{\codett{=}}{\codett{=}}{$6$}{$7$}          \\
    \tokenstream{\codett{identifier}}{\codett{b}}{$8$}{$9$} \\
    \tokenstream{\codett{+}}{\codett{+}}{$10$}{$11$}        \\
    \tokenstream{\codett{number}}{\codett{60}}{$12$}{$14$}  \\
    \tokenstream{\codett{/}}{\codett{/}}{$15$}{$16$}        \\
    \tokenstream{\codett{identifier}}{\codett{c}}{$17$}{$18$}
  \end{aligned}
\end{equation}
In this case, the metadata collected by the analyzer is very pedantic including even redundant data as the lexemes for simple patterns of the token names \codett{=}, \codett{+} and \codett{/}.
When no metadata is needed it can be omitted from the token notation (e.g., \token{\{}).

In order to classify a lexeme to be of a given token name we employ the use of \textit{patterns}, and one of the important notations for the description of token patterns are regular expressions~\cite{Aho:2006:CPT:1177220}.
Regular expressions are very effective in specifying the type of patterns usually needed to classify lexemes into tokens and can be used for automatic generation of a lexical analyzer~\cite{Aho:2006:CPT:1177220}.
A simple description of the patterns for the tokens used in the example above can be given by regular expressions with rules of the form $\textit{pattern name} \rightarrow \textit{regular expression}$ as follows:
\begin{equation}\label{eq:regular_definitions}
  \begin{aligned}
    \code{id}     & \rightarrow \ {[\code{A-Za-z}]}^+ \\
    \code{number} & \rightarrow \ {[\code{0-9}]}^+    \\
    \code{let}    & \rightarrow \ \code{let}          \\
    \code{=}      & \rightarrow \ \code{=}            \\
    \code{/}      & \rightarrow \ \code{/}            \\
    \code{+}      & \rightarrow \ \code{+}            \\
  \end{aligned}
\end{equation}

Regular expressions are built recursively out of smaller regular expressions, and each regular expression $r$ describes a language $L$, written $L(r)$, which is also defined recursively from $r$'s sub-expressions~\cite{Aho:2006:CPT:1177220}.
We can define regular expressions starting with the regular expression $\epsilon$, for describing the language $L(\epsilon) = \{ \epsilon \}$, then, defining a regular expression $a$ for all $s \in \Sigma$ where $L(a) = \{ s \}$~\cite{Aho:2006:CPT:1177220}.
Then, building the induction supposing that $r$ and $s$ are regular expressions denoting languages $L(r)$ and $L(s)$, respectively, as
\begin{align*}
  r \mid s & = L(r) \cup L(s) \\
  r s      & = L(r) L(s)      \\
  r^*      & = {L{(r)}}^*     \\
  r^+      & = {L{(r)}}^+
\end{align*}
Using these operations we can build all regular expressions~\cite{Aho:2006:CPT:1177220}.
Furthermore, a group of unions $s_1 \mid s_2 \mid \dots \mid s_k$ can be abbreviated as $[s_1 s_2 \dots s_k]$~\cite{Aho:2006:CPT:1177220}.

The notation presented in~\eqref{eq:regular_definitions} is defined by \textcite{Aho:2006:CPT:1177220} as \textit{regular definitions} where, if $\Sigma$ is an alphabet, then a regular definition is a sequence of definitions of the form:
\begin{equation*}
  \begin{aligned}
    d_1 & \rightarrow \ r_1 \\
    d_2 & \rightarrow \ r_2 \\
        & \dots             \\
    d_n & \rightarrow \ r_n
  \end{aligned}
\end{equation*}
where, each definition name $d_i$ is a new symbol not present in $\Sigma$ and unique from the other definition names.
Also, each $r_i$ is a regular expression over the alphabet $\Sigma \cup \{ d_1, d_2, \dots, d_{i-1}\}$ such that $r_i$ can use the definitions above him as regular expressions~\cite{Aho:2006:CPT:1177220}.
In order to facilitate the use of definition names in a regular expression, modern tools often write the definition name between the markers \code{[:} and \code{:]} as in
\begin{equation}\label{eq:regular_definitions_with_variables}
  \begin{aligned}
    \code{digit}  & \rightarrow \ [\code{0-9}]                                      \\
    \code{letter} & \rightarrow \ [\code{a-zA-Z}]                                   \\
    \code{id}     & \rightarrow \ \code{[:letter:]}{[\code{[:letter:][:digit:]}]}^* \\
    \code{number} & \rightarrow \ \code{[:digit:]}^+                                \\
    \code{let}    & \rightarrow \ \code{let}                                        \\
    \code{=}      & \rightarrow \ \code{=}                                          \\
    \code{/}      & \rightarrow \ \code{/}                                          \\
    \code{+}      & \rightarrow \ \code{+}                                          \\
  \end{aligned}
\end{equation}

Formally, the lexer for a programming language is trying to prove that a given input string $s$ belongs to a language $L$.
For example, a programming language could define the regular definitions \code{id}, \code{number}, \code{let}, \code{=}, \code{/}, and \code{+}, from~\eqref{eq:regular_definitions_with_variables}, as the patterns for its token names (\code{digit} and \code{letter} serve only as building blocks).
Then, the language $L$ for the parser of such programming language is constructed by making the union of the languages described by all the token name's regular expressions and applying the Kleen closure to generate the set of all permutations getting
\begin{equation*}
  L = {(L(\code{id}) \cup L(\code{number}) \cup L(\code{let}) \cup L(\code{=}) \cup L(\code{/}) \cup L(\code{+}))}^*
\end{equation*}

During the lexer execution, every time the lexer identifies a lexeme from the input string for a given token name's regular expression, the lexer outputs a token for that lexeme and consumes the lexeme from the input string~\cite{Aho:2006:CPT:1177220}.
If an input string does match a lexeme for any token name regular expression, the lexer should report the error to the compiler's user~\cite{Aho:2006:CPT:1177220}.

The regular definition rules of token names can then, either be used by lexical analyzer generators to automatically generate a program for lexical analysis, or, be used as the formal specification for handwritten lexers to be based upon~\cite{Aho:2006:CPT:1177220}.
In both cases the output will either be the list of tokens or the lexical error encountered during the reading of the input string.

\section{Syntax Analysis}\label{chapter:background:sec:syntax}

The second phase of a compiler is called syntax analysis, or \textit{parsing}.
The \textit{parser} receives as input a token stream produced by the lexical analyzer and creates a tree-like intermediate representation that is constrained by a particular grammatical structure~\cite{Aho:2006:CPT:1177220}.

If we give the token stream~\eqref{figure:introduction_token_stream} as input to a parser it may produce the \textit{parse tree} structure found on Figure~\ref{figure:introduction_ast}.
This structure has more information than the linear token stream it received as input.
For example, it is prepared in such a way to preserve the order of operations from classic arithmetic.
Consequently, the tree is composed of two interior nodes labeled $\bnfvar{Expr}$ for binary operations: the bottom one, denoting the sub-expression
\begin{equation*}
  \bnfvar{Expr}_{bottom} = \code{60 / c}
\end{equation*}
and the top one, denoting the whole expression
\begin{equation*}
  \bnfvar{Expr}_{top} = \code{a + $\bnfvar{Expr}_{bottom}$} = \code{a + 60 / c}
\end{equation*}
This structure makes explicit that we must first evaluate the result of $\bnfvar{Expr}_{bottom}$, dividing \codett{60} by \codett{c}, before we can evaluate the result of $\bnfvar{Expr}_{top}$, adding \codett{a} to the result of $\bnfvar{Expr}_{bottom}$.

\begin{figure}[ht]
  \centering
  \begin{tikzpicture}
    \node (is-root) {\bnfvar{Let}} [sibling distance=2cm]
    child { node {\bnfter{let}} }
    child {
        node {\bnfvar{Pattern}}
        child {
            node[align=center] {\bnfter{id}}
            child {
                node {a}
                edge from parent[dashed]
              }
          }
        child[missing] {}
      }
    child { node {\bnfter{=}} }
    child {
        node {\bnfvar{Expr}}
          [sibling distance=1.2cm]
        child {
            node {\bnfter{id}}
            child {
                node {b}
                edge from parent[dashed]
              }
          }
        child[missing]{}
        child {
            node {\bnfvar{BinOp}}
            child{ node {\bnfter{+}} }
            child[missing]{}
          }
        child[missing]{}
        child {
            node {\bnfvar{Expr}}
              [sibling distance=2cm]
            child[missing]{}
            child {
                node {\bnfter{number}}
                child {
                    node {60}
                    edge from parent[dashed]
                  }
              }
            child {
                node {\bnfvar{BinOp}}
                child {
                    node {\bnfter{/}}
                  }
              }
            child {
                node {\bnfter{id}}
                child {
                    node {c}
                    edge from parent[dashed]
                  }
              }
          }
      };
  \end{tikzpicture}
  \caption{
    A possible parse tree output for the token stream~\eqref{figure:introduction_token_stream}.
    Leafs are shown as the token names from the token stream.
    Here lexemes for non-trivial tokens are appended with dashed lines underneath token names for example purposes.
  }\label{figure:introduction_ast}
\end{figure}

\subsection{Precise definition of context-free grammars and ambiguity}
Every programming language has precise rules that prescribe the correct syntactic structure of its programs~\cite{Aho:2006:CPT:1177220}.
In order to formally describe the rules of such syntactic structure, we can use a context-free grammar~\cite{Aho:2006:CPT:1177220}.
A context-free grammar is a finite set of constructs that allows us to build the set of all strings, usually called \textit{sentences}, of a given context-free Language~\cite{sipser2012introduction}.
They have four components:
\begin{itemize}
  \item A finite set $\Sigma$ of \textit{terminal} symbols, usually called tokens, composed by the set of token names defined in the lexical aspects of the language~\cite{Aho:2006:CPT:1177220};
  \item A finite set $V$ of \textit{nonterminals}, or \textit{variables}, disjoint from the set of terminals, usually called \textit{syntactic variables}~\cite{Aho:2006:CPT:1177220};
  \item A finite set $R$ of \textit{productions}, or \textit{rules}, where each production is of the form
        \begin{equation*}
          \begin{alignedat}{2}
            \bnfprod{Variable}{s}
          \end{alignedat}
        \end{equation*}
        where \bnfvar{V} is the variable at the production's \textit{head}, and $s \in {(\Sigma \cup V)}^*$ is the string at the production's \textit{body}~\cite{sipser2012introduction};
  \item And, a designation of a nonterminal $S$ as the \textit{start} of the grammar~\cite{Aho:2006:CPT:1177220}.
\end{itemize}

A simple expression language could have its syntactical structure formalized by the following grammar
\begin{equation}~\label{eq:ambiguous_expr_grammar}
  \begin{alignedat}{2}
    \bnfprod{Expr}{\bnfvar{Expr} \bnfvar{BinOp} \bnfvar{Expr}} \\
    \bnfmore{\bnfter{(} \bnfvar{Expr} \bnfter{)}}               \\
    \bnfmore{\bnfter{number}}                                     \\
    \bnfmore{\bnfter{id}}                                         \\
    \bnfprod{BinOp}{\bnfter{+} \bnfor{\bnfter{-}} \bnfor{\bnfter{*}} \bnfor{\bnfter{/}}}
  \end{alignedat}
\end{equation}
where: terminals have the token names between quotes (e.g., $\bnfter{id}$);
nonterminals have the syntactic variables as normal words (e.g., $\bnfvar{Expr}$);
and, the designation of the starting variable is done by the variable at the head of the first production (e.g., in the example above the starting nonterminal is $\bnfvar{Expr}$).
Furthermore, multiple productions can be abbreviated by using the \textit{or} operator denoted by `$\mid$' turning
\begin{alignat}{2}
  \bnfprod{V}{$s_1$} \nonumber \\
  \bnfprod{V}{$s_2$} \nonumber
\end{alignat}
into
\begin{alignat}{2}
  \bnfprod{V}{$s_1$ \bnfor{$s_2$}} \nonumber
\end{alignat}

In order to show that a particular string of terminal symbols is in the language formalized by a context-free grammar, we can perform a derivation~\cite{sipser2012introduction}.
A derivation starts with, first, writing the start variable~\cite{sipser2012introduction}.
Then, second, we choose any variable from the string written so far and a production with the same variable as head replacing the chosen variable with the body of the chosen production~\cite{sipser2012introduction}.
Then, we repeat the second step until there is no more variables and the string is formed only by terminals~\cite{sipser2012introduction}.

Formally, \textcite{sipser2012introduction} defines that $u$ \textit{derives} $v$, written $u \Rightarrow^* v$ where $u, v \in {(\Sigma \cup V)}^*$, if $u = v$ or if there exists strings $s_1, s_2, s_3, \dots, s_k \in {(\Sigma \cup V)}^*$ for $k \geq 0$ where
\begin{equation*}
  u \Rightarrow s_1 \Rightarrow  s_2, \Rightarrow s_3, \Rightarrow \dots \Rightarrow s_k \Rightarrow v
\end{equation*}
We can then define that the language $L$ of the grammar is the set $L = \{s \in \Sigma^* \mid S \Rightarrow^* s\}$, meaning $L$ is the set of all strings $s$ composed only by terminal symbols that can be derived from the starting variable $S$.

Take, for example, the string $s = \code{id + number * id}$ and the language $L$ specified by grammar~\eqref{eq:ambiguous_expr_grammar}. We can proof that $s \in L$ by showing that $\bnfvar{Expr} \Rightarrow^* s$, so we start by writing the start variable
\begin{equation*}
  \bnfvar{Expr}
\end{equation*}
then choose a production with \bnfvar{Expr} as head and substitute \bnfvar{Exrp} with the production's body. We know there are two binary operations, so we choose the production which its body is \code{\bnfvar{Expr} \bnfvar{BinOP} \bnfvar{Expr}} producing the derivation
\begin{equation*}
  \bnfvar{Expr} \Rightarrow \code{\bnfvar{Expr} \bnfvar{BinOp} \bnfvar{Expr}}
\end{equation*}
For the next derivation step, we are open to choose what variable to derivate first.
Although we could choose any variable from the string during derivation, we will stick to leftmost derivations, which mandates that we choose the leftmost variable, highlighted in \textbf{bold}, of the string to perform the derivation.
So, we could have the leftmost derivation
\begin{equation}\label{eq:ambiouous_derivation1}
  \begin{aligned}
    \code{\bnfvar{\textbf{Expr}}}
     & \Rightarrow
    \code{\bnfvar{\textbf{Expr}} \bnfvar{BinOp} \bnfvar{Expr}}                              \\
     & \Rightarrow
    \code{\bnfvar{\textbf{Expr}} \bnfvar{BinOp} \bnfvar{Expr} \bnfvar{BinOp} \bnfvar{Expr}} \\
     & \Rightarrow
    \code{\bnfter{id} \bnfvar{\textbf{BinOp}} \bnfvar{Expr} \bnfvar{BinOp} \bnfvar{Expr}}   \\
     & \Rightarrow
    \code{\bnfter{id} \bnfter{+} \bnfvar{\textbf{Expr}} \bnfvar{BinOp} \bnfvar{Expr}}       \\
     & \Rightarrow
    \code{\bnfter{id} \bnfter{+} \bnfter{number} \bnfvar{\textbf{BinOp}} \bnfvar{Expr}}     \\
     & \Rightarrow
    \code{\bnfter{id} \bnfter{+} \bnfter{number} \bnfter{*} \bnfvar{\textbf{Expr}}}         \\
     & \Rightarrow
    \code{\bnfter{id} \bnfter{+} \bnfter{number} \bnfter{*} \bnfter{id}}
  \end{aligned}
\end{equation}
thus, proving that the $s \in L$
but, we could also have a second leftmost derivation
\begin{equation}\label{eq:ambiouous_derivation2}
  \begin{aligned}
    \code{\bnfvar{\textbf{Expr}}}
     & \Rightarrow
    \code{\bnfvar{\textbf{Expr}} \bnfvar{BinOp} \bnfvar{Expr}}                          \\
     & \Rightarrow
    \code{\bnfter{id} \bnfvar{\textbf{BinOp}} \bnfvar{Expr}}                            \\
     & \Rightarrow
    \code{\bnfter{id} \bnfter{+} \bnfvar{\textbf{Expr}}}                                \\
     & \Rightarrow
    \code{\bnfter{id} \bnfter{+} \bnfvar{\textbf{Expr}} \bnfvar{BinOp} \bnfvar{Expr}}   \\
     & \Rightarrow
    \code{\bnfter{id} \bnfter{+} \bnfter{number} \bnfvar{\textbf{BinOp}} \bnfvar{Expr}} \\
     & \Rightarrow
    \code{\bnfter{id} \bnfter{+} \bnfter{number} \bnfter{*} \bnfvar{\textbf{Expr}}}     \\
     & \Rightarrow
    \code{\bnfter{id} \bnfter{+} \bnfter{number} \bnfter{*} \bnfter{id}}
  \end{aligned}
\end{equation}
thus, having two leftmost derivations that shows the string belongs to the grammar's described language.

A grammar is said to be \textit{ambiguous} if it has a non-unique leftmost or rightmost derivation for any string of the language it describes~\cite{Aho:2006:CPT:1177220}.
Therefore, grammar~\eqref{eq:ambiguous_expr_grammar} is clearly ambiguous given derivations~\eqref{eq:ambiouous_derivation1} and~\eqref{eq:ambiouous_derivation2}.
From the parse trees in Figure~\ref{fig:ambiguous_parse_tree} we can visualize that the order of operations differ between the derivations of \code{id + number * id}.
In Figure~\ref{fig:ambiguous_parse_tree_a} we have \code{(id + number) * id} and in Figure~\ref{fig:ambiguous_parse_tree_b} we have \code{id + (number * id)} which have different mathematical meanings.

\begin{figure}[ht]
  \centering
  \begin{subfigure}[b]{0.4\textwidth}
    \centering
    \begin{tikzpicture}
      \node {\bnfvar{Expr}} [sibling distance=1.7cm]
      child {
          node {\bnfvar{Expr}}
          child {
              node {\bnfter{id}}
            }
          child {
              node {\bnfvar{BinOp}}
              child{ node {\bnfter{+}} }
            }
          child {
              node {\bnfter{number}}
            }
        }
      child {
          node {\bnfvar{BinOp}}
          child[missing]{}
          child[missing]{}
          child {
              node {\bnfter{*}}
            }
        }
      child {
          node {\bnfter{id}}
        };
    \end{tikzpicture}
    \caption{Parse tree from derivation~\eqref{eq:ambiouous_derivation1}.}\label{fig:ambiguous_parse_tree_a}
  \end{subfigure}
  \qquad
  \qquad
  \begin{subfigure}[b]{0.4\textwidth}
    \centering
    \begin{tikzpicture}
      \node (is-root) {\bnfvar{Expr}} [sibling distance=1.7cm]
      child { node {\bnfter{id}} }
      child {
          node {\bnfvar{BinOp}}
          child {
              node {\bnfter{+}}
            }
          child[missing]{}
          child[missing]{}
        }
      child {
          node {\bnfvar{Expr}}
          child {
              node {\bnfter{number}}
            }
          child {
              node {\bnfvar{BinOp}}
              child{ node {\bnfter{*}} }
            }
          child {
              node {\bnfter{id}}
            }
        };
    \end{tikzpicture}
    \caption{Parse tree from derivation~\eqref{eq:ambiouous_derivation2}.}\label{fig:ambiguous_parse_tree_b}
  \end{subfigure}
  \caption{
    Visual representation of derivations~\eqref{eq:ambiouous_derivation1} and~\eqref{eq:ambiouous_derivation2} in its parse tree form.
  }\label{fig:ambiguous_parse_tree}
\end{figure}

Compilers use parse trees to derive meaning and, therefore, ambiguous grammars are problematic for compiling~\cite{appel2003modern}.
If we were to use some parsing generator algorithm, such as $LL$, $LR$ and their variants, an effort should be made to transform such ambiguous grammars into unambiguous grammars~\cite{appel2003modern}.
As an example, we can present an unambiguous grammar relative to the grammar~\eqref{eq:ambiguous_expr_grammar} as
\begin{equation}~\label{eq:unambiguous_expr_grammar}
  \begin{alignedat}{2}
    \bnfprod{Expr}{\bnfvar{Expr} \bnfter{+} \bnfvar{Term}} \\
    \bnfmore{\bnfvar{Expr} \bnfter{-} \bnfvar{Term}} \\
    \bnfmore{\bnfvar{Term}} \\
    \bnfprod{Term}{\bnfvar{Term} \bnfter{*} \bnfvar{Factor}} \\
    \bnfmore{\bnfvar{Term} \bnfter{/} \bnfvar{Factor}} \\
    \bnfprod{Factor}{\bnfter{(} \bnfvar{Expr} \bnfter{)}} \\
    \bnfmore{\bnfter{number}} \\
    \bnfmore{\bnfter{id}}
  \end{alignedat}
\end{equation}
which, by adding new variables to the grammar, can express unambiguously in its parse trees that: the operators \codett{*} and \codett{/} \textit{binds tighter}, or have a \textit{higher precedence}, then \codett{+} and \codett{-}; and, the operators of the same \textit{binding power}, or \textit{precedence}, are left associative.

\subsection{Review of top-down and bottom-up parsers}
There are two main categories of parser.
First we have parsers that try to construct the parse tree by finding a leftmost derivation starting from the root and creating the nodes of the parse three in preorder, called \textit{top-down} parsers~\cite{Aho:2006:CPT:1177220}.
And second we have parsers that try to construct the parse tree beginning at the leaves and working up to the root by performing a rightmost derivation in reverse applying a series of reductions on the input stream~\cite{Aho:2006:CPT:1177220}.

Top-down parsers, also called recursive descent parsers, have the advantage of its algorithms being simple enough to be used to construct parsers by hand~\cite{appel2003modern}.
Although simple, classic implementations of recursive descent parsers, such as the $LL(1)$ and $LL(k)$ predictive parsers, cannot deal with ambiguous grammars and, since they rely on leftmost derivations, cannot deal with left recursions on grammar productions~\cite{Aho:2006:CPT:1177220}.
There may also be the need to run a left factoring algorithm in order to generate a correct predictive parsing table for $LL(1)$ and $LL(k)$ automatic parser generators~\cite{Aho:2006:CPT:1177220}.

A non left recursive, left factored grammar for grammar~\eqref{eq:unambiguous_expr_grammar} can be built as show in Figure~\ref{fig:fat_expr_grammar}.
\begin{figure}[ht]
  \centering
  \begin{alignat}{2}
    \bnfprod{Expr}{\bnfvar{Term} \bnfvar{Expr2}} \label{eq:fat_expr_grammar_expr}                \\
    \bnfprod{Expr2}{\bnfter{+} \bnfvar{Term} \bnfvar{Expr2}} \label{eq:fat_expr_grammar_expr2}   \\
    \bnfmore{\bnfter{-} \bnfvar{Term} \bnfvar{Expr2}} \label{eq:fat_expr_grammar_expr3}          \\
    \bnfmore{$\epsilon$} \label{eq:fat_expr_grammar_expr4}                                       \\
    \bnfprod{Term}{\bnfvar{Factor} \bnfvar{Term2}} \label{eq:fat_expr_grammar_term}              \\
    \bnfprod{Term2}{\bnfter{*} \bnfvar{Factor} \bnfvar{Term2}} \label{eq:fat_expr_grammar_term2} \\
    \bnfmore{\bnfter{/} \bnfvar{Factor} \bnfvar{Term2}} \label{eq:fat_expr_grammar_term3}        \\
    \bnfmore{$\epsilon$} \label{eq:fat_expr_grammar_term4}                                       \\
    \bnfprod{Factor}{\bnfter{(} \bnfvar{Expr} \bnfter{)}} \label{eq:fat_expr_grammar_fact}       \\
    \bnfmore{\bnfter{number}} \label{eq:fat_expr_grammar_fac2}                                   \\
    \bnfmore{\bnfter{id}} \label{eq:fat_expr_grammar_fac3}
  \end{alignat}
  \caption{
    A context-free grammar after left factoring and removing left recursions from grammar~\eqref{eq:unambiguous_expr_grammar}.
  }\label{fig:fat_expr_grammar}
\end{figure}
This grammar complies with every constraint imposed by a classic implementation of a predictive recursive descent parser since it is unambiguous, free of left recursions and left factored.
An example implementation of predictive parsers for the variables \bnfvar{Expr}, \bnfvar{Expr2}, \bnfvar{Term}, \bnfvar{Term2} and \bnfvar{Factor} can be found respectively at Figures~\ref{fig:predictive_parser_expr},~\ref{fig:predictive_parser_expr2},~\ref{fig:predictive_parser_term},~\ref{fig:predictive_parser_term2} and~\ref{fig:predictive_parser_fact}.

\begin{figure}[ht]
  \centering
  \begin{subfigure}[b]{0.48\textwidth}
    \begin{minipage}{\textwidth}
      \begin{rustcode}
fn parse_Expr(p: Parser) -> Parse {
  let t = parse_Term(p);
  let e2 = parse_Expr2(p);
  Expr(t, e2)
}
      \end{rustcode}
    \end{minipage}
    \caption{}\label{fig:predictive_parser_expr}
  \end{subfigure}
  \quad
  \begin{subfigure}[b]{0.48\textwidth}
    \begin{minipage}{\textwidth}
      \begin{rustcode}
fn parse_Expr2(p: Parser) -> Parse {
  if p.at("+") | p.at("-") {
    let op = p.eat_token();
    let t = parse_Term(p);
    let e2 = parse_Expr2(p);
    Expr2(op, t, e2)
  } else {
    Empty
  }
}
      \end{rustcode}
    \end{minipage}
    \caption{}\label{fig:predictive_parser_expr2}
  \end{subfigure}
  \caption{
    Predictive parsers for variables \bnfvar{Expr} (a), and \bnfvar{Expr2} (b), from examples~\eqref{eq:fat_expr_grammar_expr} and~\eqref{eq:fat_expr_grammar_expr2}.
  }\label{fig:predictive_parser_expr_all}
\end{figure}

\begin{figure}[ht]
  \centering
  \begin{subfigure}[b]{0.48\textwidth}
    \begin{minipage}{\textwidth}
      \begin{rustcode}
fn parse_Term(p: Parser) -> Parse {
  let f = parse_Factor(p);
  let t2 = parse_Term2(p);
  Term(f, t2)
}
      \end{rustcode}
    \end{minipage}
    \caption{}\label{fig:predictive_parser_term}
  \end{subfigure}
  \quad
  \begin{subfigure}[b]{0.48\textwidth}
    \begin{minipage}{\textwidth}
      \begin{rustcode}
fn parse_Term2(p: Parser) -> Parse {
  if p.at("*") | p.at("/") {
    let op = p.eat_token();
    let f = parse_Factor(p);
    let t2 = parse_Term2(p);
    Term2(op, f, t2)
  } else {
    Empty
  }
}
      \end{rustcode}
    \end{minipage}
    \caption{}\label{fig:predictive_parser_term2}
  \end{subfigure}
  \caption{
    Predictive parsers for variables $\bnfvar{Term}$ (a), and $\bnfvar{Term2}$ (b), from examples~\eqref{eq:fat_expr_grammar_term} and~\eqref{eq:fat_expr_grammar_term2}.
  }\label{fig:predictive_parser_expr_term_all}
\end{figure}

\begin{figure}[ht]
  \centering
  \begin{minipage}{0.5\textwidth}
    \begin{rustcode}
fn parse_Factor(p: Parser) -> Parse {
  if p.at("(") {
    p.eat_token();
    let expr = parse_Expr(p);
    p.expect(")");
    Expr(expr)
  } else if p.at("id") {
    let token = p.eat_token();
    Id(token)
  } else if p.at("number") {
    let token = p.eat_token();
    Number(token)
  } else {
    panic!("Parse error.");
  }
}
    \end{rustcode}
  \end{minipage}
  \caption{
    A predictive parser for variable $\bnfvar{Factor}$ in~\eqref{eq:fat_expr_grammar_fact}.
  }\label{fig:predictive_parser_fact}
\end{figure}

\begin{figure}
  \centering
  \begin{minipage}{0.9\textwidth}
    \rustfile{code/pratt_example.rs}
  \end{minipage}
  \caption{
    A Pratt parser for the ambiguous expression grammar~\eqref{eq:ambiguous_expr_grammar}.
  }\label{fig:pratt_parser_expr_grammar}
\end{figure}

We were once again forced to add new variables to the already unambiguous grammar~\eqref{eq:unambiguous_expr_grammar}.
The transformations added variables without clear meanings, such as \bnfvar{Expr2} and \bnfvar{Term2}, further complicating the parse tree.
In order to enrich the capabilities of recursive descendant parsers, advances were made to allow recursive descendant parsers to parse some ambiguous grammars with left recursion without the need to left factoring.
\textcite{pratt1973operatorprecedence} proposed a \textit{top-down operator precedence} approach to parse arithmetic expressions by assigning a total order to tokens using a function for the left and right binding power of tokens in order to uniquely create parse trees of ambiguous left recursive expression grammars.
An example Pratt parser for grammar~\eqref{eq:ambiguous_expr_grammar} is found at Figure~\ref{fig:pratt_parser_expr_grammar}.
Pratt parsers work by combining recursion with iteration.
If we try to parse the sentence \code{$\code{id}_1$ + number * $\code{id}_2$} it:
\begin{itemize}
  \item parses $\codett{id}_1$ and name its node `left';
  \item enters the loop and parse \codett{+} which have a binding power to the left of $1$ and to the right of $2$;
  \item checks if it binds to the left stronger than the minimum binding power, in this case it does not since the minimum starts at $0$, continuing on to recursively parse \bnfter{number} which its node becomes the new `left' in the new recursion step;
  \item the next operator is then \codett{*} with binding power $3$ and $4$, this time the left binding power still is not smaller than the minimum of $2$, and we recursively parse $\codett{id}_2$ which again its node becomes a new `left';
  \item now, upon trying to parse a new operator, we break from the loop and return the node for $\codett{id}_2$ and name it `right'; then, we construct a node \bnfvar{Expr} with `left', which is \codett{number} for this iteration, the operator \codett{*}, and `right', which is $\codett{id}_2$;
  \item we then return again with another \bnfvar{Expr} node with `left', which now is $\codett{id}_1$, the operator \codett{+}, and the last returned expression as `right' finally forming the parse tree with correct precedence found at Figure~\ref{fig:ambiguous_parse_tree_b}.
\end{itemize}
The loop to keep checking if the next operation binds stronger than the minimum for the recursion guaranties both correct associativity and operator precedence~\cite{pratt1973operatorprecedence}.

Besides the work of~\textcite{pratt1973operatorprecedence}, there are recursive descent parsers which can parse a bigger range of ambiguous and left recursive grammars.
\textcite{10.1007/978-3-540-77442-6_12} proposes a handwritten parser combinator approach that uses composable higher-order functions, together with memoization and backtracking, in order to allow parsing both ambiguity and left recursive grammars in polynomial time.
There is also the works of~\textcite{10.1145/583852.581483} that proposes a handwritten parser approach, called \textit{Packrat}, using memoization, backtracking, and unlimited look ahead to parse $LL(k)$ and $LR(k)$ grammars by using \textit{Parsing Expression Grammar} (PEG) definitions, formalized by~\textcite{10.1145/982962.964011}, which differs from context-free grammars by imposing a total order to the rules set of the grammar.
Therefore, PEGs cannot be ambiguous because there is no choice involved in which production to choose for derivation~\cite{10.1145/982962.964011}.
Furthermore, \textcite{10.1145/1328408.1328424} extended Packrat to allow left-recursion by changing its memoization process.

Bottom-up parsers have fewer context-free grammar constraints~\cite{Aho:2006:CPT:1177220}.
However, they still have troubles with ambiguous grammars due to shift-reduce conflicts but no longer require a non-left recursive and left factored grammar~\cite{Aho:2006:CPT:1177220}.
Writing a bottom-up shift-reduce parser for an $LR$ class grammar is not an easy task, thankfully
parser generators for $LR(1)$ and $LR(k)$ languages are capable of generating very efficient parsers for a large range of context-free grammars~\cite{Aho:2006:CPT:1177220}.
Even allowing grammar designers to solve shifting-reduce conflicts by hand assigning whether to shift or to reduce for every conflict~\cite{Aho:2006:CPT:1177220}.

Although using bottom-up parser generators are desirable for its expressiveness and efficiency, in the works of \textcite{matklad2020challenginglrparsing, matklad2020prattparsing} for the Rust Analyzer, and~\textcite{Parr13} for the ANTLR parser generator, the authors advocate to implement handwritten recursive descent top-down parser for it allows the implementation of better error messages and error recovery algorithms for modern compiler implementations and tools.

\subsection{Semantic actions and syntax-directed translations}

As put by~\textcite{appel2003modern} \textquote{A compiler must do more than recognize whether a sentence belongs to the language of a grammar, it must do something useful with that sentence}.
The specification of \textit{semantic actions} allows us to do something useful with sentences that are parsed~\cite{appel2003modern}.
They are pieces of code that we can attach to the body of grammar production rules to specify \textit{syntax-directed definitions}~\cite{Aho:2006:CPT:1177220}.
Syntax-directed definitions, then, are used to perform \textit{syntax-directed translations} by executing the code specified by the semantic actions during specific moments of the parsing process~\cite{Aho:2006:CPT:1177220}.
Take the following syntax-directed definition
\begin{alignat}{2}
  \bnfprod{Expr}{\bnfvar{Term} \bnfvar{Expr2} \{Expr.n = Expr(Term.n, Expr2.n)\}} \label{eq:sdd_example}
\end{alignat}
Note that the attribute \codett{n} for the variables \bnfvar{Expr}, \bnfvar{Term} and \bnfvar{Expr2}, can be thought of as the return node value of a parsing function for this production.
If we give a closer look at the parser implemented in Figure~\ref{fig:predictive_parser_expr}, we can see this semantic action at line 4, after parsing \bnfvar{Term} and \bnfvar{Expr2}, returning the value of the attribute \codett{Expr.n}.
Furthermore, the position of the semantic action in a production body determines when the semantic action executes during parsing.
In this example it is positioned after the parser finishes parsing the production.

It is possible to write an entire compiler into the semantic actions of a parser~\cite{appel2003modern}.
However, this approach limits the compiler to analyze the source program in the strict order it is parsed~\cite{appel2003modern}.
Hence, using some sort of intermediate representation between syntax analysis and semantics analysis allows the semantic analyzer to be free from the constraints of the parsing algorithm~\cite{appel2003modern}.
One possible solution is to design the semantic actions in such a way that they output an intermediate tree data structure~\cite{appel2003modern}.
When the parser builds a tree with leaf nodes for each token and interior nodes for each production parsed we call such tree a \textit{concrete parse tree}~\cite{appel2003modern}.
An example of a concrete parse tree together with metadata collected for each node can be found at Figure~\ref{fig:concrete_syntax_tree}.

\begin{figure}[ht]
  \centering
  \begin{minipage}{0.5\textwidth}
    \begin{rustcode}
Expr
  Term
    Factor
      Number "5"
  Expr2
    Plus "+"
    Term
      Factor
        Id "a"
    \end{rustcode}
  \end{minipage}
  \caption{
    A sample concrete syntax tree of grammar~\ref{fig:fat_expr_grammar} for sentence \codett{5 + a}.
  }\label{fig:concrete_syntax_tree}
\end{figure}

Sometimes a concrete parse tree is too verbose, like a tree for the grammar in Figure~\ref{fig:fat_expr_grammar}, and, it is ideal to transform this tree into a simpler one in order to facilitate semantic analysis~\cite{appel2003modern}.
We can then create a second grammar with the \textit{abstract syntax} of the language.
The abstract syntax grammar does not need to follow the constraints of syntax analysis and only exists to facilitate the later stages of compilation~\cite{appel2003modern}.
For example, we can use the ambiguous grammar at~\eqref{eq:ambiguous_expr_grammar} as the abstract syntax for the language described by the grammar in Figure~\ref{fig:fat_expr_grammar}.

A compiler could easily translate a concrete parse tree into an \textit{Abstract Syntax Tree} (AST)~\cite{appel2003modern}.
An AST can be programmed by the use of programming constructs such as \textit{Abstract Data Types} (ADT) as show in Figure~\ref{fig:abstract_syntax_adt}.
In Section~\ref{chapter:background:sec:semantic} we will explore how to formally define the abstract tree and the rules that allows us to check if the semantic properties of the language are sound.

\begin{figure}[ht]
  \centering
  \begin{minipage}{0.6\textwidth}
    \begin{rustcode}
enum Ast {
  Expr(Ast, BinOp, Ast),
  Number(Int),
  Id(String),
}
enum BinOp {
  Plus,
  Minus,
  Times,
  Div,
}
    \end{rustcode}
  \end{minipage}
  \caption{
    An example of an abstract syntax for grammar at Figure~\eqref{eq:ambiguous_expr_grammar} described by Abstract data types.
  }\label{fig:abstract_syntax_adt}
\end{figure}

\subsection{Syntax error recovery}

When a parser fails to find a derivation for the input it is called a syntax error~\cite{appel2003modern}.
If we find an error, it would be advantageous for the user of the compiler if the parsing did not halt the compilers' execution on the first error~\cite{appel2003modern}.
This is what can be accomplished with error recovery~\cite{appel2003modern}.
One way we can build error recovery into top-down parsers is by the use of recovery token sets.
Essentially, once a parser using recovery token set finds an error, it discards all tokens until it finds a token in the recovery token set, then returns an error node containing the discarded tokens hopping that the father nodes will be able to continue parsing from the token found in the recovery token set.

\section{Semantic Analysis}\label{chapter:background:sec:semantic}

The third phase of a compiler is called semantic analysis.
It is responsible to use the AST generated by the parser to check if the source program is semantically consistent with the language specification.
An important part of semantic analysis is \textit{type checking} where it will try to validate the program to the language specification's type system.

If we feed a type checker with the abstract syntax tree for the program
\begin{equation*}
  \codett{b + 60 * c}
\end{equation*}
it may make use of a \textit{context} containing the type information of the identifiers \code{b} and \code{c} to type check all nodes of the tree for consistency.
Suppose a context that maps the identifier \code{c} to the type \code{Bool}, written \typer{c}{Bool}, which would compose of the values \code{true} and \code{false}.
With this information, the type checker would be able to decide if the operation \code{60 * c} is semantically sound to the language's type system.
Does the language type system specifies as valid to divide a number by a boolean?
If it does, what does it mean to divide the value \codett{60} by \codett{false}?

Maybe, whoever designed the language decided that, if the dividend is a value of type \code{Int} and the divisor is a value of type \code{Bool}, it will use some rule to convert the divisor's type to number in order to keep the program semantically sound.
Maybe, the language's type system forbids this behavior and will halt the compilation process with some error message.
These are decisions made when building a language's type system.

In order to star reasoning about the semantic aspects of a language suppose the following abstract syntax
\begin{equation}~\label{eq:sample_term_ast}
  \begin{alignedat}{2}
    \astprod{term}{true} \\
    \astmore{false} \\
    \astmore{if term then term else term} \\
    \astmore{0} \\
    \astmore{succ term} \\
    \astmore{pred term} \\
    \astmore{iszero term}
  \end{alignedat}
\end{equation}
Note that the grammar for an abstract tree does not have different notations for its terminal and non-terminal symbols in order to not clutter the inference rules with notation symbols~\cite{pierce2002types}.
The symbol \code{term} at the left of $\bnfdef$ defines the set of \textit{terms} and defines that we are going to use the symbol \code{term} to range over the set terms~\cite{pierce2002types}.
The same symbol \code{term} when used at the right of $\bnfdef$ is called a \textit{metavariable} simply to differentiate from the variables of the programming language in question, and, we may substitute the \code{term} metavariables for any instance of terms~\cite{pierce2002types}.

The set of productions~\eqref{eq:sample_term_ast} defines a set of all possible terms, although compact and expressive it is only one of several ways of describing the syntax of a language~\cite{pierce2002types}.
According to \textcite{pierce2002types}, using the grammar to define the set of all terms is actually just a compact notation for the following inductive definition: The set of all terms is the smallest set $T$ such that
\begin{align}
   & \{\code{true},\ \code{false},\ \code{0}\} \subseteq T \label{eq:example_clause1} \\
   & \text{if } \code{term} \in T \text{, then } \{
  \code{succ \code{term}},
  \ \code{pred \code{term}},
  \ \code{iszero \code{term}}
  \} \subseteq T \label{eq:example_clause2}                                           \\
   & \text{if } \code{term}_1, \code{term}_2, \code{term}_3 \in T \text{, then }
  \code{if $\code{term}_1$ then $\code{term}_2$ else $\code{term}_3$} \in T\label{eq:example_clause3}
\end{align}
Formally, this definition defines $T$ as a set of \textit{trees} and not a set of strings as we show in Section~\ref{chapter:background:sec:syntax} with derivations~\cite{pierce2002types}.
A shorthand definition for the same inductive definition of terms can be given in \textit{inference rules}, as following:
\begin{align*}
  \code{true} \in T &  & \code{false} \in T &  & \code{0} \in T
\end{align*}
\begin{align*}
  \inferrule{
    \code{term} \in T
  }{
    \code{succ term} \in T
  }
   &  &
  \inferrule{
    \code{term} \in T
  }{
    \code{pred \code{term}} \in T
  }
   &  &
  \inferrule{
    \code{term} \in T
  }{
    \code{pred \code{term}} \in T
  }
\end{align*}
\begin{align*}
  \inferrule{
  \code{term}_1 \in T \\ \code{term}_2 \in T \\ \code{term}_3 \in T
  }{
    \code{if $\code{term}_1$ then $\code{term}_2$ else $\code{term}_3$} \in T
  }
\end{align*}
where the first three rules restate the first clause in~\eqref{eq:example_clause1}, the middle three rules restate the second clause in~\eqref{eq:example_clause2}, and the last rule restate clause in~\eqref{eq:example_clause3}~\cite{pierce2002types}.
Each rule means that if we found the statements in the premises listed above the line, then we may derive the conclusion below the line~\cite{pierce2002types}.
When defining the syntax as inference rules, the fact that $T$ is the smallest possible set is not stated explicitly but implied, also, the rules with no premise are called \textit{axioms} and are written with no bar, since there is nothing to put above it~\cite{pierce2002types}.
Furthermore, \textcite{pierce2002types} points out that what we are calling \textit{inference rules} are actually \textit{rule schemas} which represents the infinite set of \textit{concrete rules} that can be obtained by substituting each variable by all possible sentences of the syntactic category, e.g., substituting each metavariable for \code{term} by every possible term in the inference rules above.

We can than proof that \code{if true than succ 0 else pred succ 0} is a \code{term} as
\begin{equation*}
  \inferrule{
    \code{true} \in T
    \\ \inferrule{\code{0} \in T}{\code{succ 0} \in T}
    \\ \inferrule{
      \inferrule{\code{0} \in T}{\code{succ 0} \in T}
    }{
      \code{pred succ 0} \in T
    }
  }{
    \code{if true than succ 0 else pred succ 0} \in T
  }
\end{equation*}
Here we see the concrete inference rules in play, and not the rules schemas.
We constructed the inference rules in such a way that proving a tree is part of $T$ is no different from finding a derivation for a string $s$ for the grammar.

\subsection{Operational Semantics}

The \textit{operational semantics} of a language allows for the precise definition of how terms are evaluated, that is, the precise definition of the \textit{semantics} of the language~\cite{pierce2002types}.
It specifies the behavior of a programming language by defining an \textit{abstract machine} that uses the terms of the programming language as its instructions instead of a low-level processor's instruction set~\cite{pierce2002types}.
Hence, the use of the term `abstract' it~\cite{Aho:2006:CPT:1177220}.

For simple languages like~\eqref{eq:sample_term_ast} the state of the abstract machine can be defined by just a \code{term} and its behavior by a \textit{transition function} that for each \code{term} either performs an evaluation step, or halts and announces the evaluation has ended~\cite{pierce2002types}.
In order to define operational semantics for~\eqref{eq:sample_term_ast} we first extend the abstract syntax with a new metavariable \code{value} for the possible end results of evaluation as
\begin{equation}\label{eq:sample_ast_values}
  \begin{aligned}
    \astprod{value}{true} \\
    \astmore{false}       \\
    \astmore{nvalue}      \\
    \astprod{nvalue}{0}   \\
    \astmore{succ nvalue}
  \end{aligned}
\end{equation}
and second, we define and \textit{evaluation relation}, or \textit{judgments}, on terms, written \evalr{term}{$\code{term}_1$} meaning \code{term} evaluates to $\code{term}_1$ in on step~\cite{pierce2002types}.
The evaluation relation for~\eqref{eq:sample_term_ast} is the smallest relations defined by the inference rules in Figure~\ref{fig:sample_term_ast_opsem}.

\begin{figure}[ht]
  \begin{minipage}{\textwidth}
    \begin{align*}
      \inferrule*[right=Eval-IfTrue]{ }{\evalr{if true then $\code{term}_1$ else $\code{term}_2$}{$\code{term}_1$}}
    \end{align*}
    \begin{align*}
      \inferrule*[right=Eval-IfFalse]{ }{\evalr{if false then $\code{term}_1$ else $\code{term}_2$}{$\code{term}_2$}}
    \end{align*}
    \begin{align*}
      \inferrule*[right=Eval-If]{
        \evalr{$\code{term}_1$}{$\code{term}_4$}
      } {
        \evalr{%
          if $\code{term}_1$ then $\code{term}_2$ else $\code{term}_3$%
        }{%
          if $\code{term}_4$ then $\code{term}_2$ else $\code{term}_3$%
        }
      }
    \end{align*}
    \begin{align*}
      \inferrule*[right=Eval-Succ]{
        \evalr{$\code{term}_1$}{$\code{term}_2$}
      } {
        \evalr{%
          succ $\code{term}_1$%
        }{%
          succ $\code{term}_2$%
        }
      }
    \end{align*}
    \begin{align*}
      \inferrule*[right=Eval-PredSucc]{ }{\evalr{pred succ nvalue}{nvalue}}
    \end{align*}
    \begin{align*}
      \inferrule*[right=Eval-Pred]{
        \evalr{$\code{term}_1$}{$\code{term}_2$}
      } {
        \evalr{%
          pred $\code{term}_1$%
        }{%
          pred $\code{term}_2$%
        }
      }
    \end{align*}
    \begin{align*}
      \inferrule*[right=Eval-IszeroZero]{ }{\evalr{iszero 0}{true}}
    \end{align*}
    \begin{align*}
      \inferrule*[right=Eval-Iszero]{
        \evalr{$\code{term}_1$}{$\code{term}_2$}
      } {
        \evalr{%
          iszero $\code{term}_1$%
        }{%
          iszero $\code{term}_2$%
        }
      }
    \end{align*}
  \end{minipage}
  \caption{
    The evaluation relations for the operational semantics of the abstract syntax~\eqref{eq:sample_term_ast} with values defined by~\eqref{eq:sample_ast_values}.
    Adapted from \textcite{pierce2002types}.
  }\label{fig:sample_term_ast_opsem}
\end{figure}

Using the evaluation relations from Figure~\ref{fig:sample_term_ast_opsem} we can evaluate the for example
\begin{equation}\label{eq:eval_step1}
  \code{if iszero 0 then succ 0 else false}
\end{equation}
looking at Figure~\ref{fig:sample_term_ast_opsem}, we see that the only inference rule we can apply is Eval-If, applying this rule to~\eqref{eq:eval_step1} we get
\begin{equation*}
  \inferrule*[right=Eval-If]{
    \inferrule*[right=Eval-IszeroZero]{ }{
      \evalr{iszero 0}{true}
    }
  } {
    \evalr{%
      if iszero 0 then succ 0 else false%
    }{%
      if true then succ 0 else false%
    }
  }
\end{equation*}
proving that
\begin{equation*}
  \evalr{\code{if iszero 0 then succ 0 else false}}{\code{if true then succ 0 else false}}
\end{equation*}
by first applying rule Eval-If, then applying the axiom Eval-IszeroZero.
We can continue the evaluation by applying rule Eval-IfTrue as in
\begin{equation}\label{eq:eval_step2}
  \inferrule*[right=Eval-IfTrue]{ } {
    \evalr{%
      if true then succ 0 else false%
    }{%
      succ 0%
    }
  }
\end{equation}
thus proving that
\begin{equation*}
  \evalr{\code{if true then succ 0 else false}}{succ 0}
\end{equation*}
Now the only evaluation rule that fits \code{succ 0} is Eval-Succ, but if we try to apply it like in
\begin{equation*}
  \inferrule*[right=Eval-Succ]{
    \inferrule*[right=$?$]{ }
    {
      \evalr{0}{$?$}
    }
  } {
    \evalr{%
      succ 0%
    }{%
      $?$%
    }
  }
\end{equation*}
we quickly find that there is no rule to evaluate \code{0}, so the abstract machine halts and checks if the last evaluated result is a value~\cite{pierce2002types}.
In this case \code{succ 0} does belong in \code{values}, so the abstract machine successfully evaluated
\begin{equation*}
  \evalr{\code{if iszero 0 then succ 0 else false}}{\code{succ 0}}
\end{equation*}
\begin{equation*}
\end{equation*}
Note that if the \code{if} condition evaluated to \code{false} instead of \code{true}, we would have applied rule Eval-IfFalse instead of Eval-IfTrue at evaluation step~\eqref{eq:eval_step2}. Thus, evaluating the input term to \code{false} instead of \code{succ 0} which are values of different \textit{kinds}, but this behavior was intentionally described by the evaluation relation rules.

If, for example, the programmer of this language wrote a program that at some point evaluated to
\begin{equation*}
  \code{if 0 then true else false}
\end{equation*}
or
\begin{equation*}
  \code{succ true}
\end{equation*}
we quickly find out that there is no evaluation rule that can be applied, and, given that those terms are not present in \code{value}, the abstract machine not only halts, but we say it is \textit{stuck}~\cite{pierce2002types}.
In this case we would either have to define what it means for \code{0} to be used as a condition and what it means for constructing the successor of \code{true}, or halt the execution and return a runtime error to the user of the program being executed~\cite{pierce2002types}.

Terms that get stuck during evaluation correspond to meaningless or erroneous programs~\cite{pierce2002types}.
We would rather be able to tell, without evaluating the program, that its evaluation will \textit{not} get stuck~\cite{pierce2002types}.
For that, we would need to be able to differentiate between the different kinds of values computed by terms~\cite{pierce2002types}.
In the next section we introduce to the language~\eqref{eq:sample_term_ast} the \code{Nat} and \code{Bool} types in order to make sure every program evaluated does not get stuck.

\subsection{The typing relation}\label{ch:background:sec:typing_relation}

In order to ensure that programs will correctly evaluate to a value we will introduce a new relation called the \textit{typing relation}.

The typing relation for arithmetic expressions is written as \typer{term}{Type}, and means that \code{term} is of type \code{Type} where \code{Type} is a new syntactic form for the types of the language~\cite{pierce2002types}. Also, the relation is defined by a set of inference rules assigning types to terms~\cite{pierce2002types}.

We introduce the syntactic form for the metavariable \code{Type} to the syntax~\eqref{eq:sample_term_ast} as
\begin{equation}~\label{eq:sample_term_ast_bool_t}
  \begin{alignedat}{2}
    \astprod{Type}{Nat} \\
    \astmore{Bool}
  \end{alignedat}
\end{equation}
and define the inference rules for the typing relation of the abstract syntax~\eqref{eq:sample_term_ast} as show in Figure~\ref{fig:sample_term_ast_tyrel}.
\begin{figure}[ht]
  \begin{minipage}{\textwidth}
    \begin{align*}
      \inferrule*[right=Ty-True]{ }{\typer{true}{Bool}} &  & \inferrule*[right=Ty-False]{ }{\typer{false}{Bool}}
    \end{align*}
    \begin{align*}
      \inferrule*[right=Ty-If]{
        \typer{$\code{term}_1$}{Bool}
      \\ \typer{$\code{term}_2$}{Type}
      \\ \typer{$\code{term}_3$}{Type}
      }{
        \typer{if $\code{term}_1$ then $\code{term}_2$ else $\code{term}_3$}{Type}
      }
    \end{align*}
    \begin{align*}
      \inferrule*[right=Ty-Succ]{
        \typer{$\code{t}_1$}{Nat}
      }{
        \typer{succ $\code{t}_1$}{Nat}
      }
       &  &
      \inferrule*[right=Ty-Pred]{
        \typer{$\code{t}_1$}{Nat}
      }{
        \typer{pred $\code{t}_1$}{Nat}
      }
       &  &
      \inferrule*[right=Ty-Iszero]{
        \typer{$\code{t}_1$}{Nat}
      }{
        \typer{iszero $\code{t}_1$}{Bool}
      }
    \end{align*}
  \end{minipage}
  \caption{
    The inference rules for the typing relation of the abstract syntax~\eqref{eq:sample_term_ast} with types defined by~\eqref{eq:sample_term_ast_bool_t}.
    Adapted from \textcite{pierce2002types}.
  }\label{fig:sample_term_ast_tyrel}
\end{figure}

Now we can use the inference rules from~\ref{fig:sample_term_ast_tyrel} to prove if the term
\begin{equation*}
  \typer{iszero succ 0}{Bool}
\end{equation*}
by applying the following sequence of inferences
\begin{equation*}
  \inferrule*[right=Ty-IsZero]{
    \inferrule*[right=Ty-Succ]
    {
      \inferrule*[right=Ty-Zero]{ }{\typer{0}{Nat}}
    }{
      \typer{succ 0}{Nat}
    }
  }{
    \typer{iszero succ 0}{Bool}
  }
\end{equation*}
but, if we try to prove something wrong like
\begin{equation*}
  \typer{succ true}{Num} \quad \text{or} \quad \typer{if 0 then true else false}{Bool}
\end{equation*}
we will quickly find that there are no rules that allow us to reach the type relation axioms.
Furthermore, if we try to prove and infer the type of the example~\ref{eq:eval_step1} we end up with the following rule applications
\begin{equation*}
  \inferrule*[right=Ty-If]{
    \typer{iszero 0}{Bool}
    \\ \inferrule*[right=Ty-Succ]{
      \inferrule*[right=Ty-Zero]{ }{\typer{0}{Nat}}
    }{
      \typer{succ 0}{Nat}
    }
    \\ \typer{false}{Bool}
  }{
    \typer{if iszero 0 the succ 0 else false}{$?$}
  }
\end{equation*}
when trying to inductively define the typing relation for example~\ref{eq:eval_step1}, we can not properly use the induction rule Ty-If because the terms in both branches of the \code{if} evaluate to different types, and the rule Ty-If mandates that both branches evaluate to the same type for the rule to be sated.

\subsection{Pure simply typed lambda calculus}

In this section we continue expanding the abstract syntax in discussion in order to accommodate the use of variables, let expressions, function abstractions, and function applications. In doing so we will be discussing a variation of what \textcite{pierce2002types} calls \textit{Pure simply typed lambda-calculus}.

To bring our discussion to the simply typed lambda calculus, we resume all constructs we discussed so far relevant to type checking, together with the newly added ones as the following abstract syntax
\begin{equation}~\label{eq:simply_typed_lc}
  \begin{alignedat}{2}
    \astprod{term}{true} \\
    \astmore{false} \\
    \astmore{if term then term else term} \\
    \astmore{0} \\
    \astmore{succ term} \\
    \astmore{pred term} \\
    \astmore{iszero term} \\
    \astmore{var} \\
    \astmore{\astlet{var}{term}{term}} \\
    \astmore{\fnabs{var}{Type}{term}} \\
    \astmore{term term} \\
    \astprod{var}{a $\mid$ b $\mid$ c $\mid$ $\dots$ $\mid$ x $\mid$ y $\mid$ z $\mid$ aa $\mid$ ab $\mid$ $\dots$} \\
    \astprod{Type}{Bool} \\
    \astmore{Nat}
  \end{alignedat}
\end{equation}

But what happens if we try to prove \typer{succ a}{Nat} for example?
We currently have no rules that allow us to infer the type of the variable \codett{a}.
Nor do we have the capabilities with the current presented theory.
In order to for us to be capable of expressing the types of variables we need to extend the typing relation from a two-place relation into a three-place relation adding a context $\Gamma$~\cite{pierce2002types}.

The context $\Gamma$, also called \textit{typing context}, is a set of assumptions of the form \typer{var}{Type}~\cite{pierce2002types}.
It is described by the syntactic form
\begin{equation}~\label{eq:sample_term_ast_bool_g}
  \begin{alignedat}{2}
    \astprod{$\Gamma$}{\bnfvar{$\Gamma$}, \typer{var}{Type}} \\
    \bnfmore{$\emptyset$}
  \end{alignedat}
\end{equation}
$\Gamma$ is essentially a list of typing relations that grows to the right by applying the \code{,} operator, and we can also take away a typing relation from the context clever use of this notation during the design of inference rules~\cite{pierce2002types}.

The new three-place typing relation is written \typecxr{}{term}{Type}, and it means that \code{term} has type \code{Type} under the context $\Gamma$~\cite{pierce2002types}.
If the context is empty we write \typecxre{$\emptyset$}{term}{Type}, but the $\emptyset$ is usually omitted as in \typecxre{}{term}{Type}.

\textcite{pierce2002types} defines the three-place typing relation inference rules as show in Figure~\ref{fig:simpli_typed_type_rel}.
\begin{figure}[ht]
  \begin{minipage}{\textwidth}
    \begin{align*}
      \inferrule*[right=Ty-Var]{
        \typer{var}{Type} \in \Gamma
      }{
        \typecxr{}{var}{Type}
      }
    \end{align*}
    \begin{align*}
      \inferrule*[right=Ty-Let]{
        \typecxr{}{$\code{term}_1$}{$\code{Type}_1$}
      \\ \typecxr{\typecx{var}{$\code{Type}_1$}}{$\code{term}_2$}{$\code{Type}_2$}
      }{
        \typecxr{}{\astlet{var}{$\code{term}_1$}{$\code{term}_2$}}{$\code{Type}_2$}
      }
    \end{align*}
    \begin{align*}
      \inferrule*[right=Ty-FnAbs]{
        \typecxr{\typecx{var}{$\code{Type}_1$}}{term}{$\code{Type}_2$}
      }{
        \typecxr{}{\fnabs{var}{$\code{Type}_1$}{term}}{\fnty{$\code{Type}_1$}{$\code{Type}_2$}}
      }
    \end{align*}
    \begin{align*}
      \inferrule*[right=Ty-FnApp]{
        \typecxr{}{$\code{term}_1$}{\fnty{$\code{Type}_1$}{$\code{Type}_2$}}
      \\ \typecxr{}{$\code{term}_2$}{$\code{Type}_1$}
      }{
        \typecxr{}{$\code{term}_1$ $\code{term}_2$}{$\code{Type}_2$}
      }
    \end{align*}
  \end{minipage}
  \caption{
    The inference rules for the three-place typing relation of the abstract syntax~\eqref{eq:simply_typed_lc}.
    Adapted from \textcite{pierce2002types}.
  }\label{fig:simpli_typed_type_rel}
\end{figure}
Supposing all typing relation rules from Figure~\ref{fig:sample_term_ast_tyrel} were updated to the newer three-part type relation, In order to exemplify the workings of rules Ty-Var, Ty-Le, Ty-FnAbs, and Ty-FnApp, we will build the inference tree for the term
\begin{equation*}
  \astlet{fun}{\fnabs{x}{Nat}{iszero x}}{fun succ 0}
\end{equation*}
as presented in the following inference trees
\begin{align*}
  \inferrule*[right=Ty-FnAbs]{
    \inferrule*[right=Ty-Iszero]{
      \inferrule*[right=Ty-Var]{
        \typer{x}{Nat} \in \Gamma
      } {
        \typecxr{\typecx{x}{Nat}}{x}{Nat}
      }
    }{
      \typecxr{\typecx{x}{Nat}}{iszero x}{Bool}
    }
  }{
    \typecxr{}{%
      \fnabs{x}{Nat}{iszero x}%
    }{\fnty{Nat}{Bool}}
  }
\end{align*}
\begin{align*}
  \inferrule*[right=Ty-Var]{
    \typer{fun}{\fnty{Nat}{Nat}} \in \Gamma
  } {
    \typecxr{\typecx{fun}{\fnty{Nat}{Nat}}}{fun}{\fnty{Nat}{Nat}}
  }
\end{align*}
\begin{align*}
  \inferrule*[right=Ty-Succ]{
    \inferrule*[right=Ty-Zero]{ }{
      \typecxr{\typecx{fun}{\fnty{Nat}{Nat}}}{0}{Nat} \in \Gamma
    }
  } {
    \typecxr{\typecx{fun}{\fnty{Nat}{Nat}}}{succ 0}{Nat}
  }
\end{align*}
\begin{align*}
  \inferrule*[right=Ty-FnApp]{
    \typecxr{\typecx{fun}{\fnty{Nat}{Bool}}}{fun}{\fnty{Nat}{Bool}}
  \\\\
    \typecxr{\typecx{fun}{\fnty{Nat}{Bool}}}{succ 0}{Nat}
  }{
    \typecxr{\typecx{fun}{\fnty{Nat}{Bool}}}{fun succ 0}{Bool}
  }
\end{align*}
\begin{align*}
  \inferrule*[right=Ty-Let]{
    \typecxr{}{%
      \fnabs{x}{Nat}{iszero x}%
    }{%
      \fnty{Nat}{Bool}%
    }
  \\\\
    \typecxr{%
      \typecx{fun}{\fnty{Nat}{Bool}}}{fun succ 0}{Bool}%
  }{%
    \typecxr{}{\astlet{fun}{\fnabs{x}{Nat}{iszero x}}{fun succ 0}}{Bool}%
  }
\end{align*}

\section{Intermediate code generation}\label{chapter:background:sec:intermediate}

As the last phase of a compiler's front end we have the intermediate code generation.

To do. Here goes an introduction of Single Static Assignment intermediate representations.

\chapter{Refinement types with predicates}\label{ch:refinement_types}

In order to explore refinement types, we will be discussing the works by \textcite{jhala2020tutorial} which was written in order to collect in a single place the authors previous efforts in \textcite{vazou2014liquidhaskell} building the LiquidHaskell extension  which opened the way for many implementations of refinement types on top of existing languages.
Notable implementation include the work of \textcite{vekris2016refinementtypescript} adding refinement types for the TypeScript language, the work of \textcite{vazou2018refinementruby} adding refinement types for the Ruby language, and the work of \textcite{sammler2021refinedc} integrating refinement types for the C language.

In \textcite{jhala2020tutorial}, the authors elaborate that the type systems of common high level programming languages can allow types to be \textit{refined} with logic predicates.
The authors call this technique \textit{Refinement types with predicates}~\cite{jhala2020tutorial}.

Refinement types with predicates allows programmers to constrain existing types by using predicates to assert desired properties of the values they want to describe~\cite{jhala2020tutorial}.
They are particularly advantageous because they offer the option to add information to the type system about the invariants and correctness properties a programmer may care about, and, it is done in such a way that, if the programmer desires, no refinement needs to be added and type system can be thought like a typical type system of common higher level languages~\cite{jhala2020tutorial}.

Furthermore, programmers can start with no refinements and incrementally add refinements to ensure important properties about the source program~\cite{jhala2020tutorial}.
They could begin with basic safety requirements, e.g., eliminating division by zero and buffer overflow, or guarantee that a function does not receive an empty collection, and then incrementally add to the specification invariants of custom data types~\cite{jhala2020tutorial}.
Ultimately going all the way to specifying and verifying the correctness of different procedures at compile-time~\cite{jhala2020tutorial}.

By enabling verification on the same language as the programming language, refinement types bridge implementation and proof together~\cite{jhala2020tutorial}.
This approach creates a development cycle were the implementation hits programmers to what properties are important to verify, and the verification hits on how the implementation can be restructured to better express the invariants and enable formal proof~\cite{jhala2020tutorial}.


\section{Extending the simply typed lambda calculus with refinements}

Concerning the discussion of the implementation of a type system with refinements with predicates, the work by \textcite{jhala2020tutorial} explain how to extend the simply typed lambda calculus we explored in Section~\ref{chapter:background:sec:semantic} together with the respective inference rules and the newly added typing relations.

\subsection{Abstract syntax of predicates and constraints}

The authors start the construction by defining the abstract syntax for the predicates and constraints syntactical categories as seen in Figure~\ref{fig:refinement_predicates}.
\begin{figure}[ht]
  \begin{minipage}{\textwidth}
    \begin{alignat*}{4}
      \astprod{p}{x $\mid$ y $\mid$ z $\mid\ \dots$} & \quad & \textit{Variables}              \\
      \astmore{true $\mid$ false}                    & \quad & \textit{Booleans}               \\
      \astmore{0 $\mid$ -1 $\mid$ 1 $\mid\ \dots$ }  & \quad & \textit{Numbers}                \\
      \astmore{%
        $\code{p}_1$ + $\code{p}_2$
        $\mid$ $\code{p}_1$ - $\code{p}_2$
        $\mid$ $\code{p}_1$ * $\code{p}_2$
        $\mid\ \dots$
      }                                              & \quad & \textit{Arithmetic Operations}  \\
      \astmore{$\code{p}_1$ \&\& $\code{p}_2$}       & \quad & \textit{Conjunction}            \\
      \astmore{$\code{p}_1$ || $\code{p}_2$}         & \quad & \textit{Disjunction}            \\
      \astmore{!$\code{p}_1$}                        & \quad & \textit{Negation}               \\
      \astmore{if $\code{p}_1$ then $\code{p}_2$ else $\code{p}_3$}
                                                     & \quad & \textit{Conditional}            \\
      \astmore{$\code{p}_1\bowtie\code{p}_2$}        & \quad & \textit{Interpreted Operations} \\
      \astmore{$f{(\code{p})}$}                      & \quad & \textit{Uninterpreted Function} \\
      \\
      \astprod{c}{p}                                 & \quad & \textit{Predicate}              \\
      \astmore{$\code{c}_1\land\code{c}_2$}          & \quad & \textit{Conjunction}            \\
      \astmore{$\forall\code{\typer{x}{b}.\ p}\implies\code{c}$ }
                                                     & \quad & \textit{Implication}            \\
    \end{alignat*}
  \end{minipage}
  \caption{
    Abstract syntax for refinement predicates and constraints.
    Adapted from~\cite{jhala2020tutorial}.
  }\label{fig:refinement_predicates}
\end{figure}
The syntax used by the authors is a taken from the \textit{quatifier-free} fragments of linear arithmetic and uninterpreted functions~\cite{jhala2020tutorial}.
The definition of predicates \code{p} includes boolean literals, integer literals, variables ranging boolean and integer values, linear arithmetic operations, and boolean operations.
Also, the ternary operator
\begin{equation*}
  \code{if $\code{p}_1$ then $\code{p}_2$ else $\code{p}_3$}
\end{equation*}
was added as an abbreviation to
\begin{equation*}
  (\code{p}_1 \implies x = \code{p}_2)\land(\lnot \code{p}_1 \implies x = \code{p}_3)
\end{equation*}
and the operation $\code{p}_1\bowtie\code{p}_2$ was added to represent all interpreted operators the logic of predicates can be extended with in order to use features from other STM decidable logics (e.g., set operations)~\cite{jhala2020tutorial}.
All the other operations are mapped to uninterpreted functions, were the only thing the SMT solver knows about them is the axiom of congruence which says the for all variables $x$ and $y$, if $x = y$ then $f(x) = f(y)$~\cite{jhala2020tutorial}.
Examples of predicates include the ones we have discussed in the Introduction of this work, we can use \code{0 <= v} to denote the natural numbers and \code{0 <= v \&\& v = length(x)} to denote the values of valid indices of an array.

A type checker with refinements produces what is called \textit{Verification Condition} (VC) constraints \code{c}, which are defined in Figure~\ref{fig:refinement_predicates}.
Constrains are either a single quantifier-free predicate \code{p}, or a conjunction $\code{c}_1 \land \code{c}_2$, or an implication of the form $\forall\code{\typer{x}{T}.p}\implies\code{c}$~\cite{jhala2020tutorial}.
The latter form of a constraint means that for all \code{x} of a given base type \code{b}, if p holds then so must c.
These constraints can than be passed to an SMT that will verify if they are sated, and, if so, it is guaranteed that the program is well typed and is capable of being evaluated~\cite{jhala2020tutorial}.

\subsection{Abstract syntax of terms and types}

The abstract syntax for terms and types presented by the authors is summarized in Figure~\ref{fig:refinement_terms}.

\begin{figure}[ht]
  \begin{minipage}{\textwidth}
    \begin{alignat*}{4}
      \astprod{e}{c}                   & \quad & \textit{Constants}          \\
      \astmore{x}                      & \quad & \textit{Variables}          \\
      \astmore{if x then e else e}     & \quad & \textit{If Expression}      \\
      \astmore{let x = e in e}         & \quad & \textit{Let Binding}        \\
      \astmore{rec x = e:t in e}       & \quad & \textit{Recursive Binding}  \\
      \astmore{$\lambda$x.e}           & \quad & \textit{Function}           \\
      \astmore{e x}                    & \quad & \textit{Application}        \\
      \astmore{e:t}                    & \quad & \textit{Type Annotation}    \\
      \\
      \astprod{t}{b\{x|p\}}            & \quad & \textit{Refined Base}       \\
      \astmore{\fnty{x:t}{t}}          & \quad & \textit{Dependent Function} \\
      \astprod{b}{int}                 & \quad & \textit{Base Integer Type}  \\
      \astmore{bool}                   & \quad & \textit{Base Boolean Type}  \\
      \\
      \astprod{$\Gamma$}{$\emptyset$}  & \quad & \textit{Empty Context}      \\
      \astmore{$\Gamma$, \typer{x}{t}} & \quad & \textit{Variable Binding}   \\
    \end{alignat*}
  \end{minipage}
  \caption{
    Abstract syntax for terms, types, and context of a simply typed lambda calculus with refinements.
    Adapted from~\cite{jhala2020tutorial}.
  }\label{fig:refinement_terms}
\end{figure}

The terms presented by the authors are similar to the terms explores in Section~\ref{chapter:background:sec:semantic}.
They differ only by the function application, which now must receive a variable as parameter, by the introduction of type annotation terms, and, by the introduction of a recursive binding that allows the use of recursive functions.
Also, the constants \code{c} includes primitives such as integers and primitive functions such as \code{sum} and \code{sub} for arithmetic operations.
Furthermore, in the definition of types, we find:
\begin{itemize}
  \item The syntactic variable \code{b} for the languages primitive types (e.g., the set of all integers \code{int});

  \item The refined types \code{t} composed of a base type \code{b} and the definition of a refinement \code{\{x|p\}}, which is a pair of a \textit{value variable} \code{x} and a logical predicate \code{p} from the SMT logic in Figure~\ref{fig:refinement_predicates}.

  \item And, a dependent function type \fnty{x:$\code{t}_1$}{$\code{t}_2$} composed of a binder \code{x} of type $\code{t}_1$ which can appear at the refinement predicate of $\code{t}_2$.
\end{itemize}
In order to reduce verbosity, a refined type which allows all values, written \code{b\{x|true\}}, can be abbreviated into its base type \code{b}.

\subsection{Variable substitution rules for refinement types}

The authors use the notation \code{t[x := y]} to denote a type substitution where all free occurrences of \code{y} inside the type \code{t} are substituted by \code{x}.
For the refined type, the substitution is defined as
\begin{equation*}
  \code{b\{x|p\}[x := y]} = \code{b\{x|p\}}
\end{equation*}
for when there is no free occurrences of x in the refined type, and, defined as
\begin{equation*}
  \code{b\{z|p\}[x := y]} = \code{b\{z|p[x := y]\}}
\end{equation*}
for when \code{x} is a free variable in the refinement type.
Analogously, the substitution of the refined function type is defined as:
\begin{align*}
  \code{(\fnty{x:$\code{t}_1$}{$\code{t}_2$})[x := y]} & = \code{\fnty{x:$\code{t}_1$[x := y]}{$\code{t}_2$}}         \\
  \code{(\fnty{z:$\code{t}_1$}{$\code{t}_2$})[x := y]} & = \code{\fnty{z:$\code{t}_1$[x := y]}{$\code{t}_2$[x := y]}}
\end{align*}

\subsection{Judgments}

For statically ensuring that the simply typed lambda calculus with refinement types could be correctly evaluated, \textcite{jhala2020tutorial} make use of a few new judgments (i.e., relations):
\begin{itemize}
  \item The \textit{Well-sortedness} judgment \ctxtr{}{p} meaning that in the context $\Gamma$ the predicate \code{p} is \textit{well-sorted}, that it, \code{p} has boolean type under the context $\Gamma$ with all refinement types erased using the type relation of the unrefined simply typed lambda calculus~\cite{jhala2020tutorial}.

  \item The \textit{Well-formedness} judgment \ctxtr{}{t} meaning that in the context $\Gamma$ the type \code{t} is \textit{well-formed}, that is, each $t$ refinement predicate is boolean-valued under the variables bound in the context or type~\cite{jhala2020tutorial}.

        \begin{figure}[ht]
          \begin{align*}
            \inferrule*[right=Wf-Base]{
              \ctxtr{\typecx{x}{b}}{p}
            }{
              \ctxtr{}{b\{x|p\}}
            }
             &  &
            \inferrule*[right=Wf-Fun]{
            \ctxtr{}{$\code{t}_1$} \\
              \ctxtr{\typecx{x}{$\code{t}_1$}}{$\code{t}_2$}
            }{
              \ctxtr{}{\fnty{x:$\code{t}_1$}{$\code{t}_2$}}
            }
          \end{align*}
          \caption{
            Inference rules for the Well-formedness judgment. Adapted from \textcite{jhala2020tutorial}.
          }\label{fig:tutorial_well_formedness}
        \end{figure}

        The inference rules in Figure~\ref{fig:tutorial_well_formedness} define the well-formedness judgment.
        The rule Wf-Base defines that, for a refinement \code{b\{x|p\}} to be well-formed in a context $\Gamma$, the predicate \code{p} needs to be well-sorted in the context $\Gamma$ extended with the binding \typer{x}{b} (assuming the erasure of all refinements from $\Gamma$)~\cite{jhala2020tutorial}.
        And, the rule Wf-Fun defines that, for a function type \fnty{x:$\code{t}_1$}{$\code{t}_2$} to be well-formed, the type $\code{t}_1$ needs to be well-formed, and, the type $\code{t}_2$ needs to be well-formed with the context extended with the parameter \typer{x}{$\code{t}_1$}~\cite{jhala2020tutorial}.

  \item The \textit{Entailment} judgment \ctxtr{}{c} meaning that in the context $\Gamma$ the constraint \code{c} is \textit{valid}, that is, the constraint \code{c} `is true'~\cite{jhala2020tutorial}.

        \begin{figure}[ht]
          \begin{align*}
            \inferrule*[right=Ent-Empty]{
              \code{SmtValid(c)}
            }{
              \ctxtre{$\emptyset$}{c}
            }
             &  &
            \inferrule*[right=Ent-Reduce]{
              \ctxtr{}{$\forall\code{\typer{x}{b}.\ p}\implies\code{c}$}
            }{
              \ctxtr{\typecx{x}{b\{x|p\}}}{c}
            }
          \end{align*}
          \caption{
            Inference rules for the entailment judgment. Adapted from \textcite{jhala2020tutorial}.
          }\label{fig:tutorial_entailment}
        \end{figure}

        The inference rules in Figure~\ref{fig:tutorial_entailment} define the entailment judgment.
        The rule Ent-Empty defines that the context $\Gamma$ entails the constraint \code{c} given that \code{c} is SMT solvable~\cite{jhala2020tutorial}.
        Furthermore, the rule Ent-Reduce defines how to reduce the context into the verification condition by removing bindings from the context and translating them into constraints~\cite{jhala2020tutorial}.

  \item The \textit{Subtyping} judgment \subtycxr{}{$\code{t}_1$}{$\code{t}_2$} meaning that in the context $\Gamma$ the type $\code{t}_1$ is a subtype of $\code{t}_2$, that is, all values of $\code{t}_1$ are present in $\code{t}_2$ but the contrary may not be true~\cite{jhala2020tutorial}.

        \begin{figure}[ht]
          \begin{gather*}
            \inferrule*[right=Sub-Base]{
              \ctxtr{}{%
                $\forall$%
                \code{\typer{$\code{x}_1$}{b}.\ $\code{p}_1$}%
                $\implies$%
                \code{$\code{p}_2$[$\code{x}_2$ := $\code{x}_1$]}%
              }
            }{
              \subtycxr{}{b\{$\code{x}_1$|$\code{p}_1$\}}{b\{$\code{x}_2$|$\code{p}_2$\}}
            }
            \\\\
            \inferrule*[right=Sub-Fun]{
              \subtycxr{}{$\code{t}_{2.1}$}{$\code{t}_{1.1}$}
              \\
              \subtycxr{\typecx{$\code{x}_2$}{$\code{t}_{2.1}$}}{$\code{t}_{1.2}$}{$\code{t}_{2.2}$}
            }{
              \subtycxr{}{%
                \fnty{%
                  $\code{x}_1$:$\code{t}_{1.1}$%
                }{%
                  $\code{t}_{1.2}$%
                }%
              }{%
                \fnty{%
                  $\code{x}_2$:$\code{t}_{2.1}$%
                }{%
                  $\code{t}_{2.2}$%
                }%
              }
            }
          \end{gather*}
          \caption{
            Inference rules for the Subtyping judgment. Adapted from \textcite{jhala2020tutorial}.
          }\label{fig:tutorial_subtyping}
        \end{figure}

        The inference rules in Figure~\ref{fig:tutorial_subtyping} define the subtyping judgment.
        The rule Sub-Base defines that, for a refined type \code{b\{$\code{x}_1$|$\code{p}_1$\}} to be a subtype of \code{b\{$\code{x}_2$|$\code{p}_2$\}}, the context has to entail the constraint \code{$\forall$\code{\typer{$\code{x}_1$}{b}.\ $\code{p}_1$}$\implies$\code{$\code{p}_2$[$\code{x}_2$ := $\code{x}_1$]}} which means that if $\code{p}_1$ holds then so must $\code{p}_2$ with all free occurrences of $\code{x}_2$ substituted by $\code{x}_1$~\cite{jhala2020tutorial}.
        And, the rule Sub-Fun decomposes the subtype of function types into the contravariant subtype of the input types and the covariant subtype of the return types~\cite{jhala2020tutorial}.

  \item The \textit{Synthesis} judgment \synthcxr{}{e}{t} meaning that in the context $\Gamma$ the type \code{t} can be generated, or synthesized, for the term \code{e}~\cite{jhala2020tutorial}.
        This judgment is relative to the typing relation for the unrefined simply typed lambda calculus in Section~\ref{chapter:background:sec:semantic}.

        The inference rules in Figure~\ref{fig:tutorial_synthesis} define the synthesis judgment.
        They work together with the \textit{checking} judgment inference rules to build the \textit{bidirectional typing} system described in Section~\ref{sub:bidirectional_typing}.

  \item And, the \textit{Checking} judgment \checkcxr{}{e}{t} meaning that in the context $\Gamma$ the type \code{t} is a valid type for the term \code{e}, that is, constraining the term \code{e} to \textit{synthesize} a type \code{s} where \subtyr{s}{t}~\cite{jhala2020tutorial}.
        This judgment is used to verify that a term is of given annotated type and to push the type annotation inside it's sub-terms to get localized type obligations for inner expressions~\cite{jhala2020tutorial}.

        The inference rules in Figure~\ref{fig:tutorial_checking} define the checking judgment, complementing the inference rules for the synthesis judgment.
\end{itemize}

\subsection{Bidirectional typing}\label{sub:bidirectional_typing}

The typing relation \typecxr{}{x}{T} presented in Section~\ref{ch:background:sec:typing_relation} for the simply typed lambda calculus proves that the program is sound by synthesizing a type for each leaf term on the abstract tree and then subsequently synthesizing the types for the intermediate terms until we synthesize the type for the root term thus proving the program's soundness traversing the abstract tree in a single direction, i.e., from the leaves to the root.

The bidirectional typing rules proposed by \textcite{jhala2020tutorial} levers the type synthesis of terms by combining the synthesis with checking judgments.
We will explore the inference rules present in Figure~\ref{fig:tutorial_synthesis} and Figure~\ref{fig:tutorial_checking} by exploring how they prove the types for each term \code{e} described in Figure~\ref{fig:refinement_terms}.

\begin{figure}[ht]
  \begin{align*}
    \inferrule*[right=Syn-Var]{
    \typer{x}{t} \in \Gamma \\
      \code{self(x, t)} = \code{s}
    }{
      \synthcxr{}{x}{s}
    }
     &  &
    \inferrule*[right=Syn-Const]{
      \code{prim(c) $=$ t}
    }{
      \synthcxr{}{c}{t}
    }
  \end{align*}
  \begin{align*}
    \inferrule*[right=Syn-App]{
    \synthcxr{}{e}{\fnty{y:$\code{t}_1$}{$\code{t}_2$}} \\
      \checkcxr{}{x}{$\code{t}_1$}
    }{
      \synthcxr{}{e x}{$\code{t}_2$[y := x]}
    }
     &  &
    \inferrule*[right=Syn-Ann]{
    \ctxtr{}{t}                                         \\
      \checkcxr{}{e}{t}
    }{
      \synthcxr{}{e:t}{t}
    }
  \end{align*}
  \caption{
    Inference rules for the Synthesis judgment. Adapted from \textcite{jhala2020tutorial}.
  }\label{fig:tutorial_synthesis}
\end{figure}

\begin{figure}[ht]
  \begin{align*}
    \inferrule*[right=Chk-Syn]{
    \synthcxr{}{e}{$\code{t}_1$} \\
      \subtycxr{}{$\code{t}_1$}{$\code{t}_2$}
    }{
      \checkcxr{}{e}{$\code{t}_2$}
    }
     &  &
    \inferrule*[right=Chk-Fun]{
      \checkcxr{\typecx{x}{$\code{t}_1$}}{e}{$\code{t}_2$}
    }{
      \checkcxr{}{$\lambda$x.e}{\fnty{x:$\code{t}_1$}{$\code{t}_2$}}
    }
  \end{align*}

  \begin{equation*}
    \inferrule*[right=Chk-Let]{
      \synthcxr{}{$\code{e}_1$}{$\code{t}_1$} \\
      \checkcxr{\typecx{x}{$\code{t}_1$}}{$\code{e}_2$}{$\code{t}_2$}
    }{
      \checkcxr{}{let x = $\code{e}_1$ in $\code{e}_2$}{$\code{t}_2$}
    }
  \end{equation*}

  \begin{equation*}
    \inferrule*[right=Chk-Rec]{
      \ctxtr{}{$\code{t}_1$} \\
      \checkcxr{\typecx{x}{$\code{t}_1$}}{$\code{e}_1$}{$\code{t}_1$} \\
      \checkcxr{\typecx{x}{$\code{t}_1$}}{$\code{e}_2$}{$\code{t}_2$}
    }{
      \checkcxr{}{rec x = $\code{e}_1$:$\code{t}_1$ in $\code{e}_2$}{$\code{t}_2$}
    }
  \end{equation*}

  \begin{equation*}
    \inferrule*[right=Chk-If]{
      \qquad \quad \ \code{y} \notin \Gamma \\
      \checkcxr{}{x}{bool} \\\\
      \checkcxr{\typecx{y}{int\{y|x\}}}{$\code{e}_1$}{t} \\
      \checkcxr{\typecx{y}{int\{y|!x\}}}{$\code{e}_2$}{t}
    }{
      \checkcxr{}{if x then $\code{e}_1$ else $\code{e}_2$}{t}
    }
  \end{equation*}

  \caption{
    Inference rules for the Checking judgment. Adapted from \textcite{jhala2020tutorial}.
  }\label{fig:tutorial_checking}
\end{figure}

The terms that can be synthesized and their respective inference rules for their synthesis definition are:
\begin{itemize}
  \item Constant terms \code{c} synthesize their primitive type denoted by \code{prim(c)} as formalized by the rule Syn-Const.
        For example, the literals \code{0} and \code{1} for the unrefined type \code{int} are mapped to singleton types as in
        \begin{align*}
          \code{prim(0)}     & = \code{int\{x|x == 0\}} \\
          \code{prim(1)}     & = \code{int\{x|x == 1\}} \\
          \code{prim(true)}  & = \code{bool\{x|x\}}     \\
          \code{prim(false)} & = \code{bool\{x|!x\}}    \\
        \end{align*}
        Also, primitive functions as arithmetic operations are assigned to types that reflect their semantics as in
        \begin{gather*}
          \code{prim(add)} = \fnty{x:int}{\fnty{y:int}{\code{int\{z|z == x + y\}}}} \\
          \code{prim(sub)} = \fnty{x:int}{\fnty{y:int}{\code{int\{z|z == x - y\}}}} \\
        \end{gather*}
        and comparison operations as in
        \begin{align*}
          \code{prim(leq)} & = \fnty{x:int}{\fnty{y:int}{\code{bool\{z|z == x < y\}}}}  \\
          \code{prim(get)} & = \fnty{x:int}{\fnty{y:int}{\code{bool\{z|z == x >= y\}}}} \\
        \end{align*}

  \item Variable terms \code{x} synthesize the type resulting from \code{self(x, t)} if $\typer{x}{t} \in \Gamma$ through the use of the rule Syn-Var where the function \code{self} is defined as
        \begin{equation*}
          \code{self(x, t)} = \begin{cases}
            \code{b\{y| p \&\& y == x\}} & \textrm{if \code{t}} = \code{b\{y|p\}} \\
            \code{t}                     & \textrm{otherwise}
          \end{cases}
        \end{equation*}
        The use of \code{self} guaranties that we further strengthen base type refinements by bring the value of \code{x} into the refinement predicate of \code{t}.
        For example, consider the function \code{abs} for calculating the absolute value on an integer \code{x}
        \begin{align*}
          \code{abs} = \lambda\code{x. let c = leq(0, x) in if (c) then (x) else (sub 0 x)}
        \end{align*}
        of type
        \begin{equation*}
          \typer{abs}{\fnty{x:int}{int\{y| y >= 0 \&\& y >= x\}}}
        \end{equation*}
        If \code{x} had type \code{int\{x|true\}}, the body term of the \code{abs} function would never be a subtype of its output type, while, when extending the type predicate into the type \code{int\{y| true \&\& x == y\}} the generated validity constraint can reason on the value of \code{x} and check the validity of the function \code{abs}~\cite{jhala2020tutorial}.

  \item Application terms \code{e x} synthesize the output type of the function type synthesized for the term \code{e} with its input binder substituted by the real argument variable \code{x} through rule Syn-App.
        Also, the variable \code{x} is constrained by the input type by the checking judgment \checkcxr{}{x}{$\code{t}_1$} in the premises of Syn-App.
        For example, given the context
        \begin{equation*}
          \Gamma = \emptyset\typecx{nat}{int\{x|x > 0\}}\typecx{one}{int\{x| x == 1\}}
        \end{equation*}
        the term \code{add nat one} would synthesize
        \begin{equation*}
          \synthcxr{}{add nat one}{int\{z|z == nat + one\}}
        \end{equation*}

        The only inference rule from the checking judgment definition that can check the premises
        \begin{equation*}
          \checkcxr{}{nat}{int} \quad \textrm{and} \quad \checkcxr{}{one}{int}
        \end{equation*}
        is the rule Chk-Syn, which, will make sure that the variables \code{nat} and \code{one} synthesize types that are subtypes of the respective arguments type for the function \code{add}.

        Furthermore, \textcite{jhala2020tutorial} explain that the function application must receive variables as its arguments in order to properly substitute the input binders in the function's output type.
        The authors explain how to expand the type syntax to include existential types $\exists\code{x:s.t}$ in order to bypass this constraint but, in order to ease exposition and implementation, \textcite{jhala2020tutorial} opt by keeping the constraint because a program can be easily modified to follow the constraint during analysis.
        For example, the program
        \begin{equation*}
          \code{add (add 5 5) one}
        \end{equation*}
        can be easily translated into
        \begin{equation*}
          \code{let aux = add 5 5 in add aux one}
        \end{equation*}
        instead of further complicating the set of inference rules for allowing non-variable terms as arguments.

  \item Annotation terms \code{e:t} synthesize its annotated type \code{t} if the annotated term \code{e} can be checked against the type \code{t} and the type \code{t} is well-formed as defined by rule Syn-Ann.
\end{itemize}

In order to complete the discussion of the bidirectional typing rules, we discuss the terms that can be checked and their checking definition rules:
\begin{itemize}
  \item Function terms \code{$\lambda$x.e} do not directly synthesize a type, those can only be checked against a type through the Chk-Fun inference rule.
        A function can be checked against the type \fnty{x:$\code{t}_1$}{$\code{t}_2$} if its body \code{e} can be checked against $\code{t}_2$ with the context extended with the argument binding \typer{x}{$\code{t}_1$} as defined by the rule Chk-Fun.

  \item Let Binding terms \code{let x = $\code{e}_1$ in $\code{e}_2$} also do not directly synthesize a type requiring to be checked through the Chk-Let inference rule.
        A let binding can be checked against the type $\code{t}_2$ if $\code{e}_2$ can be checked against $\code{t}_2$ with the context extended with the binding \typer{x}{$\code{t}_1$} which has to be synthesized for $\code{e}_1$.

  \item Recursive Binding terms \code{rec x = $\code{e}_1$:$\code{t}_1$ in $\code{e}_2$} is a similar case to let bindings and can be checked through the Chk-Rec inference rule.
        A recursive binding differs from the let binding by requiring a type annotation in the expression $\code{e}_1$, and by pushing the type binding \typer{x}{$\code{t}_1$} into the checking of $\code{e}_1$ instead of synthesizing its type.

  \item If Expression terms \code{if x then $\code{e}_1$ else $\code{e}_2$} also do not directly synthesize a type requiring to be checked through the Chk-If inference rule.
        An if expression can be checked to have type \code{t} in a context $\Gamma$ if both $\code{e}_1$ and $\code{e}_2$ can be checked to have type \code{t}.
        Furthermore, the inference rule Chk-If differs from a classic definition of an if expression term by adding into the context a \textit{fresh} \code{y} variable (i.e., not present in the context) bound to a refinement that captures the exact value of the condition \code{x} when checking the terms $\code{e}_1$ and $\code{e}_2$~\cite{jhala2020tutorial}.
        If this binding was not present in the context, the checking relation
        \begin{gather*}
          \code{not} = \lambda\code{x.if x then false else true} \\
          \checkcxr{}{not}{\fnty{x:bool}{bool\{b|b == !x\}}}
        \end{gather*}
        would not be able to prove that the output type of the \code{not} function is the inverse of the condition value receive as input~\cite{jhala2020tutorial}.
\end{itemize}
Last, the terms for constants, variables, applications, and, annotations, can be checked by the use of the Chk-Syn inference rule.
The subsumpiton rule Chk-Syn connects the checking and synthesis judgments by defining that if a term \code{e} synthesized a type $\code{t}_1$ and the type $\code{t}_1$ is subsumed by type $\code{t}_2$ (i.e., $\code{t}_1$ is subtype of $\code{t}_2$) then we can check \code{e} against $\code{t}_2$.

\subsection{An implementation of a verification condition generator}

In order to verify the soundness of a program for the proposed simply typed lambda calculus with refinements, \textcite{jhala2020tutorial} describes an implementation of a verification condition (VC) generator.
The proposed generator takes as input the program's abstract syntax and outputs a VC constraint \code{c} which can be validated by an SMT solver and implies the program's soundness~\cite{jhala2020tutorial}.
More specifically, the authors describe an algorithm implementation for the subtyping, synthesis and checking judgments.

The algorithms make use of an implication constraint written \implcons{x}{t}{c} defined as
\begin{equation*}
  \implcons{x}{t}{c} = \begin{cases}
    \forall\code{x:b. p[y := x]} \implies \code{c} & \textrm{if \code{t}} = \code{b\{y|p\}} \\
    \code{c}                                       & \textrm{otherwise}
  \end{cases}
\end{equation*}

The subtyping relation can be implemented as a function \code{sub} that takes two types $\code{t}_1$ and $\code{t}_2$ as input and outputs a constraint \code{c} which the validity of \code{c} implies the subtyping relation \subtyr{$\code{t}_1$}{$\code{t}_2$} formalized by \textcite{jhala2020tutorial} as the following proposition:
\begin{equation*}
  \textrm{if \code{sub($\code{t}_1$, $\code{t}_2$)} = \code{c} and \ctxtr{}{c} then \subtycxr{}{$\code{t}_1$}{$\code{t}_2$}}
\end{equation*}
The function \code{sub} is implemented as shown in Figure~\ref{fig:subtyping_function_impl} where each definition is relative to the inference rules Sub-Base and Sub-Fun from Figure~\ref{fig:tutorial_subtyping}.

\begin{figure}[ht]
  \begin{align*}
    \code{sub(b\{$\code{x}_1$, $\code{p}_1$\}, b\{$\code{x}_2$, $\code{p}_2$\})}
     & = \forall\code{x:b. $\code{p}_1$} \implies \code{$\code{p}_2$[$\code{x}_2$ := $\code{x}_1$]} \\
    \code{sub(\fnty{$\code{x}_1$:$\code{t}_{1.1}$}{$\code{t}_{1.2}$}, \fnty{$\code{x}_2$:$\code{t}_{2.1}$}{$\code{t}_{2.2}$})}
     & = c_1 \land \implcons{$\code{x}_2$}{$\code{t}_{2.1}$}{$c_2$}                                 \\
     & \textrm{where:}                                                                              \\
     & \quad c_1 = \code{sub($\code{t}_{1.1}$, $\code{t}_{2.1}$)}                                   \\
     & \quad c_2 = \code{sub($\code{t}_{1.2}$[$\code{x}_1$ := $\code{x}_2$], $\code{t}_{2.2}$)}
  \end{align*}
  \caption{
    Subtyping function for the subtyping relation.
    Adapted from \textcite{jhala2020tutorial}.
  }\label{fig:subtyping_function_impl}
\end{figure}

The synthesis relation can be implemented as a function \code{synth} that takes as input the context $\Gamma$ and a term \code{e} and outputs the tuple \code{(c, t)} where \code{t} is the type of \code{e} in $\Gamma$ and the constraint \code{c}'s validity implies the synthesis relation as formalized by \textcite{jhala2020tutorial} in the proposition:
\begin{equation*}
  \textrm{if \code{synth($\Gamma$, e)} = \code{(c, t)} and \ctxtr{}{c} then \synthcxr{}{\code{e}}{\code{t}}}
\end{equation*}
The function \code{synth} is implemented as shown in Figure~\ref{fig:synth_function_impl} where each definition is relative to the inference rules Syn-Var, Syn-Const, Syn-Ann, and Syn-App from Figure~\ref{fig:tutorial_synthesis}.

\begin{figure}[ht]
  \begin{align*}
    \code{synth($\Gamma$, x)}
     & = \code{(true, self(x, t))}                                                           \\
     & \textrm{where:}                                                                       \\
     & \quad \typer{x}{t} \in \Gamma                                                         \\
    \code{synth($\Gamma$, c)}
     & = \code{(true, prim(c))}                                                              \\
    \code{synth($\Gamma$, e y)}
     & = \code{($c_1 \land c_2$, $\code{t}_2$[x := y])}                                      \\
     & \textrm{where:}                                                                       \\
     & \quad \code{($c_1$, \fnty{x:$\code{t}_1$}{$\code{t}_2$})} = \code{synth($\Gamma$, e)} \\
     & \quad c_2 = \code{check($\Gamma$, y, $\code{t}_1$)}                                   \\
    \code{synth($\Gamma$, e:t)}
     & = \code{($c$, t)}                                                                     \\
     & \textrm{where:}                                                                       \\
     & \quad c = \code{check($\Gamma$, e, t)}
  \end{align*}
  \caption{
    Synthesis function for the synthesis relation.
    Adapted from \textcite{jhala2020tutorial}.
  }\label{fig:synth_function_impl}
\end{figure}

Last, the checking relation can be implemented as a function \code{check} that takes as input the context $\Gamma$, a term \code{e}, and, an expected type \code{t}, and output a constraint \code{c} which validity implies the synthesis relation as formalized by \textcite{jhala2020tutorial} in the proposition:
\begin{equation*}
  \textrm{if \code{check($\Gamma$, e, t)} = \code{c} and \ctxtr{}{c} then \checkcxr{}{\code{e}}{\code{t}}}
\end{equation*}
The function \code{check} is implemented as shown in Figure~\ref{fig:check_function_impl} where each definition is relative to the inference rules Chk-Fun, Chk-Let, Chk-Rec, Chk-If, and Chk-Syn, present in Figure~\ref{fig:tutorial_checking}.

\begin{figure}[ht]
  \begin{align*}
    \code{check($\Gamma$, e, $\code{t}_2$)}
     & = c_1 \land c_2                                                         \\
     & \textrm{where:}                                                         \\
     & \quad (c_1,\ \code{t}_1) = \code{synth($\Gamma$, e)}                    \\
     & \quad c_2 = \code{sub($\code{t}_1$, $\code{t}_2$)}                      \\
    \code{check($\Gamma$, $\lambda$x.e, \fnty{x:$\code{t}_1$}{$\code{t}_2$})}
     & = \implcons{x}{$\code{t}_1$}{$c$}                                       \\
     & \textrm{where:}                                                         \\
     & \quad c = \code{check($\Gamma_1$, e, t)}                                \\
     & \quad \Gamma_1 = \Gamma\typecx{x}{$\code{t}_1$}                         \\
    \code{check($\Gamma$, let x = $\code{e}_1$ in $\code{e}_2$, $\code{t}_2$)}
     & = c_1 \land \implcons{x}{$\code{t}_1$}{$c_2$}                           \\
     & \textrm{where:}                                                         \\
     & \quad (c_1,\ \code{t}_1) = \code{synth($\Gamma$, $\code{e}_1$)}         \\
     & \quad c_2 = \code{check($\Gamma_1$, $\code{e}_2$, $\code{t}_2$)}        \\
     & \quad \Gamma_1 = \Gamma\typecx{x}{$\code{t}_1$}                         \\
    \code{check($\Gamma$, rec x = $\code{e}_1$:$\code{t}_1$ in $\code{e}_2$, $\code{t}_2$)}
     & = c_1 \land c_2                                                         \\
     & \textrm{where:}                                                         \\
     & \quad c_1 = \code{check($\Gamma_1$, $\code{e}_1$, $\code{t}_1$)}        \\
     & \quad c_2 = \code{check($\Gamma_1$, $\code{e}_2$, $\code{t}_2$)}        \\
     & \quad \Gamma_1 = \Gamma\typecx{x}{$\code{t}_1$}                         \\
    \code{check($\Gamma$, if x then $\code{e}_1$ else $\code{e}_2$, t)}
     & = c_1 \land c_2                                                         \\
     & \textrm{where:}                                                         \\
     & \quad c_1 = \implcons{y}{int\{x|x\}}{check($\Gamma$, $\code{e}_1$, t)}  \\
     & \quad c_2 = \implcons{y}{int\{x|!x\}}{check($\Gamma$, $\code{e}_2$, t)}
  \end{align*}
  \caption{
    Synthesis function for the synthesis relation.
    Adapted from \textcite{jhala2020tutorial}.
  }\label{fig:check_function_impl}
\end{figure}

\chapter{The LLVM intermediate representation}\label{ch:llvm}

To do. Work in progress.

\chapter{The Ekitai language}\label{chapter:proposal}

In this chapter we will describe the design of the \textit{Ekitai} programming and the implementation of its front end to the LLVM intermediate representation.

\section{The Ekitai's lexer}

The lexical aspects of the Ekitai programming language are defined by Figures~\ref{fig:ekitai_tokens_helper} and~\ref{fig:ekitai_tokens}.
We decided to use an automatic lexer generator since we found no advantage in building one by hand and the automatic parser generator used was capable of lexing all token patterns in the specification.\footnote{The automatic lexer generator used in this work is Logos~\cite{logos2020}.}

\begin{figure}[ht]
  \small
  \begin{align*}
    \code{digit} & \rightarrow \ [\code{0-9}]                  \\
    \code{alpha} & \rightarrow \ [\code{a-zA-Z}]               \\
    \code{word}  & \rightarrow \ [\code{[:digit:][:alpha:]\_}]
  \end{align*}
  \caption{
    Building block regular definitions for token patterns of Figure~\ref{fig:ekitai_tokens}.
  }\label{fig:ekitai_tokens_helper}
\end{figure}
\begin{figure}[ht]
  \centering
  \begin{subfigure}[b]{0.3\textwidth}
    \small
    \begin{minipage}{\textwidth}
      \begin{align*}
        \code{Comma}            & \rightarrow \ \code{,}  \\
        \code{Collon}           & \rightarrow \ \code{:}  \\
        \code{SemiColon}        & \rightarrow \ \code{;}  \\
        \code{OpenParenthesis}  & \rightarrow \ \code{(}  \\
        \code{CloseParenthesis} & \rightarrow \ \code{)}  \\
        \code{OpenBraces}       & \rightarrow \ \code{\{} \\
        \code{CloseBraces}      & \rightarrow \ \code{\}} \\
        \code{Equals}           & \rightarrow \ \code{=}  \\
        \code{Plus}             & \rightarrow \ \code{+}  \\
        \code{Minus}            & \rightarrow \ \code{-}  \\
        \code{Asterisk}         & \rightarrow \ \code{*}  \\
        \code{Ampersand}        & \rightarrow \ \code{\&}
      \end{align*}
    \end{minipage}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\textwidth}
    \begin{minipage}{\textwidth}
      \small
      \begin{align*}
        \code{Slash}             & \rightarrow \ \code{/}    \\
        \code{Percent}           & \rightarrow \ \code{\%}   \\
        \code{Greater}           & \rightarrow \ \code{>}    \\
        \code{Less}              & \rightarrow \ \code{<}    \\
        \code{Exclamation}       & \rightarrow \ \code{!}    \\
        \code{Pipe}              & \rightarrow \ \code{|}    \\
        \code{DoubleCollon}      & \rightarrow \ \code{::}   \\
        \code{DoubleEquals}      & \rightarrow \ \code{==}   \\
        \code{ExclamationEquals} & \rightarrow \ \code{!=}   \\
        \code{GreaterEquals}     & \rightarrow \ \code{>=}   \\
        \code{LessEquals}        & \rightarrow \ \code{<=}   \\
        \code{DoublePipe}        & \rightarrow \ \code{||}   \\
        \code{DoubleAmpersand}   & \rightarrow \ \code{\&\&}
      \end{align*}
    \end{minipage}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\textwidth}
    \small
    \begin{minipage}{\textwidth}
      \begin{align*}
        \code{ThinArow} & \rightarrow \ \code{->}    \\
        \code{FatArow}  & \rightarrow \ \code{=>}    \\
        \code{FnKw}     & \rightarrow \ \code{fn}    \\
        \code{LetKw}    & \rightarrow \ \code{let}   \\
        \code{IfKw}     & \rightarrow \ \code{if}    \\
        \code{ElseKw}   & \rightarrow \ \code{else}  \\
        \code{TrueKw}   & \rightarrow \ \code{true}  \\
        \code{FalseKw}  & \rightarrow \ \code{false} \\
        \code{TypeKw}   & \rightarrow \ \code{type}  \\
        \code{MatchKw}  & \rightarrow \ \code{match} \\
        \code{NewKw}    & \rightarrow \ \code{new}
      \end{align*}
    \end{minipage}
  \end{subfigure}
  \begin{subfigure}[b]{\textwidth}
    \small
    \begin{minipage}{\textwidth}
      \begin{align*}
        \code{Identifier} & \rightarrow \ [\code{[:alpha:]\_}]{[\code{[:word:]}]}^* \\
        \code{Integer}    & \rightarrow \
        \code{[:digit:]}{[\code{[:digit:]\_}]}^*[\code{[:Identifier:]}\epsilon]
      \end{align*}
    \end{minipage}
  \end{subfigure}
  \caption{
    Token names and patterns for the Ekitai lexer.
  }\label{fig:ekitai_tokens}
\end{figure}

Note in Figure~\ref{fig:ekitai_tokens} that the token name \code{Identifier} has, in its described language, common strings with token names for keywords, i.e., token names ending in \code{Kw}. The automatic lexer algorithm solves this problem by applying two disambiguation rules:
\begin{itemize}
  \item Longer matching lexemes have priority over shorter matching lexemes, i.e., \code{>=} has higher priority than \code{>} for token names \code{GreaterEquals} and \code{Greater}.
  \item Groups have lower priority that singular regular expressions, i.e., $[\code{ab}]$ has less priority than \code{a} and \code{b}, and, permutations have the lowest priority, i.e, $a^*$.
\end{itemize}
Furthermore, the token name \code{Integer} must start with a digit and is followed by any number of digits and underscores, allowing to separate long numbers (i.e., \code{100\_000}), with an optional identifier at the end, allowing for the use of suffixes (i.e., \code{42i64}).


\section{The Ekitai's parser}

The Ekitai's grammar was built with a handwritten Pratt parser in mind.
In order to keep the grammar out of clutter, we will present the grammar using simplified token names.
Instead of writing \code{LetKw} or \code{DoubleAmpersand} we will stick to writing \code{let} and \code{\&\&} respectively.
Furthermore, we will use \code{Id} for short of \code{Identifier} and \code{Int} for short of \code{Integer}.

First we define the syntax structure for the source program's top level items by presenting the grammar productions in Figure~\ref{fig:ekitai_syntax_toplevel}.

\begin{figure}[ht]
  \begin{alignat*}{2}
    \bnfprod{SourceFile}{
      \bnfvar{ItemList}
    } \\
    \bnfprod{ItemList}{
      \bnfvar{Item}
      \bnfvar{ItemList}
      \bnfor{$\epsilon$}
    } \\
    \bnfprod{Item}{
      \bnfvar{FnDef}
      \bnfor{\bnfvar{TypeDef}}
    } \\
    \bnfprod{TypeDef}{
      \bnfter{type}
      \bnfvar{Name}
      \bnfter{\{}
      \bnfvar{ValueConsList}
      \bnfter{\}}
    } \\
    \bnfprod{ValueConsList}{
      \bnfvar{ValueCons}
      \bnfter{,}
      \bnfvar{ValueConsList}
    } \\
    \bnfmore{
      \bnfvar{ValueCons} \bnfter{,}
      \bnfor{\bnfvar{ValueCons}}
    } \\
    \bnfprod{ValueCons}{
      \bnfvar{Name} \bnfter{(} \bnfvar{TypeList} \bnfter{)}
    } \\
    \bnfprod{TypeList}{
      \bnfvar{Type} \bnfter{,} \bnfvar{TypeList}
    } \\
    \bnfmore{
      \bnfvar{Type}
      \bnfor{$\epsilon$}
    } \\
    \bnfprod{FnDef}{
      \bnfter{fn}
      \bnfvar{Name}
      \bnfter{(}
      \bnfvar{ParamList}
      \bnfter{)}
      \bnfter{->}
      \bnfvar{BlockExpr}
    } \\
    \bnfprod{ParamList}{
      \bnfvar{Param} \bnfter{,} \bnfvar{ParamList}
    } \\
    \bnfmore{
      \bnfvar{Param}
      \bnfor{$\epsilon$}
    } \\
    \bnfprod{Param}{
      \bnfvar{Name}
      \bnfter{:}
      \bnfvar{Type}
    } \\
    \bnfprod{Name}{
      \bnfter{Id}
    }
  \end{alignat*}
  \caption{
    Top level grammar productions for the Ekitai's parser grammar.
  }\label{fig:ekitai_syntax_toplevel}
\end{figure}
The variable \bnfvar{SourceFile} is the start symbol of the Ekytai's syntax grammar.
It generates a list of Items \bnfvar{FnDef} and \bnfvar{TypeDef} for function definitions and type definitions respectively.

From the grammar in~\ref{fig:ekitai_syntax_toplevel}, we are able to see the structure of the top level constructs. For example, the following program illustrates a \bnfvar{SourceFile} with two \bnfvar{Item} in the \bnfvar{ItemList}:
\begin{center}
  \begin{minipage}{0.8\textwidth}
    \begin{rustcode}
type SomeTypeName {
  SomeConstructorName(Type1, Type2, ... ),
  ...
}
fn some_fn_name(arg_name1: Type1, arg_name_2: Type2, ...) -> {
  ...
}
\end{rustcode}
  \end{minipage}
\end{center}
Further, when defining the induction rules for semantic analysis we will use the \bnfvar{Path} variable to index the constructors inside the \code{type} definition, for example the path
\begin{equation*}
  \code{SomeTypeName::SomeConstructorName}
\end{equation*}
will index the constructor
\begin{equation*}
  \code{SomeConstructorName}
\end{equation*}
inside the type definition for
\begin{equation*}
  \code{SomeTypeName}
\end{equation*}
The \bnfvar{Path} variable is defined on Figure~\ref{fig:ekitai_syntax_terms} as part of Ekitais's grammar for terms.

Beyond the top level items, we still have to define the variables for \bnfvar{Type} and for \bnfvar{BlockExpr}.
The \bnfvar{BlockExpr} variable is part of the grammar for Ekitai's terms as shown in Figure~\ref{fig:ekitai_syntax_terms}.
While the \bnfvar{Type} variable is part of the grammar for Ekitai's types defined in Figure~\ref{fig:ekitai_syntax_types}.

\begin{figure}[ht]
  \centering
  \small
  \begin{subfigure}[b]{0.4\textwidth}
    \begin{alignat*}{2}
      \bnfprod{Expr}{
        \bnfvar{Literal}
      } \\
      \bnfmore{
        \bnfvar{Path}
      } \\
      \bnfmore{
        \bnfter{(}
        \bnfvar{Expr}
        \bnfter{)}
      } \\
      \bnfmore{
        \bnfvar{Expr}
        \bnfvar{InfixOp}
        \bnfvar{Expr}
      } \\
      \bnfmore{
        \bnfvar{PrefixOp}
        \bnfvar{Expr}
      } \\
      \bnfmore{
        \bnfvar{Expr}
        \bnfter{(}
        \bnfvar{ArgList}
        \bnfter{)}
      } \\
      \bnfmore{
        \bnfvar{BlockExpr}
      } \\
      \bnfmore{
        \bnfter{if}
        \bnfvar{Expr}
        \bnfvar{BlockExpr}
      } \\
      \bnfcont{
        \bnfter{else}
        \bnfvar{BlockExpr}
      } \\
      \bnfmore{
        \bnfvar{MatchExpr}
      } \\
      \bnfmore{
        \bnfter{new}
        \bnfvar{Expr}
      }
    \end{alignat*}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \begin{alignat*}{2}
      \bnfprod{InfixOp}{
        \bnfter{+}
        \bnfor{\bnfter{-}}
        \bnfor{\bnfter{*}}
        \bnfor{\bnfter{/}}
        \bnfor{\bnfter{\%}}
      } \\
      \bnfmore{
        \bnfter{>}
        \bnfor{\bnfter{>=}}
        \bnfor{\bnfter{<}}
        \bnfor{\bnfter{<=}}
      } \\
      \bnfmore{
        \bnfter{==}
        \bnfor{\bnfter{!=}}
        \bnfor{\bnfter{\&\&}}
        \bnfor{\bnfter{||}}
      } \\
      \bnfprod{PrefixOp}{
        \bnfter{-}
        \bnfor{\bnfter{!}}
        \bnfor{\bnfter{*}}
        \bnfor{\bnfter{\&}}
      } \\
      \bnfprod{ArgList}{
        \bnfvar{Expr}
        \bnfter{,}
        \bnfvar{ArgList}
      } \\
      \bnfmore{
        \bnfvar{Expr}
        \bnfor{$\epsilon$}
      } \\
      \bnfprod{BlockExpr}{
        \bnfter{\{}
        \bnfvar{StatementList}
        \bnfvar{Expr}
        \bnfter{\}}
      } \\
      \bnfprod{StatementList}{
        \bnfvar{Statement} \bnfter{;} \bnfvar{StatementList}
      } \\
      \bnfmore{
        $\epsilon$
      } \\
      \bnfprod{MatchExpr}{
        \bnfter{match}
        \bnfvar{Expr}
        \bnfter{\{}
        \bnfvar{CaseList}
        \bnfter{\}}
      } \\
      \bnfprod{CaseList}{
        \bnfvar{Pattern} \bnfter{=>} \bnfvar{Expr} \bnfter{,} \bnfvar{CaseList}
      } \\
      \bnfmore{
        $\epsilon$
      }
    \end{alignat*}
  \end{subfigure}
  \begin{subfigure}[b]{\textwidth}
    \begin{alignat*}{2}
      \bnfprod{Statement}{
        \bnfter{let}
        \bnfvar{Name}
        \bnfter{=}
        \bnfvar{Expr}
      } \\
      \bnfprod{Pattern}{
        \bnfvar{Path}
        \bnfter{(}
        \bnfvar{NameList}
        \bnfter{)}
      } \\
      \bnfprod{NameList}{
        \bnfvar{Name} \bnfter{,} \bnfvar{NameList}
      } \\
      \bnfmore{
        \bnfvar{Name}
        \bnfor{$\epsilon$}
      } \\
      \bnfprod{Path}{
        \bnfvar{Name}
        \bnfter{::}
        \bnfvar{Name}
      } \\
      \bnfmore{
        \bnfvar{Name}
      } \\
      \bnfprod{Literal}{
        \bnfter{Int}
        \bnfor{\bnfter{true}}
        \bnfor{\bnfter{false}}
      } \\
    \end{alignat*}
  \end{subfigure}
  \caption{
    Expression grammar productions for the Ekitai's parser grammar.
  }\label{fig:ekitai_syntax_terms}
\end{figure}

\begin{figure}[ht]
  \centering
  \begin{alignat*}{2}
    \bnfprod{Type}{
      \bnfvar{Name}
    } \\
    \bnfmore{
      \bnfter{*}
      \bnfvar{Type}
    } \\
    \bnfmore{
      \bnfter{\{}
      \bnfvar{Name}
      \bnfter{:}
      \bnfvar{Type}
      \bnfter{|}
      \bnfvar{Expr}
      \bnfter{\}}
    }
  \end{alignat*}
  \caption{
    Type grammar productions for the Ekitai's parser grammar.
  }\label{fig:ekitai_syntax_types}
\end{figure}

The syntax for refinements defined in Figure~\ref{fig:ekitai_syntax_types} allows for any expression as the refinement predicate.
We do that in order to reuse the parser for \bnfvar{Expr} and leave to the semantic analysis to restrict what sentences from \bnfvar{Expr} are allowed as refinement predicates.

\section{The Ekitai's type system}

To do. Work in progress.

\section{The Ekitai's LLVM intermediate code generator}

To do. Work in progress.

\section{Conclusion}

To do. Work in progress.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Elementos pós-textuais                                       %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\postextual{}

\printbibliography{}

\end{document}
