\documentclass[
  oneside,
  english,
  coorientadorbanca,
  noabntexcite
]{ufsc-thesis-rn46-2019}

% font config
\usepackage{fontspec}

\usepackage{polyglossia}
\setdefaultlanguage{english}
\setotherlanguages{portuguese}

% math and code typesetting
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{mathpartir} % typesetting inference rules
\usepackage[outputdir=build, newfloat]{minted}

% graphics
\usepackage{pdfpages} % including pdf files
\usepackage{tikz}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{positioning}
\usetikzlibrary{trees}

% text revisions
\usepackage{paper_alcides}

% citation and references
\usepackage{hyperref} % links
\usepackage{csquotes} % Quoting authors
\usepackage{caption} % configures hyperref to point to top of images
\usepackage{subcaption} % adds subfigures

% bibliography
\usepackage[style=abnt]{biblatex}
\addbibresource{bibliography/llvm.bib}
\addbibresource{bibliography/pratt.bib}
\addbibresource{bibliography/refinements.bib}
\addbibresource{bibliography/thesis.bib}

% commands
\setmainfont{lmroman10}[
  Path=./fonts/lm/,
  OpticalSize=18,
  Scale=MatchLowercase,
  LetterSpace=1.0,
  UprightFont=*-regular,
  BoldFont=*-bold,
  ItalicFont=*-italic,
  BoldItalicFont=*-bolditalic,
  FontFace = {m}{\shapedefault}{*-regular},
  FontFace = {m}{it}{*-italic},
  FontFace = {b}{\shapedefault}{*-bold},
  FontFace = {b}{it}{*-bolditalic},
]

\setsansfont{lmsans10}[
  Path=./fonts/lm/,
  OpticalSize=18,
  LetterSpace=1.0,
  Scale=MatchLowercase,
  UprightFont=*-regular,
  BoldFont=*-bold,
  ItalicFont=*-oblique,
  BoldItalicFont=*-boldoblique,
  FontFace = {m}{\shapedefault}{*-regular},
  FontFace = {m}{sl}{*-oblique},
  FontFace = {b}{\shapedefault}{*-bold},
  FontFace = {b}{sl}{*-boldoblique},
]

\setmonofont{lmmonolt10}[%
  Path=./fonts/lm/,
  OpticalSize=18,
  LetterSpace=1.0,
  Scale=MatchLowercase,
  UprightFont=*-regular,
  BoldFont=*-bold,
  ItalicFont=*-oblique,
  BoldItalicFont=*-boldoblique,
  FontFace = {m}{\shapedefault}{*-regular},
  FontFace = {m}{sl}{*-oblique},
  FontFace = {b}{\shapedefault}{*-bold},
  FontFace = {b}{sl}{*-boldoblique}%,
]

\newfontfamily\scpfamily{SourceCodePro}[
  NFSSFamily=sourcecodepro,
  Path=./fonts/sourcecodepro/,
  OpticalSize=18,
  Scale=MatchLowercase,
  UprightFont=*-Medium,
  ItalicFont=*-MediumIt,
  BoldFont=*-Bold,
  BoldItalicFont=*-BoldIt,
  FontFace = {el}{\shapedefault}{*-ExtraLight},
  FontFace = {el}{it}{*-ExtraLightIt},
  FontFace = {l}{\shapedefault}{*-Light},
  FontFace = {l}{it}{*-LightIt},
  FontFace = {sl}{\shapedefault}{*-Regular},
  FontFace = {sl}{it}{*-It},
  FontFace = {m}{\shapedefault}{*-Medium},
  FontFace = {m}{it}{*-MediumIt},
  FontFace = {sb}{\shapedefault}{*-Semibold},
  FontFace = {sb}{it}{*-SemiboldIt},
  FontFace = {b}{\shapedefault}{*-Bold},
  FontFace = {b}{it}{*-BoldIt},
  FontFace = {ub}{\shapedefault}{*-Black},
  FontFace = {ub}{it}{*-BlackIt},
]

\newminted{rust}{
  fontfamily=sourcecodepro,
  breaklines,
  linenos,
  fontsize=\footnotesize
}

\newmintedfile[rustfile]{rust}{
  fontfamily=sourcecodepro,
  breaklines,
  linenos,
  fontsize=\footnotesize
}

\newcommand\myflowchartlowermargin{1.2cm}

\def\bnfdef{::=}
\newcommand{\codett}[1]{\text{\scpfamily#1}}
\newcommand{\code}[1]{\text{\scpfamily\setlength\spaceskip{0.35em}#1}}
\newcommand{\bnfvar}[1]{\codett{#1}}
\newcommand{\bnfter}[1]{\textrm{`}\codett{#1}\textrm{'}}
\newcommand{\bnfor}[1]{{$\mid$} #1}
\newcommand{\bnfprod}[2]{\bnfvar{#1} & \ & \bnfdef{} & \ \code{#2}}
\newcommand{\bnfmore}[1]{            & \ & \mid{}    & \ \code{#1}}

\newcommand{\token}[1]{$\langle\codett{#1}\rangle$}

\newcommand{\typer}[2]{\code{#1 \codett{:} #2}}
\newcommand{\typecxr}[3]{\code{{$\Gamma$}#1 {$\vdash$} \typer{#2}{#3}}}
\newcommand{\typecx}[2]{,\;\typer{#1}{#2}}

\newcommand{\tokenstream}[4]{
  & \langle\text{#1,} && \text{\{\textit{lexeme}: #2,} && \text{\textit{begin}: #3,} && \text{\textit{end}: #4\}}\rangle%
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Configurações da classe (dados do trabalho)                  %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Informações para capa e folha de rosto/certificacao

% Caso o título contenha alguma porção LaTeX ilegível, defina um título
% alternativo opcional com []'s para ser usado no campo Title do PDF
% IMPORTANTE: Os títulos deveriam ser iguais. Apenas use um título
% alternativo se o título não puder ser expresso com letras e números
\titulo{A programming language with refinement types and its LLVM-IR front end implementation.}

\autor{Bernardo Ferrari Mendonça}
\data{24 de Fevereiro de 2022} % ou \today
\instituicao{Universidade Federal de Santa Catarina}
\centro{Centro Tecnológico}
\local{Florianópolis} % Apenas cidade! Sem estado
\programa{Programa de Graduação em Ciência da Computação}
% Os dois próximos itens são usados para gerar o \preambulo
%\tese % ou \dissertacao ou \tcc
%\titulode{doutor em Ciência da Computação}

%%% Atenção! No caso de TCC, além de usar \tcc, outros comandos devem ser fornecidos:
%%%
\tcc{}
\departamento{Departamento de Informática e Estatística}
\curso{Ciência da Computação}
\titulode{Bacharel em Ciência da Computação}
% %% Para TCCs, orientadores e coorientadores podem ser externos, logo a
% %% BU exige que sua afiliação seja explicitada. Por padrão, assume-se
% %% UFSC. Você pode alterar a afiliação com os comandos abaixo:

% Orientador, coorientador, membros da banca e coordenador
% As regras da BU agora exigem que Dr. apareça **depois** do nome
% Dica: para gerar Profᵃ. use Prof\textsuperscript{a}.
% Dica 2: para feminino use \orientadora e \coorientadora
\orientadorext{Prof.\ Alcides Miguel Cachulo Aguiar Fonseca, Dr.}{Universidade de Lisboa}
\coorientadorext{Prof.\ Rafael de Santiago, Dr.}{Universidade Federal de Santa Catarina}
\membrabanca{Prof\textsuperscript{a}. Jerusa Marchi, Dr.}{Universidade Federal de Santa Catarina}
% Dica: se feminino, \coordenadora
\coordenador{Prof.\ Jean Everson Martina, Dr.}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Principais elementos pré-textuais                            %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Inicia parte pré-textual do documento capa, folha de rosto, folha de
% aprovação, aprovação, resumo, lista de tabelas, lista de figuras, etc.
\pretextual{}
\imprimircapa{}
\imprimirfolhaderosto*
% Atenção! \cleardoublepages são inseridos automaticamente
% Atenção! esse \protect é importante
\protect{}%\incluirfichacatalografica{ficha.pdf}
\imprimirfolhadecertificacao{}

% Listas de "coisas". O * no final faz com que as listas não sejam
% incluídas como entratas do sumário (\tableofcontents)

% \listoftables*
% \listofalgorithms*
% \listoffigures*
% \tableofcontents*

\begin{dedicatoria}
  This work is dedicated to my mother, Marcela Ferrari, my sister Camila Ferrari, and to my grandparents Márcio Ferrari and Cláudia Ferrari who plowed the field where I bloom.
\end{dedicatoria}

\begin{agradecimentos}
  TODO ack
\end{agradecimentos}

\begin{epigrafe}
  TODO epigrafe
\end{epigrafe}

\begin{resumo}[Resumo]
  TODO Resumo

  % Atenção! a BU exige separação através de ponto (.). Ela recomanda de 3 a 5 keywords
  \vspace{\baselineskip}
  \textbf{Palavras-chave:} linguagem de programação\@. tipos refinados\@. representação intermediária de código\@. compilador\@. LLVM\@. LLVM-IR\@.
\end{resumo}

\begin{abstract}
  TODO abstract

  \vspace{\baselineskip}
  \textbf{Keywords:} Keyword. Another Compound Keyword. Bla.
\end{abstract}

\listoffigures*  % O * evita que apareça no sumário

% \begin{listadesimbolos}
%   \($1\)   & Atribuição \\
%   \($1\)   & Quantificação existencial \\
%   $\rightarrow$   & Implicação \\
%   $\wedge$   & E lógico \\
%   $\vee$   & Ou lógico \\
%   $\neg$   & Negação lógica \\
%   $\mapsto$   & Mapeia para \\
%   $\sqsubseteq$   & Subclasse (em ontologias) \\
%   $\subseteq$   & Subconjunto: $\forall x\;.\; x \in A \rightarrow x \in B$ \\
%   $\langle\ldots\rangle$ & Tupla \\
%   $\forall$   & Quantificação universal \\
%   mmmmm & Nenhum sentido, apenas estou aqui para demonstrar a largura máxima dessas colunas. Ao abrir o ambiente \texttt{listadesimbolos}, pode-se fornecer um argumento opcional indicando a largura da coluna da esquerda (o default é de 5em): \texttt{\textbackslash{}begin\{listadesimbolos\}[2cm] .... \textbackslash{}end\{listadesimbolos\}} \\
% \end{listadesimbolos}

\tableofcontents*

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Corpo do texto                                               %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\textual{}

\chapter{Introduction}\label{chapter:introduction}

As stated by~\textcite{Aho:2006:CPT:1177220}, \textquote{Programming languages are notations to describe computations to people and to machines}.
These notations can take numerous forms.
They range from lower-level languages, such as machine code ready to be executed by a specific machine, to a higher-level language, such as C, Java, Rust, Haskell and Ml.
Lower-level languages like machine code are very verbose in how they describe computations; usually describing directly to the machine when and how to execute each computation through simple notations like: add the values from two data locations, compare two values, jump the next 4 instructions, and so on~\cite{Aho:2006:CPT:1177220}.
Whereas, in a higher-level language, we can describe computations in a more abstract set of notations, such as functions and types, without needing to expose details from the specific machine that will execute them~\cite{Aho:2006:CPT:1177220}.
Hence, using higher-level programming languages eases how people can describe computations to machines and to one another~\cite{Aho:2006:CPT:1177220}.
However, in order to translate a higher-lever programming language to machine code capable of running on a machine's processor, we need to design and build programs called compilers~\cite{Aho:2006:CPT:1177220}.

A compiler is a program that receives as input a program written in a \textit{source} language and translates it to a semantically equivalent program written in a \textit{target} language
~\cite{Aho:2006:CPT:1177220}.
This is what enables us to write programs in higher-level languages that are able to execute at a machine's processor.
A program written with a higher-level language such as C is fed into a compiler like Clang, then Clang translates the provided source program to a program in a target language, like Intel's x86 processor's machine code, ready to be executed.
During the translation process between the \textit{source} and \textit{target} languages, the compiler goes through two major execution steps: the analysis step, and the synthesis step~\cite{Aho:2006:CPT:1177220}.
The analysis step, called the compiler's \textit{front end}, organizes the information included in the source program into a grammatical structure, and then uses this grammatical structure, together with some metadata collected during its construction, to build what is called an intermediate representation~\cite{Aho:2006:CPT:1177220}.
Furthermore, the synthesis step, called the compiler's \textit{back end}, uses this intermediate representation to compute the desired target program~\cite{Aho:2006:CPT:1177220}.

Though it is possible to build a compiler that translates directly to a target machine code, this hinders portability and modularity~\cite{appel2003modern}.
Suppose we wish to implement a compiler for the source language $i$ to the target machine language $j$, we can implement just the compiler's \textit{front end} for $i$ and use a proven working \textit{back end} for $j$~\cite{Aho:2006:CPT:1177220}.
Therefore, if we wish to implement compilers for $n$ different programming languages to $m$ different machine languages, we can avoid building $n \times m$ compilers building $n$ \textit{front ends} and $m$ \textit{back ends}~\cite{Aho:2006:CPT:1177220}.

If we give the analysis-synthesis model of a compiler a more fine-grained look, we can identify that the compiler's front and back end operate as a series of phases, each one transforming one intermediate representation to another in order to further advance the computation of the target program~\cite{Aho:2006:CPT:1177220}.
The analysis step, or front end, may be subdivided into: a lexical analyzer, a syntax analyzer, a semantic analyzer and an intermediate code generator; also, the synthesis step may be subdivided into: machine-independent code optimization, code generation, and machine-dependent code optimization~\cite{Aho:2006:CPT:1177220}.
The different phases and the intermediate representations between them can be seen at Figure~\ref{figure:compilation-phases}.

\begin{figure}[!ht]
  \centering
  \begin{tikzpicture}[
    >={Latex[width=2mm, length=2mm]},
    ir/.style = {
        fill=white
      },
    phase/.style = {
        rectangle,
        rounded corners,
        minimum width=7.2cm,
        minimum height=1cm,
        text centered,
        draw=black,
      },
    ]
    \node (start) [ir] {source program};
    \node (lexical) [phase, below=\myflowchartlowermargin/2 of start] {Lexical Analyzer};
    \node (syntax) [phase, below=\myflowchartlowermargin of lexical] {Syntax Analyzer};
    \node (semantic) [phase, below=\myflowchartlowermargin of syntax] {Semantic Analyzer};
    \node (intermediate) [phase, below=\myflowchartlowermargin of semantic] {Intermediate Code Generation};
    \node (opt ind) [phase, below=\myflowchartlowermargin of intermediate] {Machine-Independent Code Optimizer};
    \node (codegen) [phase, below=\myflowchartlowermargin of opt ind] {Code Generator};
    \node (opt dep) [phase, below=\myflowchartlowermargin of codegen] {Machine-Dependent Code Optimizer};
    \node (end) [below=\myflowchartlowermargin of opt dep] {};

    \node (frontend) [right=2cm of syntax] {front end};
    \node (backend) [right=2cm of codegen] {back end};

    \draw [->] (start) -- (lexical);
    \draw [->] (lexical) -- node [ir] {token stream} (syntax);
    \draw [->] (syntax) -- node [ir] {syntax tree} (semantic);
    \draw [->] (semantic) -- node [ir] (syntax tree) {syntax tree} (intermediate);
    \draw [->] (intermediate) -- node [ir] {intermediate representation} (opt ind);
    \draw [->] (opt ind) -- node [ir] {intermediate representation} (codegen);
    \draw [->] (codegen) -- node [ir] {target-machine code} (opt dep);
    \draw [->] (opt dep) -- node [ir] {target-machine code} (end);
    \draw (lexical.east) -| (frontend.north);
    \draw (intermediate) -| (frontend);
    \draw (opt ind.east) -| (backend.north);
    \draw (opt dep.east) -| (backend);
  \end{tikzpicture}
  \caption{Phases of a compiler and the intermediate representations between them. Adapted from~\textcite{Aho:2006:CPT:1177220}.}\label{figure:compilation-phases}
\end{figure}

According to \textcite{appel2003modern}, \textquote{An intermediate representation (IR) is a kind of abstract machine language that can express the target-machine operations without committing to too much machine-specific detail}.
The authors continue by adding that the IR \textquote{is also independent of the details of the source language}~\cite{appel2003modern}.
This means that the abstract notations exclusive to a higher-level language are handled by the front end of a compiler so that, by the time the source program is transformed into the IR, the computations described by the IR are semantic equivalent to the computations described by the source program but are now in the notations of an abstract machine language.
Although the semantics of the computations are the same, there may be semantics in the higher-level language that are not present in the IR.\@
The front end of a compiler is then responsible to check if all semantic aspects of the source program are sound to the source language specifications before the generation of the IR~\cite{Aho:2006:CPT:1177220}.
The authors explain that the checks made during compilation are called \textit{Static Checks}, and they are not only capable of assuring that the source program can be successfully compiled, but have the potential to catch programming errors early, before the program can be executed~\cite{Aho:2006:CPT:1177220}. One of the static checks executed during compilation is \textit{type checking} and is part of the semantic analysis phase of the compiler's front end~\cite{appel2003modern}.

The type checking executed during the semantic analysis phase is designed in accordance with the source language's type system.
\textcite{pierce2002types} defines a type system as: \textquote{A tractable syntactic method for proving the absence of certain program behaviors by classifying phrases according to the kinds of values they compute}.
Hence, a phrase written with higher-level notations such as
\begin{equation}\label{eq:type_example1}
  \codett{x * 30}
\end{equation}
may be classified to compute a value of type \codett{Int}, written
\begin{equation*}
  \typer{x * 30}{Int}
\end{equation*}
meaning that~\eqref{eq:type_example1} is a phrase that computes a mathematical integer value.
As a counter example, if defined by the type system that the multiplication between a value of type \codett{Bool} and a value of type \codett{Int} was not allowed, and we had both classifications
\begin{equation*}
  \typer{x}{Bool}
  \qquad
  \text{and}
  \qquad
  \typer{30}{Int}
\end{equation*}
the phrase in~\eqref{eq:type_example1} would be semantically unsound and should be indicated as a type error by the type checker.

According to \textcite{jhala2020tutorial}, type systems are mainly used to describe valid sets of values that can be used for different computations so that the compiler can eliminate a variety of possible run-time errors before the target program execution.
Type systems like the ones used by popular modern programming languages, such as C\#, Haskell, Java, OCaml, Rust and Scala, have similar kinds of rules and are the most widespread tool used to guarantee the correct behavior of a program~\cite{jhala2020tutorial}.
\textcite{jhala2020tutorial} affirm that, although type systems are widespread and effective, well-typed programs do go wrong.
The authors elaborate a few wrong behaviors that are common to the most popular type systems. Within them, we have:
\begin{itemize}
  \item \textbf{Division by zero:} Constraining the types of the division operation to \codett{Int} does not protect the program to execute a division by zero at run-time, and it does not guarantee that the arithmetic operations will not under- or over-flow~\cite{jhala2020tutorial}.
  \item \textbf{Buffer overflow:} Constraining the index of the access of an \codett{Array} or \codett{String} to \codett{Int} does not protect the program to try to access data from beyond the data structure's end~\cite{jhala2020tutorial}.
\end{itemize}

An effort can be made while designing a type system so that they can further restrict the values of certain types.
We can extend a type system to further \textit{refine} its types with logic predicates and this method is called \textit{Refinement types with predicates}~\cite{jhala2020tutorial}.
It allows programmers to constrain existing types by using predicates to assert desired properties of the values they want to describe~\cite{jhala2020tutorial}.
For example, while \codett{Int} types can assume any integer values, we can write the refined type
\begin{equation*}
  \codett{type Nat = \{v:Int | 0 <= v\}}
\end{equation*}
where the newly defined type \codett{Nat} will only be able to assume positive integer values.
Alone, this refinements may seam a just a gimmick, but combined with functions the programmer can describe precise contracts that describe the functions legal inputs and outputs~\cite{jhala2020tutorial}.
For example, the author of an \codett{array} library may specify the functions signatures types
\begin{equation*}
  \begin{aligned}
     & \codett{fn size: x:array(a) -> \{v:Nat | v = length(x)\}}      \\
     & \codett{fn  get: x:array(a) -> \{v:Nat | v < length(x)\} -> a} \\
  \end{aligned}
\end{equation*}
where \codett{size} and \codett{get} are functions and \codett{x} is the name of the first argument.
In this type system, a call to \code{size(arr)} returns a value $s$ of type \codett{Nat} constrained to a single value equal to the length of \codett{arr}; hence, the type of $s$ constrains $s$ to the exact length of \codett{arr}.
Furthermore, a call to \code{get(arr, i)} requires the index \codett{i} to be within the bounds of \codett{arr}.
Given these definitions, the refinement type checker can then prove, during the analysis phase (i.e.,\ at compile-time), that the contracts of both \codett{size} and \codett{get} will not be violated, ensuring all array access to be sated when executing the target program (i.e.,\ at run-time)~\cite{jhala2020tutorial}.

\section{Motivation and research problem}

There is an increasing number of uses of refinement types being implemented on top of existing languages.
For example: the work of~\textcite{vazou2014liquidhaskell} presenting LiquidHaskell as refinement types for the Haskell language; the work of~\textcite{vekris2016refinementtypescript} integrating refinement types for the TypeScript language; the work of~\textcite{sammler2021refinedc} integrating refinement types for the C language; and, the work of~\textcite{vazou2018refinementruby} integrating refinement types for the Ruby language.

Although refinement types have been proved useful in improving the static checking capabilities of higher level languages, there is a lack of research on bringing refinement types to the intermediate code generation phase of a compiler's front end.
Hence, we state the research problem in the form of the question: What can we discover when we add refinement types to the intermediate code generation phase of a compiler's front end?

\section{Goals}\label{chapter:introduction:sec:goals}

The primary goal of this work is to discuss and validate the following thesis: A front-end for a higher-level language with refinement types can make use of refinement types to allow optimizations opportunities during intermediate code generation not present in higher-level languages without refinement types.

For allowing this discussion, we will be designing a language with refinement types called \textit{Ekitai} and implementing its front end.
Designing a new language is particularly advantageous because we have full control of all the language features being implemented and allow us to incrementally design the language, the refinement type system, and the intermediate code generation, one feature at a time.

\subsection{Specific Goals}\label{chapter:introduction:sec:goals:specific_goals}
The specific research artifacts constructed by this work that allow the discussion of the thesis are:
\begin{itemize}
  \item The specification of \textit{Ekitai's} lexical elements;
  \item The specification of \textit{Ekitai's} syntax;
  \item The specification of \textit{Ekitai's} type system with refinement types;
  \item The implementation of \textit{Ekitai's} front end including:
        \begin{itemize}
          \item A lexical analyzer;
          \item A syntactic analyzer;
          \item A semantic analyzer with a type checker;
          \item An intermediate code generator;
        \end{itemize}
  \item The implementation of optimizations during intermediate code generation.
\end{itemize}

\section{Methodology}

The aim of this work is to find optimizations opportunities during intermediate code generation of a higher-level language with refinement types.
In order to achieve the specific goals specified in Section~\ref{chapter:introduction:sec:goals:specific_goals} we will employ different methods during research.

In the specification of the \textit{Ekitai} programming language aspects we will employ techniques from the established literature, such as books and articles, on language design and compiler construction.
Also, in order to specify the type system with refinement types we analyze the recent publications about refinement types in the ACM Sigplan's conferences and journals.
Whereas in the implementation of \textit{Ekitai's} front end we will develop a front end using the Rust programming language employing techniques from the established literature, such as books and articles, and from open-source implementations of modern industry programming language compilers and tools.

\section{Structure of the work}

In the next chapter, Chapter~\ref{chapter:background}, we will introduce the background knowledge needed to design a higher level language's front end. Furthermore, in Chapter~\ref{chapter:related_work}, we explore other pieces of work from articles and industry that are tangent to our goal. Then, in Chapter~\ref{chapter:proposal}, we describe the proposed \textit{Ekitai} language.

\chapter{A review on front end design and implementation}\label{chapter:background}

In this chapter we present a brief review of the background needed to build a compiler's front end.
We begin by presenting the formal definition of a language in Section~\ref{ch:background:sec:strings_and_languages}.
Then, give a brief overview of what is a lexical analyzer and how it is constructed in Section~\ref{chapter:background:sec:lexical}.
We continue by presenting the aspects of a syntax analyzer in Section~\ref{chapter:background:sec:syntax}.
Furthermore, we go on to explore the semantic analyzer and the tools used to formalize the type system in Section~\ref{chapter:background:sec:semantic}.
And then, explore the aspects of intermediate code generation in Section~\ref{chapter:background:sec:intermediate}.

\section{Strings and Languages}\label{ch:background:sec:strings_and_languages}

As stated by \textcite{sipser2012introduction}, \textquote{strings of characters are fundamental building blocks in computer science}.
In order to define what a string of characters is, \textcite{sipser2012introduction} defines an \textit{alphabet} to be any nonempty finite set composed by its \textit{symbols}.
The author elaborates that a \textit{string over an alphabet} is a finite sequence of symbols from that alphabet~\cite{sipser2012introduction}.
For example, if we build the English alphabet as the set of symbols
\begin{equation*}
  \Sigma = \{\code{a}, \code{b}, \code{c}, \code{d}, \dots, \code{x}, \code{y}, \code{z}\}
\end{equation*}
then, the word \code{compiler} would be a string over $\Sigma$.

Formally, a string $s$ over an alphabet $\Sigma$ is a string $s$ such that $s \in \Sigma^*$, called the Kleen closure of $\Sigma$~\cite{HopcroftMotwaniUllman07}.
In order to perform the inductive construction of $\Sigma^*$ we need to first define the empty string, written $\epsilon$, then, define the concatenation operation $uv = s$, where
\begin{gather*}
  u, v, s, w \in \Sigma^* \\
  u = u_1 u_2 u_3 \dots u_i \quad \text{where} \quad u_1 u_2 u_3 \dots u_i \in \Sigma \quad \text{and} \quad i \geq 0\\
  v = v_1 v_2 v_3 \dots v_i \quad \text{where} \quad v_1 v_2 v_3 \dots v_i \in \Sigma \quad \text{and} \quad j \geq 0\\
  s = u_1 u_2 u_3 \dots u_i v_1 v_2 v_3 \dots v_i \\
  u\epsilon = u \quad \epsilon u = u\\
  (uv)w = u(vw)
\end{gather*}
Next, we perform the inductive construction
\begin{align*}
  \Sigma^0     & = \{ \epsilon \}                                           \\
  \Sigma^1     & = \Sigma                                                   \\
  \Sigma^{i+1} & = \{ uv \mid u \in \Sigma^{i}\ \text{and}\ v \in \Sigma \} \\
  \Sigma^*     & = \bigcup_{i \geq 0} \Sigma^i
\end{align*}
This construction shows that $\Sigma^*$ is composed of all strings of finite size for all permutations of the alphabet~\cite{HopcroftMotwaniUllman07}.

After the definition of the set of all strings over an alphabet $\Sigma*$, we can broadly define a language $L$ as a proper subset of $\Sigma^*$, written $L \subseteq \Sigma^*$.
Although having a simple definition, proving that a string $s$ belongs to a language $L$ turns out to be rather challenging.
Furthermore, we end up using different languages with different symbols for lexical, syntax and semantic analysis in order to verify the source program.
Although, for example, the lexer may take as the alphabet symbols the characters present in the encoding of its input (e.g., UTF-8 and ASCII), the parser may have as its alphabet symbols the token names outputted by the lexer.
In the next sections~\ref{chapter:background:sec:lexical},~\ref{chapter:background:sec:syntax}, and~\ref{chapter:background:sec:semantic}, we explore the different formal tools the different analysis steps use to prove that a string of their respective symbols is a valid source program and how they produce an output for the next step in compilation.

\section{Lexical Analysis}\label{chapter:background:sec:lexical}

The first phase of a compiler is called lexical analysis.
The lexical analyzer will read a stream of characters from the source program and group the characters into sequences called \textit{lexemes} according to the \textit{patterns} defined for each \textit{token name}~\cite{Aho:2006:CPT:1177220}.
\textcite{Aho:2006:CPT:1177220} describes that, for each lexeme grouped by reading the input character stream, the lexical analyzer will generate a \textit{token} which constitutes a pair of the token name and some optional metadata (e.g., the start and end position of the respective lexeme in the input character stream) written
\begin{equation*}
  \langle \textit{token name},\ \textit{metadata}\rangle
\end{equation*}
For example, upon analyzing the following program:
\begin{equation*}
  \codett{let a = b + 60 / c}
\end{equation*}
A lexical analyzer may output the following token stream:
\begin{equation}\label{figure:introduction_token_stream}
  \begin{aligned}
    \tokenstream{\codett{let}}{\codett{let}}{$0$}{$3$}      \\
    \tokenstream{\codett{identifier}}{\codett{a}}{$4$}{$5$} \\
    \tokenstream{\codett{=}}{\codett{=}}{$6$}{$7$}          \\
    \tokenstream{\codett{identifier}}{\codett{b}}{$8$}{$9$} \\
    \tokenstream{\codett{+}}{\codett{+}}{$10$}{$11$}        \\
    \tokenstream{\codett{number}}{\codett{60}}{$12$}{$14$}  \\
    \tokenstream{\codett{/}}{\codett{/}}{$15$}{$16$}        \\
    \tokenstream{\codett{identifier}}{\codett{c}}{$17$}{$18$}
  \end{aligned}
\end{equation}
In this case, the metadata collected by the analyzer is very pedantic including even redundant data as the lexemes for simple patterns of the token names \codett{=}, \codett{+} and \codett{/}.
When no metadata is needed it can be omitted from the token notation (e.g., \token{\{}).

In order to classify a lexeme to be of a given token name we employ the use of \textit{patterns}, and one of the important notations for the description of token patterns are regular expressions~\cite{Aho:2006:CPT:1177220}.
Regular expressions are very effective in specifying the type of patterns usually needed to classify lexemes into tokens and can be used for automatic generation of a lexical analyzer~\cite{Aho:2006:CPT:1177220}.
A simple description of the patterns for the tokens used in the example above can be given by regular expressions with rules of the form $\textit{token name} \rightarrow \textit{regular expression}$ as follows:
\begin{equation*}
  \begin{aligned}
    \codett{identifier} & \rightarrow \ {[A-Za-z]}^+ \\
    \codett{number}     & \rightarrow \ {[0-9]}^+    \\
    \codett{let}        & \rightarrow \ let          \\
    \codett{=}          & \rightarrow \ =            \\
    \codett{/}          & \rightarrow \ /            \\
    \codett{+}          & \rightarrow \ +            \\
  \end{aligned}
\end{equation*}
This rules can then, either be used by lexical analyzer generators to automatically generate a program for lexical analysis, or, be used as the formal specification for handwritten lexers to be based upon~\cite{Aho:2006:CPT:1177220}.

\section{Syntax Analysis}\label{chapter:background:sec:syntax}

The second phase of a compiler is called syntax analysis, or \textit{parsing}.
The \textit{parser} receives as input a token stream produced by the lexical analyzer and creates a tree-like intermediate representation that is constrained by a particular grammatical structure~\cite{Aho:2006:CPT:1177220}.

If we give the token stream~\eqref{figure:introduction_token_stream} as input to a parser it may produce the \textit{parse tree} structure found on Figure~\ref{figure:introduction_ast}.
This structure has more information than the linear token stream it received as input.
For example, it is prepared in such a way to preserve the order of operations from classic arithmetic.
Consequently, the tree is composed of two interior nodes labeled $\bnfvar{Expr}$ for binary operations: the bottom one, denoting the sub-expression
\begin{equation*}
  \bnfvar{Expr}_{bottom} = \code{60 / c}
\end{equation*}
and the top one, denoting the whole expression
\begin{equation*}
  \bnfvar{Expr}_{top} = \code{a + $\bnfvar{Expr}_{bottom}$} = \code{a + 60 / c}
\end{equation*}
This structure makes explicit that we must first evaluate the result of $\bnfvar{Expr}_{bottom}$, dividing \codett{60} by \codett{c}, before we can evaluate the result of $\bnfvar{Expr}_{top}$, adding \codett{a} to the result of $\bnfvar{Expr}_{bottom}$.

\begin{figure}[ht]
  \centering
  \begin{tikzpicture}
    \node (is-root) {\bnfvar{Let}} [sibling distance=2cm]
    child { node {\bnfter{let}} }
    child {
        node {\bnfvar{Pattern}}
        child {
            node[align=center] {\bnfter{id}}
            child {
                node {a}
                edge from parent[dashed]
              }
          }
        child[missing] {}
      }
    child { node {\bnfter{=}} }
    child {
        node {\bnfvar{Expr}}
          [sibling distance=1.2cm]
        child {
            node {\bnfter{id}}
            child {
                node {b}
                edge from parent[dashed]
              }
          }
        child[missing]{}
        child {
            node {\bnfvar{BinOp}}
            child{ node {\bnfter{+}} }
            child[missing]{}
          }
        child[missing]{}
        child {
            node {\bnfvar{Expr}}
              [sibling distance=2cm]
            child[missing]{}
            child {
                node {\bnfter{number}}
                child {
                    node {60}
                    edge from parent[dashed]
                  }
              }
            child {
                node {\bnfvar{BinOp}}
                child {
                    node {\bnfter{/}}
                  }
              }
            child {
                node {\bnfter{id}}
                child {
                    node {c}
                    edge from parent[dashed]
                  }
              }
          }
      };
  \end{tikzpicture}
  \caption{
    A possible parse tree output for the token stream~\eqref{figure:introduction_token_stream}.
    Leafs are shown as the token names from the token stream.
    Here lexemes for non-trivial tokens are appended with dashed lines underneath token names for example purposes.
  }\label{figure:introduction_ast}
\end{figure}

\subsection{Precise definition of context-free grammars and ambiguity}
Every programming language has precise rules that prescribe the correct syntactic structure of its programs~\cite{Aho:2006:CPT:1177220}.
In order to formally describe the rules of such syntactic structure, we can use a context-free grammar~\cite{Aho:2006:CPT:1177220}.
A context-free grammar is a finite set of constructs that allows us to build the set of all sentences of a given context-free Language~\cite{sipser2012introduction}.
They have four components:
\begin{itemize}
  \item A finite set $\Sigma$ of \textit{terminal} symbols, usually called tokens, composed by the set of token names defined in the lexical aspects of the language~\cite{Aho:2006:CPT:1177220};
  \item A finite set $V$ of \textit{nonterminals}, or \textit{variables}, disjoint from the set of terminals, usually called \textit{syntactic variables}~\cite{Aho:2006:CPT:1177220};
  \item A finite set $R$ of \textit{productions}, or \textit{rules}, where each production is of the form
        \begin{equation*}
          \begin{alignedat}{2}
            \bnfprod{Variable}{s}
          \end{alignedat}
        \end{equation*}
        where \bnfvar{V} is the variable at the production's \textit{head}, and $s \in {(\Sigma \cup V)}^*$ is the string at the production's \textit{body}~\cite{sipser2012introduction};
  \item And, a designation of a nonterminal $S$ as the \textit{start} of the grammar~\cite{Aho:2006:CPT:1177220}.
\end{itemize}

A simple expression language could have its syntactical structure formalized by the following grammar
\begin{equation}~\label{eq:ambiguous_expr_grammar}
  \begin{alignedat}{2}
    \bnfprod{Expr}{\bnfvar{Expr} \bnfvar{BinOp} \bnfvar{Expr}} \\
    \bnfmore{\bnfter{(} \bnfvar{Expr} \bnfter{)}}               \\
    \bnfmore{\bnfter{number}}                                     \\
    \bnfmore{\bnfter{id}}                                         \\
    \bnfprod{BinOp}{\bnfter{+} \bnfor{\bnfter{-}} \bnfor{\bnfter{*}} \bnfor{\bnfter{/}}}
  \end{alignedat}
\end{equation}
where: terminals have the token names between quotes (e.g., $\bnfter{id}$);
nonterminals have the syntactic variables as normal words (e.g., $\bnfvar{Expr}$);
and, the designation of the starting variable is done by the variable at the head of the first production (e.g., in the example above the starting nonterminal is $\bnfvar{Expr}$).
Furthermore, multiple productions can be abbreviated by using the \textit{or} operator denoted by `$\mid$' turning
\begin{alignat}{2}
  \bnfprod{V}{$s^1$} \nonumber \\
  \bnfprod{V}{$s^2$} \nonumber
\end{alignat}
into
\begin{alignat}{2}
  \bnfprod{V}{$s^1$ \bnfor{$s^2$}} \nonumber
\end{alignat}

In order to show that a particular sentence is in the language formalized by a context-free grammar, we can perform a derivation~\cite{sipser2012introduction}.
A derivation starts with, first, writing the start variable~\cite{sipser2012introduction}.
Then, second, we choose any variable from the sentence written so far and a production with the same variable as head replacing the chosen variable with the body of the chosen production~\cite{sipser2012introduction}.
Then, we repeat the second step until there is no more variables and the sentence is formed only by terminals~\cite{sipser2012introduction}.

Formally, \textcite{sipser2012introduction} defines that, given sentences $u, v \in {(\Sigma \cup V)}^*$, $u$ \textit{derives} $v$, written $u \Rightarrow^* v$, if $u = v$ or if there exists sentences $s_1, s_2, s_3, \dots, s_k \in {(\Sigma \cup V)}^*$ for $k \geq 0$ where
\begin{equation*}
  u \Rightarrow s_1 \Rightarrow  s_2, \Rightarrow s_3, \Rightarrow \dots \Rightarrow s_k \Rightarrow v
\end{equation*}
we can define that the language $L$ of the grammar is the set $L = \{s \in \Sigma^* \mid S \Rightarrow^* s\}$, meaning $L$ is the set of all sentences $s$ composed only by terminal symbols that can be derived from the starting variable $S$.

Take, for example, the sentence $s = \code{id + number * id}$ and the language $L$ specified by grammar~\eqref{eq:ambiguous_expr_grammar}. We can proof that $s \in L$ by showing that $\bnfvar{Expr} \Rightarrow^* s$, so we start by writing the start variable
\begin{equation*}
  \bnfvar{Expr}
\end{equation*}
then choose a production with \bnfvar{Expr} as head and substitute \bnfvar{Exrp} with the production's body. We know there are two binary operations, so we choose the production which its body is \code{\bnfvar{Expr} \bnfvar{BinOP} \bnfvar{Expr}} producing the derivation
\begin{equation*}
  \bnfvar{Expr} \Rightarrow \code{\bnfvar{Expr} \bnfvar{BinOp} \bnfvar{Expr}}
\end{equation*}
For the next derivation step, we are open to choose what variable to derivate first.
Although we could choose any variable from the sentence during derivation, we will stick to leftmost derivations, which mandates that we choose the leftmost variable, highlighted in \textbf{bold}, of the sentence to perform the derivation.
So, we could have the leftmost derivation
\begin{equation}\label{eq:ambiouous_derivation1}
  \begin{aligned}
    \code{\bnfvar{\textbf{Expr}}}
     & \Rightarrow
    \code{\bnfvar{\textbf{Expr}} \bnfvar{BinOp} \bnfvar{Expr}}                              \\
     & \Rightarrow
    \code{\bnfvar{\textbf{Expr}} \bnfvar{BinOp} \bnfvar{Expr} \bnfvar{BinOp} \bnfvar{Expr}} \\
     & \Rightarrow
    \code{\bnfter{id} \bnfvar{\textbf{BinOp}} \bnfvar{Expr} \bnfvar{BinOp} \bnfvar{Expr}}   \\
     & \Rightarrow
    \code{\bnfter{id} \bnfter{+} \bnfvar{\textbf{Expr}} \bnfvar{BinOp} \bnfvar{Expr}}       \\
     & \Rightarrow
    \code{\bnfter{id} \bnfter{+} \bnfter{number} \bnfvar{\textbf{BinOp}} \bnfvar{Expr}}     \\
     & \Rightarrow
    \code{\bnfter{id} \bnfter{+} \bnfter{number} \bnfter{*} \bnfvar{\textbf{Expr}}}         \\
     & \Rightarrow
    \code{\bnfter{id} \bnfter{+} \bnfter{number} \bnfter{*} \bnfter{id}}
  \end{aligned}
\end{equation}
thus, proving that the $s \in L$
but, we could also have a second leftmost derivation
\begin{equation}\label{eq:ambiouous_derivation2}
  \begin{aligned}
    \code{\bnfvar{\textbf{Expr}}}
     & \Rightarrow
    \code{\bnfvar{\textbf{Expr}} \bnfvar{BinOp} \bnfvar{Expr}}                          \\
     & \Rightarrow
    \code{\bnfter{id} \bnfvar{\textbf{BinOp}} \bnfvar{Expr}}                            \\
     & \Rightarrow
    \code{\bnfter{id} \bnfter{+} \bnfvar{\textbf{Expr}}}                                \\
     & \Rightarrow
    \code{\bnfter{id} \bnfter{+} \bnfvar{\textbf{Expr}} \bnfvar{BinOp} \bnfvar{Expr}}   \\
     & \Rightarrow
    \code{\bnfter{id} \bnfter{+} \bnfter{number} \bnfvar{\textbf{BinOp}} \bnfvar{Expr}} \\
     & \Rightarrow
    \code{\bnfter{id} \bnfter{+} \bnfter{number} \bnfter{*} \bnfvar{\textbf{Expr}}}     \\
     & \Rightarrow
    \code{\bnfter{id} \bnfter{+} \bnfter{number} \bnfter{*} \bnfter{id}}
  \end{aligned}
\end{equation}
thus, having two leftmost derivations that shows the sentence belongs to the grammar's described language.

A grammar is said to be \textit{ambiguous} if it has a non-unique leftmost or rightmost derivation for any sentence of the language it describes~\cite{Aho:2006:CPT:1177220}.
Therefore, grammar~\eqref{eq:ambiguous_expr_grammar} is clearly ambiguous given derivations~\eqref{eq:ambiouous_derivation1} and~\eqref{eq:ambiouous_derivation2}.
From the parse trees in Figure~\ref{fig:ambiguous_parse_tree} we can visualize that the order of operations differ between the derivations of \code{id + number * id}.
In Figure~\ref{fig:ambiguous_parse_tree_a} we have \code{(id + number) * id} and in Figure~\ref{fig:ambiguous_parse_tree_b} we have \code{id + (number * id)} which have different mathematical meanings.

\begin{figure}[ht]
  \centering
  \begin{subfigure}[b]{0.4\textwidth}
    \centering
    \begin{tikzpicture}
      \node {\bnfvar{Expr}} [sibling distance=1.7cm]
      child {
          node {\bnfvar{Expr}}
          child {
              node {\bnfter{id}}
            }
          child {
              node {\bnfvar{BinOp}}
              child{ node {\bnfter{+}} }
            }
          child {
              node {\bnfter{number}}
            }
        }
      child {
          node {\bnfvar{BinOp}}
          child[missing]{}
          child[missing]{}
          child {
              node {\bnfter{*}}
            }
        }
      child {
          node {\bnfter{id}}
        };
    \end{tikzpicture}
    \caption{Parse tree from derivation~\eqref{eq:ambiouous_derivation1}.}\label{fig:ambiguous_parse_tree_a}
  \end{subfigure}
  \qquad
  \qquad
  \begin{subfigure}[b]{0.4\textwidth}
    \centering
    \begin{tikzpicture}
      \node (is-root) {\bnfvar{Expr}} [sibling distance=1.7cm]
      child { node {\bnfter{id}} }
      child {
          node {\bnfvar{BinOp}}
          child {
              node {\bnfter{+}}
            }
          child[missing]{}
          child[missing]{}
        }
      child {
          node {\bnfvar{Expr}}
          child {
              node {\bnfter{number}}
            }
          child {
              node {\bnfvar{BinOp}}
              child{ node {\bnfter{*}} }
            }
          child {
              node {\bnfter{id}}
            }
        };
    \end{tikzpicture}
    \caption{Parse tree from derivation~\eqref{eq:ambiouous_derivation2}.}\label{fig:ambiguous_parse_tree_b}
  \end{subfigure}
  \caption{
    Visual representation of derivations~\eqref{eq:ambiouous_derivation1} and~\eqref{eq:ambiouous_derivation2} in its parse tree form.
  }\label{fig:ambiguous_parse_tree}
\end{figure}

Compilers use parse trees to derive meaning and, therefore, ambiguous grammars are problematic for compiling~\cite{appel2003modern}.
If we were to use some parsing generator algorithm, such as $LL$, $LR$ and their variants, an effort should be made to transform such ambiguous grammars into unambiguous grammars~\cite{appel2003modern}.
As an example, we can present an unambiguous grammar relative to the grammar~\eqref{eq:ambiguous_expr_grammar} as
\begin{equation}~\label{eq:unambiguous_expr_grammar}
  \begin{alignedat}{2}
    \bnfprod{Expr}{\bnfvar{Expr} \bnfter{+} \bnfvar{Term}} \\
    \bnfmore{\bnfvar{Expr} \bnfter{-} \bnfvar{Term}} \\
    \bnfmore{\bnfvar{Term}} \\
    \bnfprod{Term}{\bnfvar{Term} \bnfter{*} \bnfvar{Factor}} \\
    \bnfmore{\bnfvar{Term} \bnfter{/} \bnfvar{Factor}} \\
    \bnfprod{Factor}{\bnfter{(} \bnfvar{Expr} \bnfter{)}} \\
    \bnfmore{\bnfter{number}} \\
    \bnfmore{\bnfter{id}}
  \end{alignedat}
\end{equation}
which, by adding new variables to the grammar, can express unambiguously in its parse trees that: the operators \codett{*} and \codett{/} \textit{binds tighter}, or have a \textit{higher precedence}, then \codett{+} and \codett{-}; and, the operators of the same \textit{binding power}, or \textit{precedence}, are left associative.

\subsection{Review of top-down and bottom-up parsers}
There are two main categories of parser.
First we have parsers that try to construct the parse tree by finding a leftmost derivation starting from the root and creating the nodes of the parse three in preorder, called \textit{top-down} parsers~\cite{Aho:2006:CPT:1177220}.
And second we have parsers that try to construct the parse tree beginning at the leaves and working up to the root by performing a rightmost derivation in reverse applying a series of reductions on the input stream~\cite{Aho:2006:CPT:1177220}.

Top-down parsers, also called recursive descent parsers, have the advantage of its algorithms being simple enough to be used to construct parsers by hand~\cite{appel2003modern}.
Although simple, classic implementations of recursive descent parsers, such as the $LL(1)$ and $LL(k)$ predictive parsers, cannot deal with ambiguous grammars and, since they rely on leftmost derivations, cannot deal with left recursions on grammar productions~\cite{Aho:2006:CPT:1177220}.
There may also be the need to run a left factoring algorithm in order to generate a correct predictive parsing table for $LL(1)$ and $LL(k)$ automatic parser generators~\cite{Aho:2006:CPT:1177220}.

A non left recursive, left factored grammar for grammar~\eqref{eq:unambiguous_expr_grammar} can be built as show in Figure~\ref{fig:fat_expr_grammar}.
\begin{figure}[ht]
  \centering
  \begin{alignat}{2}
    \bnfprod{Expr}{\bnfvar{Term} \bnfvar{Expr2}} \label{eq:fat_expr_grammar_expr}                \\
    \bnfprod{Expr2}{\bnfter{+} \bnfvar{Term} \bnfvar{Expr2}} \label{eq:fat_expr_grammar_expr2}   \\
    \bnfmore{\bnfter{-} \bnfvar{Term} \bnfvar{Expr2}} \label{eq:fat_expr_grammar_expr3}          \\
    \bnfmore{$\epsilon$} \label{eq:fat_expr_grammar_expr4}                                       \\
    \bnfprod{Term}{\bnfvar{Factor} \bnfvar{Term2}} \label{eq:fat_expr_grammar_term}              \\
    \bnfprod{Term2}{\bnfter{*} \bnfvar{Factor} \bnfvar{Term2}} \label{eq:fat_expr_grammar_term2} \\
    \bnfmore{\bnfter{/} \bnfvar{Factor} \bnfvar{Term2}} \label{eq:fat_expr_grammar_term3}        \\
    \bnfmore{$\epsilon$} \label{eq:fat_expr_grammar_term4}                                       \\
    \bnfprod{Factor}{\bnfter{(} \bnfvar{Expr} \bnfter{)}} \label{eq:fat_expr_grammar_fact}       \\
    \bnfmore{\bnfter{number}} \label{eq:fat_expr_grammar_fac2}                                   \\
    \bnfmore{\bnfter{id}} \label{eq:fat_expr_grammar_fac3}
  \end{alignat}
  \caption{
    A context-free grammar after left factoring and removing left recursions from grammar~\eqref{eq:unambiguous_expr_grammar}.
  }\label{fig:fat_expr_grammar}
\end{figure}
This grammar complies with every constraint imposed by a classic implementation of a predictive recursive descent parser since it is unambiguous, free of left recursions and left factored.
An example implementation of predictive parsers for the variables \bnfvar{Expr}, \bnfvar{Expr2}, \bnfvar{Term}, \bnfvar{Term2} and \bnfvar{Factor} can be found respectively at Figures~\ref{fig:predictive_parser_expr},~\ref{fig:predictive_parser_expr2},~\ref{fig:predictive_parser_term},~\ref{fig:predictive_parser_term2} and~\ref{fig:predictive_parser_fact}.

\begin{figure}[ht]
  \centering
  \begin{subfigure}[b]{0.48\textwidth}
    \begin{minipage}{\textwidth}
      \begin{rustcode}
fn parse_Expr(p: Parser) -> Parse {
	let t = parse_Term(p);
	let e2 = parse_Expr2(p);
	Expr(t, e2)
}
			\end{rustcode}
    \end{minipage}
    \caption{}\label{fig:predictive_parser_expr}
  \end{subfigure}
  \quad
  \begin{subfigure}[b]{0.48\textwidth}
    \begin{minipage}{\textwidth}
      \begin{rustcode}
fn parse_Expr2(p: Parser) -> Parse {
  if p.at("+") | p.at("-") {
    let op = p.eat_token();
    let t = parse_Term(p);
    let e2 = parse_Expr2(p);
    Expr2(op, t, e2)
  } else {
    Empty
  }
}
      \end{rustcode}
    \end{minipage}
    \caption{}\label{fig:predictive_parser_expr2}
  \end{subfigure}
  \caption{
    Predictive parsers for variables \bnfvar{Expr} (a), and \bnfvar{Expr2} (b), from examples~\eqref{eq:fat_expr_grammar_expr} and~\eqref{eq:fat_expr_grammar_expr2}.
  }\label{fig:predictive_parser_expr_all}
\end{figure}

\begin{figure}[ht]
  \centering
  \begin{subfigure}[b]{0.48\textwidth}
    \begin{minipage}{\textwidth}
      \begin{rustcode}
fn parse_Term(p: Parser) -> Parse {
  let f = parse_Factor(p);
  let t2 = parse_Term2(p);
  Term(f, t2)
}
      \end{rustcode}
    \end{minipage}
    \caption{}\label{fig:predictive_parser_term}
  \end{subfigure}
  \quad
  \begin{subfigure}[b]{0.48\textwidth}
    \begin{minipage}{\textwidth}
      \begin{rustcode}
fn parse_Term2(p: Parser) -> Parse {
  if p.at("*") | p.at("/") {
    let op = p.eat_token();
    let f = parse_Factor(p);
    let t2 = parse_Term2(p);
    Term2(op, f, t2)
  } else {
    Empty
  }
}
      \end{rustcode}
    \end{minipage}
    \caption{}\label{fig:predictive_parser_term2}
  \end{subfigure}
  \caption{
    Predictive parsers for variables $\bnfvar{Term}$ (a), and $\bnfvar{Term2}$ (b), from examples~\eqref{eq:fat_expr_grammar_term} and~\eqref{eq:fat_expr_grammar_term2}.
  }\label{fig:predictive_parser_expr_term_all}
\end{figure}

\begin{figure}[ht]
  \centering
  \begin{minipage}{0.5\textwidth}
    \begin{rustcode}
fn parse_Factor(p: Parser) -> Parse {
  if p.at("(") {
    p.eat_token();
    let expr = parse_Expr(p);
    p.expect(")");
    Expr(expr)
  } else if p.at("id") {
    let token = p.eat_token();
    Id(token)
  } else if p.at("number") {
    let token = p.eat_token();
    Number(token)
  } else {
    panic!("Parse error.");
  }
}
    \end{rustcode}
  \end{minipage}
  \caption{
    A predictive parser for variable $\bnfvar{Factor}$ in~\eqref{eq:fat_expr_grammar_fact}.
  }\label{fig:predictive_parser_fact}
\end{figure}

\begin{figure}
  \centering
  \begin{minipage}{0.9\textwidth}
    \rustfile{code/pratt_example.rs}
  \end{minipage}
  \caption{
    A Pratt parser for the ambiguous expression grammar~\eqref{eq:ambiguous_expr_grammar}.
  }\label{fig:pratt_parser_expr_grammar}
\end{figure}

We were once again forced to add new variables to the already unambiguous grammar~\eqref{eq:unambiguous_expr_grammar}.
The transformations added variables without clear meanings, such as \bnfvar{Expr2} and \bnfvar{Term2}, further complicating the parse tree.
In order to enrich the capabilities of recursive descendant parsers, advances were made to allow recursive descendant parsers to parse some ambiguous grammars with left recursion without the need to left factoring.
\textcite{pratt1973operatorprecedence} proposed a \textit{top-down operator precedence} approach to parse arithmetic expressions by assigning a total order to tokens using a function for the left and right binding power of tokens in order to uniquely create parse trees of ambiguous left recursive expression grammars.
An example Pratt parser for grammar~\eqref{eq:ambiguous_expr_grammar} is found at Figure~\ref{fig:pratt_parser_expr_grammar}.
Pratt parsers work by combining recursion with iteration.
If we try to parse the sentence \code{$\codett{id}_1$ + number * $\codett{id}_2$} it:
\begin{itemize}
  \item parses $\codett{id}_1$ and name its node `left';
  \item enters the loop and parse \codett{+} which have a binding power to the left of $1$ and to the right of $2$;
  \item checks if it binds to the left stronger than the minimum binding power, in this case it does not since the minimum starts at $0$, continuing on to recursively parse \bnfter{number} which its node becomes the new `left' in the new recursion step;
  \item the next operator is then \codett{*} with binding power $3$ and $4$, this time the left binding power still is not smaller than the minimum of $2$, and we recursively parse $\codett{id}_2$ which again its node becomes a new `left';
  \item now, upon trying to parse a new operator, we break from the loop and return the node for $\codett{id}_2$ and name it `right'; then, we construct a node \bnfvar{Expr} with `left', which is \codett{number} for this iteration, the operator \codett{*}, and `right', which is $\codett{id}_2$;
  \item we then return again with another \bnfvar{Expr} node with `left', which now is $\codett{id}_1$, the operator \codett{+}, and the last returned expression as `right' finally forming the parse tree with correct precedence found at Figure~\ref{eq:ambiouous_derivation2}.
\end{itemize}
The loop to keep checking if the next operation binds stronger than the minimum for the recursion guaranties both correct associativity and operator precedence~\cite{pratt1973operatorprecedence}.

Besides the work of~\textcite{pratt1973operatorprecedence}, there are recursive descent parsers which can parse a bigger range of ambiguous and left recursive grammars.
\textcite{10.1007/978-3-540-77442-6_12} proposes a handwritten parser combinator approach that uses composable higher-order functions, together with memoization and backtracking, in order to allow parsing both ambiguity and left recursive grammars in polynomial time.
There is also the works of~\textcite{10.1145/583852.581483} that proposes a handwritten parser approach, called \textit{Packrat}, using memoization, backtracking, and unlimited look ahead to parse $LL(k)$ and $LR(k)$ grammars by using \textit{Parsing Expression Grammar} (PEG) definitions, formalized by~\textcite{10.1145/982962.964011}, which differs from context-free grammars by imposing a total order to the rules set of the grammar.
Therefore, PEGs cannot be ambiguous because there is no choice involved in which production to choose for derivation~\cite{10.1145/982962.964011}.
Furthermore, \textcite{10.1145/1328408.1328424} extended Packrat to allow left-recursion by changing its memoization process.

Bottom-up parsers have fewer context-free grammar constraints~\cite{Aho:2006:CPT:1177220}.
They still have troubles with ambiguous grammars due to shift-reduce conflicts but no longer require a non-left recursive and left factored grammar~\cite{Aho:2006:CPT:1177220}.
Writing a bottom-up shift-reduce parser for an $LR$ class grammar is not an easy task, thankfully
parser generators for $LR(1)$ and $LR(k)$ languages are capable of generating very efficient parsers for a large range of context-free grammars~\cite{Aho:2006:CPT:1177220}.
Even allowing grammar designers to solve shifting-reduce conflicts by hand assigning whether to shift or to reduce for every conflict~\cite{Aho:2006:CPT:1177220}.

Although using bottom-up parser generators are desirable for its expressiveness and efficiency, in the works of \textcite{matklad2020challenginglrparsing, matklad2020prattparsing} for the Rust Analyzer, and~\textcite{Parr13} for the ANTLR parser generator, the authors advocate to implement handwritten recursive descent top-down parser for it allows the implementation of better error messages and error recovery algorithms for modern compiler implementations and tools.

\subsection{Semantic actions and syntax-directed translations}

As put by~\textcite{appel2003modern} \textquote{A compiler must do more than recognize whether a sentence belongs to the language of a grammar, it must do something useful with that sentence}.
The specification of \textit{semantic actions} allows us to do something useful with sentences that are parsed~\cite{appel2003modern}.
They are pieces of code that we can attach to the body of grammar production rules to specify \textit{syntax-directed definitions}~\cite{Aho:2006:CPT:1177220}.
Syntax-directed definitions, then, are used to perform \textit{syntax-directed translations} by executing the code specified by the semantic actions during specific moments of the parsing process~\cite{Aho:2006:CPT:1177220}.
Take the following syntax-directed definition
\begin{alignat}{2}
  \bnfprod{Expr}{\bnfvar{Term} \bnfvar{Expr2} \{Expr.n = Expr(Term.n, Expr2.n)\}} \label{eq:sdd_example}
\end{alignat}
Note that the attribute \codett{n} for the variables \bnfvar{Expr}, \bnfvar{Term} and \bnfvar{Expr2}, can be thought of as the return node value of a parsing function for this production.
If we give a closed look at the parser implemented in Figure~\ref{fig:predictive_parser_expr}, we can see this semantic action at line 4, after parsing \bnfvar{Term} and \bnfvar{Expr2}, returning the value of the attribute \codett{Expr.n}.
Furthermore, the position of the semantic action in a production body determines when the semantic action executes during parsing.
In this example it is positioned after the parser finishes parsing the production.

It is possible to write an entire compiler into the semantic actions of a parser~\cite{appel2003modern}.
However, this approach limits the compiler to analyze the source program in the strict order it is parsed~\cite{appel2003modern}.
Hence, using some sort of intermediate representation between syntax analysis and semantics analysis allows the semantic analyzer to be free from the constraints of the parsing algorithm~\cite{appel2003modern}.
One possible solution is to design the semantic actions in such a way that they output an intermediate tree data structure~\cite{appel2003modern}.
When the parser builds a tree with leaf nodes for each token and interior nodes for each production parsed we call such tree a \textit{concrete parse tree}~\cite{appel2003modern}.
An example of a concrete parse tree together with metadata collected for each node can be found at Figure~\ref{fig:concrete_syntax_tree}.

\begin{figure}[ht]
  \centering
  \begin{minipage}{0.5\textwidth}
    \begin{rustcode}
Expr
  Term
    Factor
      Number "5"
  Expr2
    Plus "+"
    Term
      Factor
        Id "a"
    \end{rustcode}
  \end{minipage}
  \caption{
    A sample concrete syntax tree of grammar~\ref{fig:fat_expr_grammar} for sentence \codett{5 + a}.
  }\label{fig:concrete_syntax_tree}
\end{figure}

Sometimes a concrete parse tree is too verbose, like a tree for the grammar in Figure~\ref{fig:fat_expr_grammar}, and, it is ideal to transform this tree into a simpler one in order to facilitate semantic analysis~\cite{appel2003modern}.
We can then create a second grammar with the \textit{abstract syntax} of the language.
The abstract syntax grammar does not need to follow the constraints of syntax analysis and only exists to facilitate the later stages of compilation~\cite{appel2003modern}.
For example, we can use the ambiguous grammar at~\eqref{eq:ambiguous_expr_grammar} as the abstract syntax for the language described by the grammar in Figure~\ref{fig:fat_expr_grammar}.

A compiler could easily translate a concrete parse tree into an \textit{Abstract Syntax Tree} (AST)~\cite{appel2003modern}.
An AST can be programmed by the use of programming constructs such as \textit{Abstract Data Types} (ADT) as show in Figure~\ref{fig:abstract_syntax_adt}.
In Section~\ref{chapter:background:sec:semantic} we will explore how to formally define the abstract tree and the rules that allows us to check if the semantic properties of the language are sound.

\begin{figure}[ht]
  \centering
  \begin{minipage}{0.6\textwidth}
    \begin{rustcode}
enum Ast {
  Expr(Ast, BinOp, Ast),
  Number(Int),
  Id(String),
}
enum BinOp {
  Plus,
  Minus,
  Times,
  Div,
}
    \end{rustcode}
  \end{minipage}
  \caption{
    An example of an abstract syntax for grammar at Figure~\eqref{eq:ambiguous_expr_grammar} described by Abstract data types.
  }\label{fig:abstract_syntax_adt}
\end{figure}

\subsection{Syntax error recovery}

When a parser fails to find a derivation for the input it is called a syntax error~\cite{appel2003modern}.
If we find an error, it would be advantageous for the user of the compiler if the parsing did not halt the compilers' execution on the first error~\cite{appel2003modern}.
This is what can be accomplished with error recovery~\cite{appel2003modern}.
One way we can build error recovery into top-down parsers is by the use of recovery token sets.
Essentially, once a parser using recovery token set finds an error, it discards all tokens until it finds a token in the recovery token set, then returns an error node containing the discarded tokens hopping that the father nodes will be able to continue parsing from the token found in the recovery token set.

\section{Semantic Analysis}\label{chapter:background:sec:semantic}

The third phase of a compiler is called semantic analysis.
It is responsible to use the AST generated by the parser to check if the source program is semantically consistent with the language specification.
An important part of semantic analysis is \textit{type checking} where it will try to validate the program to the language specification's type system.

If we feed a type checker with the abstract syntax tree for the program
\begin{equation*}
  \codett{b + 60 * c}
\end{equation*}
it may make use of a \textit{context} containing the type information of the identifiers \codett{b} and \codett{c} to type check all nodes of the tree for consistency.
Suppose a context that maps the identifier \codett{c} to the type \codett{Bool}, written \typer{c}{Bool}, which would compose of the values \codett{true} and \codett{false}.
With this information, the type checker would be able to decide if the operation \codett{60 / c} is semantically sound to the language's type system.
Does the language type system specifies as valid to divide a number by a boolean?
If it does, what does it mean to divide the value \codett{60} by \codett{false}?

Maybe, whoever designed the language decided that, if the dividend is a value of type number and the divisor is a value of type boolean, it will use some rule to convert the divisor's type to number in order to keep the program semantically sound.
Maybe, the language's type system forbids this behavior and will halt the compilation process with some error message.
These are decisions made when building a language's type system.

In order to star reasoning about the semantic aspects of a language suppose the following abstract syntax
\begin{equation}~\label{eq:sample_term_ast}
  \begin{alignedat}{2}
    \bnfprod{term}{\bnfvar{term}\ \bnfter{+}\ \bnfvar{term}} \\
    \bnfmore{\bnfvar{term}\ \bnfter{-}\ \bnfvar{term}} \\
    \bnfmore{\bnfvar{term}\ \bnfter{*}\ \bnfvar{term}} \\
    \bnfmore{\bnfvar{term}\ \bnfter{/}\ \bnfvar{term}} \\
    \bnfmore{\bnfter{number}} \\
    \bnfmore{\bnfter{id}}
  \end{alignedat}
\end{equation}
this set of productions defines a set of all possible terms, although compact and expressive it is only one of several ways of describing the syntax of a language~\cite{pierce2002types}.
According to \textcite{pierce2002types}, using the grammar to define the set of all terms is actually just a compact notation for the following inductive definition: The set of all terms is the smallest set $T$ such that
\begin{align}
   & \{\bnfter{nubmer},\ \bnfter{id}\} \subseteq T \label{eq:example_clause1}          \\
   & \textrm{if } \bnfvar{t}_1 \in T \textrm{ and } \bnfvar{t}_2 \in T \textrm{ then }
  \{
  \bnfvar{t}_1 \bnfter{+} \bnfvar{t}_2,
  \ \bnfvar{t}_1 \bnfter{-} \bnfvar{t}_2,
  \ \bnfvar{t}_1 \bnfter{*} \bnfvar{t}_2,
  \ \bnfvar{t}_1 \bnfter{/} \bnfvar{t}_2
  \} \subseteq T\label{eq:example_clause2}
\end{align}
Formally, this definition defines $T$ as a set of \textit{trees} and not a set of strings~\cite{pierce2002types}.
A shorthand definition for the same inductive definition of terms can be given in \textit{inference rules}, as following:
\begin{align*}
  \bnfter{id} \in T &  & \bnfter{number} \in T
\end{align*}
\begin{align*}
  \inferrule{
  \bnfvar{t}_1 \in T \\ \bnfvar{t}_2 \in T
  }{
    \bnfvar{t}_1\ \bnfter{+}\ \bnfvar{t}_2 \in T
  }
   &  &
  \inferrule{
  \bnfvar{t}_1 \in T \\ \bnfvar{t}_2 \in T
  }{
    \bnfvar{t}_1\ \bnfter{-}\ \bnfvar{t}_2 \in T
  }
\end{align*}
\begin{align*}
  \inferrule{
  \bnfvar{t}_1 \in T \\ \bnfvar{t}_2 \in T
  }{
    \bnfvar{t}_1\ \bnfter{*}\ \bnfvar{t}_2 \in T
  }
   &  &
  \inferrule{
  \bnfvar{t}_1 \in T \\ \bnfvar{t}_2 \in T
  }{
    \bnfvar{t}_1\ \bnfter{/}\ \bnfvar{t}_2 \in T
  }
\end{align*}
where the first two rules restate the first clause in~\eqref{eq:example_clause1} and the last 4 rules restate the second clause in~\eqref{eq:example_clause2}.
Each rule means that if we found the statements in the premises listed above the line, then we may derive the conclusion below the line.
When given as inference rules, the fact that $T$ is the smallest possible set is not stated explicitly.
Also, the rules with no premise are called \textit{axioms} and are written with no bar, since there is nothing to put above it.
Furthermore, \textcite{pierce2002types} points out that what we are calling \textit{inference rules} are actually \textit{rule schemas} which represents the infinite set of \textit{concrete rules} that can be obtained by substituting each variable by all possible sentences of the syntactic category, e.g., substituting each variable $\bnfvar{t}$ by every possible term in the inference rules above.

We can than derive a prof for \codett{(b + 5) * c} as
\begin{equation*}
  \inferrule{
    \inferrule{
      \bnfvar{b} \in T \\ \bnfvar{5} \in T
    }{
      \bnfvar{b}\ \bnfter{+}\ \bnfvar{5} \in T
    }
    \\ \bnfvar{c} \in T
  }{
    \bnfvar{b}\ \bnfter{+}\ \bnfvar{5}\ \bnfter{*}\ \bnfvar{t} \in T
  }
\end{equation*}
Here we see the concrete inference rules in play, and not the rules schemas.
We constructed the inference rules in such a way that proving is no different from finding a derivation of the grammar.
In the next section we will show how to enrich inference rules to prove more complex languages.

\subsection{Boolean and Integer terms}

Consider following extension of~\eqref{eq:sample_term_ast} with boolean types and a couple new terms:
\begin{equation}~\label{eq:sample_term_ast_bool}
  \begin{alignedat}{2}
    \bnfprod{term}{\bnfvar{term}\ \bnfvar{op}\ \bnfvar{term}} \\
    \bnfmore{\bnfter{if}\ \bnfvar{term}\ \bnfter{then}\ \bnfvar{term}\ \bnfter{else}\ \bnfvar{term}} \\
    \bnfmore{\bnfter{number}} \\
    \bnfmore{\bnfter{true}} \\
    \bnfmore{\bnfter{false}} \\
    \bnfmore{\bnfter{id}} \\
    \bnfprod{op}{\bnfvar{oparit}} \\
    \bnfmore{\bnfvar{opbool}} \\
    \bnfmore{\bnfvar{opcomp}} \\
    \bnfprod{oparit}{\bnfter{+} \bnfor{\bnfter{-}} \bnfor{\bnfter{*}} \bnfor{\bnfter{/}}} \\
    \bnfprod{opcomp}{\bnfter{<} \bnfor{\bnfter{<=}} \bnfor{\bnfter{==}} \bnfor{\bnfter{!=}}} \\
    \bnfprod{opbool}{\bnfter{\&\&} \bnfor{\bnfter{||}}}
  \end{alignedat}
\end{equation}
Before the new abstract syntax constructs, in~\eqref{eq:sample_term_ast}, all terms were numbers or variables to numbers, and we did not need to verify if in \code{$\bnfvar{t}_1$ \bnfter{+} $\bnfvar{t}_2$} the sum operation was well-defined for $\bnfvar{t}_1$ and $\bnfvar{t}_2$.
We need a system so we can prove that both $\bnfvar{t}_1$ and $\bnfvar{t}_2$ are numbers in \code{$\bnfvar{t}_1$ \bnfter{+} $\bnfvar{t}_2$}, for accomplishing the goal we introduce the formal notion of types.

The typing relation for arithmetic expressions is written as \typer{\bnfvar{term}}{\bnfvar{Type}}, and means that \bnfvar{term} is of type \bnfvar{Type} where \bnfvar{Type} is a new syntactic form for the types of the language~\cite{pierce2002types}. Also, the relation is defined by a set of inference rules assigning types to terms~\cite{pierce2002types}.

We introduce the syntactic form for $\bnfvar{T}$ as
\begin{equation}~\label{eq:sample_term_ast_bool_t}
  \begin{alignedat}{2}
    \bnfprod{T}{\bnfter{Nat}} \\
    \bnfmore{\bnfter{Bool}}
  \end{alignedat}
\end{equation}
and the inference rules that define the typing relation for~\eqref{eq:sample_term_ast_bool} as
\begin{align*}
  \typer{\bnfter{true}}{\bnfter{Bool}}
   &  & \typer{\bnfter{false}}{\bnfter{Bool}}
   &  & \typer{\bnfter{number}}{\bnfter{Nat}}
\end{align*}
\begin{align*}
  \inferrule*[right=num-arith]{
    \typer{$\bnfvar{t}_1$}{\bnfter{Num}}
  \\ \typer{$\bnfvar{t}_2$}{\bnfter{Num}}
  \\ \bnfvar{op} \in \bnfvar{oparit}
  }{
    \typer{$\bnfvar{t}_1$ \bnfvar{op} $\bnfvar{t}_2$}{\bnfter{Num}}
  }
\end{align*}
\begin{align*}
  \inferrule*[right=num-compare]{
    \typer{$\bnfvar{t}_1$}{\bnfter{Num}}
  \\ \typer{$\bnfvar{t}_2$}{\bnfter{Num}}
  \\ \bnfvar{op} \in \bnfvar{opcomp}
  }{
    \typer{$\bnfvar{t}_1$ \bnfvar{op} $\bnfvar{t}_2$}{\bnfter{Bool}}
  }
\end{align*}
\begin{align*}
  \inferrule*[right=bool-arith]{
    \typer{$\bnfvar{t}_1$}{\bnfter{Bool}}
  \\ \typer{$\bnfvar{t}_2$}{\bnfter{Bool}}
  \\ \bnfvar{op} \in \bnfvar{opbool}
  }{
    \typer{$\bnfvar{t}_1$ \bnfvar{op} $\bnfvar{t}_2$}{\bnfter{Bool}}
  }
\end{align*}
\begin{align*}
  \inferrule*[right=if]{
    \typer{$\bnfvar{t}_1$}{\bnfter{Bool}}
  \\ \typer{$\bnfvar{t}_2$}{\bnfvar{T}}
  \\ \typer{$\bnfvar{t}_3$}{\bnfvar{T}}
  }{
    \typer{\bnfter{if} $\bnfvar{t}_1$ \bnfter{then} $\bnfvar{t}_2$ \bnfter{else} $\bnfvar{t}_3$}{\bnfvar{T}}
  }
\end{align*}
now we can prove if \typer{2 * 3 == 42}{\bnfter{Bool}} by the following sequence of inferences
\begin{equation*}
  \inferrule{
    \inferrule
    {
      \typer{\bnfvar{2}}{\bnfter{Num}}
      \\ \typer{\bnfvar{3}}{\bnfter{Num}}
      \\ \bnfter{*} \in \bnfvar{oparit}
    }{
      \typer{\bnfvar{2} \bnfter{*} \bnfvar{3}}{\bnfter{Num}}
    }
    \\ \typer{\bnfvar{42}}{\bnfter{Num}}
    \\ \bnfter{==} \in \bnfvar{opcomp}
  }{
    \\ \typer{\bnfvar{2} \bnfter{*} \bnfvar{3} \bnfter{==} \bnfvar{42}}{\bnfter{Bool}}
  }
\end{equation*}
but if we try to prove something wrong like \typer{2 * true}{\bnfter{Num}} we will quickly find that there are no rules that allow us to reach the type relation axioms.

But what happens if we try to prove \typer{a + 5}{\bnfter{Num}} for example?
We currently have no rules that allow us to know infer the type of the identifier \codett{a}.
Nor do we have the capabilities with the current presented theory.
In order to for us to be capable of expressing the types of variables we need to extend the typing relation from a two-place relation into a three-place relation adding a context $\Gamma$~\cite{pierce2002types}.

The context $\Gamma$, also called \textit{typing context}, is a set of assumptions of the form \typer{\bnfter{id}}{\bnfvar{Type}}~\cite{pierce2002types}.
It is described by the syntactic form
\begin{equation}~\label{eq:sample_term_ast_bool_g}
  \begin{alignedat}{2}
    \bnfprod{$\Gamma$}{\bnfvar{$\Gamma$} \bnfter{,} \typer{\bnfter{id}}{\bnfvar{Type}}} \\
    \bnfmore{\bnfter{$\emptyset$}}
  \end{alignedat}
\end{equation}

The new three-place typing relation is written \typecxr{}{\bnfvar{term}}{\bnfvar{Type}}.

\section{Intermediate code generation}\label{chapter:background:sec:intermediate}

As the last phase of a compiler's front end we have the intermediate code generation.

To do. Here goes an introduction of Single Static Assignment intermediate representations.

\chapter{State of the art in front end design}\label{chapter:related_work}

In this chapter we will present prior work related to refinement types and the LLVM intermediate representation.

\section{Refinement types with predicates}

A type system can be extended to further \textit{refine} its types with logic predicates~\cite{jhala2020tutorial}.
In \textcite{jhala2020tutorial}, the authors call this technique \textit{Refinement types with predicates}.
It allows programmers to constrain existing types by using predicates to assert desired properties of the values they want to describe~\cite{jhala2020tutorial}.
Refinement types offer the option to add information to the type system about the invariants and correctness properties a programmer may care about~\cite{jhala2020tutorial}.
It is done in such a way that, if the programmer desires, no refinement needs to be added and can be thought like a typical type system~\cite{jhala2020tutorial}.
On the other hand, programmers can incrementally add refinements to ensure important properties about the source program~\cite{jhala2020tutorial}.
They could begin with basic safety requirements, e.g., eliminating division by zero and buffer overflow, or guarantee that a function does not receive a empty collection, and then incrementally add to the specification invariants of custom data types~\cite{jhala2020tutorial}.
Ultimately going all the way to specifying and verifying the correctness of different procedures at compile-time~\cite{jhala2020tutorial}.
By enabling verification on the same language as the programming language, refinement types bridge implementation and prof together~\cite{jhala2020tutorial}.
This approach creates a development cycle were the implementation hits programmers to what properties are important to verify, and the verification hits on how the implementation can be restructured to better express the invariants and enable formal proof~\cite{jhala2020tutorial}.

To do. Work in progress.

\section{The LLVM intermediate representation}

To do. Work in progress.

\chapter{The Ekitai language}\label{chapter:proposal}

In this chapter we will describe the design of the \textit{Ekitai} programming and the implementation of its front end to the LLVM intermediate representation.

\section{The Ekitai's lexer}

\section{The Ekitai's parser}

In order to implement the Ekitai's parser the following
% \begin{bnf}
%     \bnfprod{Source File}{\bnfes{} \bnfor{} \bnfpn{Module Items}}\\
%     \bnfprod{Module Items}{\bnfpn{Module Item} \bnfsp{} \bnfpn{Module Items} \bnfor{}}\\
%     \bnfmore{\bnfpn{Module Item}}\\
%     \bnfprod{Module Item}{\bnfpn{Function Definition} \bnfor{}}\\
%     \bnfmore{\bnfpn{Type Definition}}\\
%     \bnfprod{Type Definition}{\bnfpn{Name} \bnfsp{} \bnfpn{Value Constructor List}}\\
%     \bnfprod{Value Constructor List}{\bnfts{\{} \bnfsp{} \bnfpn{Value Constuctors} \bnfsp{} \bnfts{\}}}\\
%     \bnfprod{Value Constructors}{\bnfpn{Value Constructor} \bnfsp{} \bnfts{,} \bnfsp{} \bnfpn{Value Constructors} \bnfor{}}\\
%     \bnfmore{\bnfpn{Value Constructor} \bnfts{,} \bnfor{}}\\
%     \bnfmore{\bnfpn{Value Constructor}}\\
%     \bnfprod{Value Constructor}{\bnfpn{Name} \bnfsp{} \bnfpn{Constructor Parameter List}}\\
%     \bnfprod{Constructor Parameter List}{\bnfts{(} \bnfpn{Constructor Parameters} \bnfts{)}}\\
%     \bnfprod{Constructor Parameters}{\bnfpn{Type}}
% \end{bnf}

To do. Work in progress.

\section{The Ekitai's type system}

To do. Work in progress.

\section{The Ekitai's LLVM intermediate code generator}

To do. Work in progress.

\section{Conclusion}

To do. Work in progress.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Elementos pós-textuais                                       %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\postextual{}

\printbibliography{}

\end{document}
